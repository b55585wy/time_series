{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": "# 6类数据，分为浅呼吸躺，深呼吸躺，浅呼吸坐，深呼吸坐，浅呼吸走，深呼吸走， 0代表胸式呼吸 1代表腹式呼吸"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# 统计一下data.xlsx每一个label分别有多少个数据，总共有6个label（呼吸躺，深呼吸躺，浅呼吸坐，深呼吸坐，浅呼吸走，深呼吸走， 0代表胸式呼吸 1代表腹式呼吸）",
   "id": "43d29691ec909ff1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T15:35:04.200977Z",
     "start_time": "2024-08-29T15:35:01.548797Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 读取Excel文件\n",
    "file_path = 'data/data.xlsx'\n",
    "df = pd.read_excel(file_path, sheet_name='Sheet1',header=None)\n",
    "\n",
    "# 统计每个label的数量\n",
    "label_counts = df.iloc[:,-1].value_counts()\n",
    "\n",
    "# 打印统计结果\n",
    "print(label_counts)\n"
   ],
   "id": "95062941be26ceac",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "lying-1    17625\n",
      "sit-0      16103\n",
      "lying-0    15824\n",
      "sit-1      14176\n",
      "walk-1     10012\n",
      "walk-0      7519\n",
      "label          1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "21133a4a8959f6aa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T15:35:50.725669Z",
     "start_time": "2024-08-29T15:35:48.180093Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "df = pd.read_excel(file_path, sheet_name='Sheet1')\n",
    "label_counts = df['label'].value_counts()\n",
    "print(label_counts)"
   ],
   "id": "6c0cb34a83e11750",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "lying-1    17625\n",
      "sit-0      16103\n",
      "lying-0    15824\n",
      "sit-1      14176\n",
      "walk-1     10012\n",
      "walk-0      7519\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T15:36:13.421823Z",
     "start_time": "2024-08-29T15:36:13.390567Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# 提取特征和标签\n",
    "features = df[['x_imu', 'y_imu', 'z_imu', 'x_gry', 'y_gry', 'z_gry']]\n",
    "labels = df['label']\n",
    "\n",
    "# 标签编码\n",
    "label_encoder = LabelEncoder()\n",
    "labels_encoded = label_encoder.fit_transform(labels)\n",
    "\n",
    "# 划分训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# 转换为PyTorch张量\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)"
   ],
   "id": "715dc8cf170e07a6",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T15:36:34.247804Z",
     "start_time": "2024-08-29T15:36:34.242384Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# 创建数据集和数据加载器\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n"
   ],
   "id": "4a056c503835b14d",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T15:59:23.811420Z",
     "start_time": "2024-08-29T15:59:23.807465Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # RNN层\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        # 全连接层\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# 参数\n",
    "input_size = features.shape[1]  # 特征数量\n",
    "hidden_size = 64\n",
    "output_size = len(label_encoder.classes_)  # 标签类别数量\n",
    "num_layers = 1\n",
    "\n",
    "# 实例化模型、损失函数和优化器\n",
    "model = RNNModel(input_size, hidden_size, output_size, num_layers)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ],
   "id": "24da17803e74581e",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T15:59:49.217006Z",
     "start_time": "2024-08-29T15:59:27.459352Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        outputs = model(inputs.unsqueeze(1))  # 需要增加一个维度以适应 RNN 的输入\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n"
   ],
   "id": "d89774d839fab3ee",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [10/4063], Loss: 1.7152\n",
      "Epoch [1/10], Step [20/4063], Loss: 1.6273\n",
      "Epoch [1/10], Step [30/4063], Loss: 1.5584\n",
      "Epoch [1/10], Step [40/4063], Loss: 1.5026\n",
      "Epoch [1/10], Step [50/4063], Loss: 1.3251\n",
      "Epoch [1/10], Step [60/4063], Loss: 1.6033\n",
      "Epoch [1/10], Step [70/4063], Loss: 1.4645\n",
      "Epoch [1/10], Step [80/4063], Loss: 1.3668\n",
      "Epoch [1/10], Step [90/4063], Loss: 1.5445\n",
      "Epoch [1/10], Step [100/4063], Loss: 1.3998\n",
      "Epoch [1/10], Step [110/4063], Loss: 1.4043\n",
      "Epoch [1/10], Step [120/4063], Loss: 1.4805\n",
      "Epoch [1/10], Step [130/4063], Loss: 1.4455\n",
      "Epoch [1/10], Step [140/4063], Loss: 1.4682\n",
      "Epoch [1/10], Step [150/4063], Loss: 1.2368\n",
      "Epoch [1/10], Step [160/4063], Loss: 0.9514\n",
      "Epoch [1/10], Step [170/4063], Loss: 1.1947\n",
      "Epoch [1/10], Step [180/4063], Loss: 1.4369\n",
      "Epoch [1/10], Step [190/4063], Loss: 1.2664\n",
      "Epoch [1/10], Step [200/4063], Loss: 1.5006\n",
      "Epoch [1/10], Step [210/4063], Loss: 1.2934\n",
      "Epoch [1/10], Step [220/4063], Loss: 1.3213\n",
      "Epoch [1/10], Step [230/4063], Loss: 1.5893\n",
      "Epoch [1/10], Step [240/4063], Loss: 1.3102\n",
      "Epoch [1/10], Step [250/4063], Loss: 1.1424\n",
      "Epoch [1/10], Step [260/4063], Loss: 0.9760\n",
      "Epoch [1/10], Step [270/4063], Loss: 1.5885\n",
      "Epoch [1/10], Step [280/4063], Loss: 1.0189\n",
      "Epoch [1/10], Step [290/4063], Loss: 0.9674\n",
      "Epoch [1/10], Step [300/4063], Loss: 1.0836\n",
      "Epoch [1/10], Step [310/4063], Loss: 0.8218\n",
      "Epoch [1/10], Step [320/4063], Loss: 1.3156\n",
      "Epoch [1/10], Step [330/4063], Loss: 0.8518\n",
      "Epoch [1/10], Step [340/4063], Loss: 1.1065\n",
      "Epoch [1/10], Step [350/4063], Loss: 1.2775\n",
      "Epoch [1/10], Step [360/4063], Loss: 1.2101\n",
      "Epoch [1/10], Step [370/4063], Loss: 1.0637\n",
      "Epoch [1/10], Step [380/4063], Loss: 1.0339\n",
      "Epoch [1/10], Step [390/4063], Loss: 1.4282\n",
      "Epoch [1/10], Step [400/4063], Loss: 1.1857\n",
      "Epoch [1/10], Step [410/4063], Loss: 1.0502\n",
      "Epoch [1/10], Step [420/4063], Loss: 1.4368\n",
      "Epoch [1/10], Step [430/4063], Loss: 1.0640\n",
      "Epoch [1/10], Step [440/4063], Loss: 1.2274\n",
      "Epoch [1/10], Step [450/4063], Loss: 1.1484\n",
      "Epoch [1/10], Step [460/4063], Loss: 0.9363\n",
      "Epoch [1/10], Step [470/4063], Loss: 0.9973\n",
      "Epoch [1/10], Step [480/4063], Loss: 0.9610\n",
      "Epoch [1/10], Step [490/4063], Loss: 0.9719\n",
      "Epoch [1/10], Step [500/4063], Loss: 1.2005\n",
      "Epoch [1/10], Step [510/4063], Loss: 1.1809\n",
      "Epoch [1/10], Step [520/4063], Loss: 1.2289\n",
      "Epoch [1/10], Step [530/4063], Loss: 1.3628\n",
      "Epoch [1/10], Step [540/4063], Loss: 0.9840\n",
      "Epoch [1/10], Step [550/4063], Loss: 1.2069\n",
      "Epoch [1/10], Step [560/4063], Loss: 1.1472\n",
      "Epoch [1/10], Step [570/4063], Loss: 1.3619\n",
      "Epoch [1/10], Step [580/4063], Loss: 1.2665\n",
      "Epoch [1/10], Step [590/4063], Loss: 1.4021\n",
      "Epoch [1/10], Step [600/4063], Loss: 1.2372\n",
      "Epoch [1/10], Step [610/4063], Loss: 1.1135\n",
      "Epoch [1/10], Step [620/4063], Loss: 1.0770\n",
      "Epoch [1/10], Step [630/4063], Loss: 1.1462\n",
      "Epoch [1/10], Step [640/4063], Loss: 1.1573\n",
      "Epoch [1/10], Step [650/4063], Loss: 1.1308\n",
      "Epoch [1/10], Step [660/4063], Loss: 1.0129\n",
      "Epoch [1/10], Step [670/4063], Loss: 1.4784\n",
      "Epoch [1/10], Step [680/4063], Loss: 0.9902\n",
      "Epoch [1/10], Step [690/4063], Loss: 0.6833\n",
      "Epoch [1/10], Step [700/4063], Loss: 1.0089\n",
      "Epoch [1/10], Step [710/4063], Loss: 0.7427\n",
      "Epoch [1/10], Step [720/4063], Loss: 0.8649\n",
      "Epoch [1/10], Step [730/4063], Loss: 1.3076\n",
      "Epoch [1/10], Step [740/4063], Loss: 1.2804\n",
      "Epoch [1/10], Step [750/4063], Loss: 1.0752\n",
      "Epoch [1/10], Step [760/4063], Loss: 1.0351\n",
      "Epoch [1/10], Step [770/4063], Loss: 0.9037\n",
      "Epoch [1/10], Step [780/4063], Loss: 0.9953\n",
      "Epoch [1/10], Step [790/4063], Loss: 0.9972\n",
      "Epoch [1/10], Step [800/4063], Loss: 1.0415\n",
      "Epoch [1/10], Step [810/4063], Loss: 0.7686\n",
      "Epoch [1/10], Step [820/4063], Loss: 0.7784\n",
      "Epoch [1/10], Step [830/4063], Loss: 1.0627\n",
      "Epoch [1/10], Step [840/4063], Loss: 1.2108\n",
      "Epoch [1/10], Step [850/4063], Loss: 1.1069\n",
      "Epoch [1/10], Step [860/4063], Loss: 1.1570\n",
      "Epoch [1/10], Step [870/4063], Loss: 0.8603\n",
      "Epoch [1/10], Step [880/4063], Loss: 1.1051\n",
      "Epoch [1/10], Step [890/4063], Loss: 0.7420\n",
      "Epoch [1/10], Step [900/4063], Loss: 1.0397\n",
      "Epoch [1/10], Step [910/4063], Loss: 1.0307\n",
      "Epoch [1/10], Step [920/4063], Loss: 1.1172\n",
      "Epoch [1/10], Step [930/4063], Loss: 0.6313\n",
      "Epoch [1/10], Step [940/4063], Loss: 1.0731\n",
      "Epoch [1/10], Step [950/4063], Loss: 0.8557\n",
      "Epoch [1/10], Step [960/4063], Loss: 1.0201\n",
      "Epoch [1/10], Step [970/4063], Loss: 0.6662\n",
      "Epoch [1/10], Step [980/4063], Loss: 1.0195\n",
      "Epoch [1/10], Step [990/4063], Loss: 1.1218\n",
      "Epoch [1/10], Step [1000/4063], Loss: 0.8434\n",
      "Epoch [1/10], Step [1010/4063], Loss: 1.1962\n",
      "Epoch [1/10], Step [1020/4063], Loss: 1.0471\n",
      "Epoch [1/10], Step [1030/4063], Loss: 1.0344\n",
      "Epoch [1/10], Step [1040/4063], Loss: 1.0956\n",
      "Epoch [1/10], Step [1050/4063], Loss: 1.0654\n",
      "Epoch [1/10], Step [1060/4063], Loss: 1.0936\n",
      "Epoch [1/10], Step [1070/4063], Loss: 0.7890\n",
      "Epoch [1/10], Step [1080/4063], Loss: 0.8381\n",
      "Epoch [1/10], Step [1090/4063], Loss: 1.1228\n",
      "Epoch [1/10], Step [1100/4063], Loss: 0.7931\n",
      "Epoch [1/10], Step [1110/4063], Loss: 0.8158\n",
      "Epoch [1/10], Step [1120/4063], Loss: 1.1952\n",
      "Epoch [1/10], Step [1130/4063], Loss: 0.6688\n",
      "Epoch [1/10], Step [1140/4063], Loss: 1.1100\n",
      "Epoch [1/10], Step [1150/4063], Loss: 0.9021\n",
      "Epoch [1/10], Step [1160/4063], Loss: 0.9532\n",
      "Epoch [1/10], Step [1170/4063], Loss: 1.4206\n",
      "Epoch [1/10], Step [1180/4063], Loss: 0.7715\n",
      "Epoch [1/10], Step [1190/4063], Loss: 0.9491\n",
      "Epoch [1/10], Step [1200/4063], Loss: 1.2116\n",
      "Epoch [1/10], Step [1210/4063], Loss: 1.1484\n",
      "Epoch [1/10], Step [1220/4063], Loss: 0.9755\n",
      "Epoch [1/10], Step [1230/4063], Loss: 1.2372\n",
      "Epoch [1/10], Step [1240/4063], Loss: 0.6729\n",
      "Epoch [1/10], Step [1250/4063], Loss: 0.7525\n",
      "Epoch [1/10], Step [1260/4063], Loss: 1.1648\n",
      "Epoch [1/10], Step [1270/4063], Loss: 1.1542\n",
      "Epoch [1/10], Step [1280/4063], Loss: 0.7164\n",
      "Epoch [1/10], Step [1290/4063], Loss: 1.0084\n",
      "Epoch [1/10], Step [1300/4063], Loss: 1.0219\n",
      "Epoch [1/10], Step [1310/4063], Loss: 1.2112\n",
      "Epoch [1/10], Step [1320/4063], Loss: 1.2211\n",
      "Epoch [1/10], Step [1330/4063], Loss: 1.4446\n",
      "Epoch [1/10], Step [1340/4063], Loss: 1.0467\n",
      "Epoch [1/10], Step [1350/4063], Loss: 1.2716\n",
      "Epoch [1/10], Step [1360/4063], Loss: 0.7888\n",
      "Epoch [1/10], Step [1370/4063], Loss: 1.1610\n",
      "Epoch [1/10], Step [1380/4063], Loss: 1.2180\n",
      "Epoch [1/10], Step [1390/4063], Loss: 0.8330\n",
      "Epoch [1/10], Step [1400/4063], Loss: 0.9286\n",
      "Epoch [1/10], Step [1410/4063], Loss: 1.0748\n",
      "Epoch [1/10], Step [1420/4063], Loss: 0.9568\n",
      "Epoch [1/10], Step [1430/4063], Loss: 1.3349\n",
      "Epoch [1/10], Step [1440/4063], Loss: 1.1021\n",
      "Epoch [1/10], Step [1450/4063], Loss: 0.8593\n",
      "Epoch [1/10], Step [1460/4063], Loss: 0.6530\n",
      "Epoch [1/10], Step [1470/4063], Loss: 1.5507\n",
      "Epoch [1/10], Step [1480/4063], Loss: 0.9073\n",
      "Epoch [1/10], Step [1490/4063], Loss: 1.0069\n",
      "Epoch [1/10], Step [1500/4063], Loss: 1.0188\n",
      "Epoch [1/10], Step [1510/4063], Loss: 0.8557\n",
      "Epoch [1/10], Step [1520/4063], Loss: 0.6781\n",
      "Epoch [1/10], Step [1530/4063], Loss: 1.3544\n",
      "Epoch [1/10], Step [1540/4063], Loss: 0.9948\n",
      "Epoch [1/10], Step [1550/4063], Loss: 1.3638\n",
      "Epoch [1/10], Step [1560/4063], Loss: 1.0564\n",
      "Epoch [1/10], Step [1570/4063], Loss: 1.3887\n",
      "Epoch [1/10], Step [1580/4063], Loss: 0.7312\n",
      "Epoch [1/10], Step [1590/4063], Loss: 0.8719\n",
      "Epoch [1/10], Step [1600/4063], Loss: 0.8284\n",
      "Epoch [1/10], Step [1610/4063], Loss: 1.2377\n",
      "Epoch [1/10], Step [1620/4063], Loss: 0.7117\n",
      "Epoch [1/10], Step [1630/4063], Loss: 1.1035\n",
      "Epoch [1/10], Step [1640/4063], Loss: 0.8751\n",
      "Epoch [1/10], Step [1650/4063], Loss: 1.2660\n",
      "Epoch [1/10], Step [1660/4063], Loss: 1.0088\n",
      "Epoch [1/10], Step [1670/4063], Loss: 0.9621\n",
      "Epoch [1/10], Step [1680/4063], Loss: 1.2732\n",
      "Epoch [1/10], Step [1690/4063], Loss: 1.0127\n",
      "Epoch [1/10], Step [1700/4063], Loss: 1.0673\n",
      "Epoch [1/10], Step [1710/4063], Loss: 1.0986\n",
      "Epoch [1/10], Step [1720/4063], Loss: 1.4200\n",
      "Epoch [1/10], Step [1730/4063], Loss: 1.1888\n",
      "Epoch [1/10], Step [1740/4063], Loss: 1.0648\n",
      "Epoch [1/10], Step [1750/4063], Loss: 1.0160\n",
      "Epoch [1/10], Step [1760/4063], Loss: 1.1932\n",
      "Epoch [1/10], Step [1770/4063], Loss: 1.1022\n",
      "Epoch [1/10], Step [1780/4063], Loss: 1.1963\n",
      "Epoch [1/10], Step [1790/4063], Loss: 1.3482\n",
      "Epoch [1/10], Step [1800/4063], Loss: 0.6077\n",
      "Epoch [1/10], Step [1810/4063], Loss: 1.0868\n",
      "Epoch [1/10], Step [1820/4063], Loss: 0.6422\n",
      "Epoch [1/10], Step [1830/4063], Loss: 0.6653\n",
      "Epoch [1/10], Step [1840/4063], Loss: 0.9161\n",
      "Epoch [1/10], Step [1850/4063], Loss: 1.1235\n",
      "Epoch [1/10], Step [1860/4063], Loss: 1.1703\n",
      "Epoch [1/10], Step [1870/4063], Loss: 0.8955\n",
      "Epoch [1/10], Step [1880/4063], Loss: 1.0399\n",
      "Epoch [1/10], Step [1890/4063], Loss: 1.1457\n",
      "Epoch [1/10], Step [1900/4063], Loss: 0.9813\n",
      "Epoch [1/10], Step [1910/4063], Loss: 0.8079\n",
      "Epoch [1/10], Step [1920/4063], Loss: 0.8171\n",
      "Epoch [1/10], Step [1930/4063], Loss: 0.9294\n",
      "Epoch [1/10], Step [1940/4063], Loss: 0.9070\n",
      "Epoch [1/10], Step [1950/4063], Loss: 0.6263\n",
      "Epoch [1/10], Step [1960/4063], Loss: 1.0887\n",
      "Epoch [1/10], Step [1970/4063], Loss: 1.0003\n",
      "Epoch [1/10], Step [1980/4063], Loss: 0.8781\n",
      "Epoch [1/10], Step [1990/4063], Loss: 0.7412\n",
      "Epoch [1/10], Step [2000/4063], Loss: 0.9973\n",
      "Epoch [1/10], Step [2010/4063], Loss: 1.2293\n",
      "Epoch [1/10], Step [2020/4063], Loss: 0.9567\n",
      "Epoch [1/10], Step [2030/4063], Loss: 0.8966\n",
      "Epoch [1/10], Step [2040/4063], Loss: 0.8507\n",
      "Epoch [1/10], Step [2050/4063], Loss: 1.0655\n",
      "Epoch [1/10], Step [2060/4063], Loss: 1.2292\n",
      "Epoch [1/10], Step [2070/4063], Loss: 0.8635\n",
      "Epoch [1/10], Step [2080/4063], Loss: 1.2246\n",
      "Epoch [1/10], Step [2090/4063], Loss: 0.6661\n",
      "Epoch [1/10], Step [2100/4063], Loss: 1.0132\n",
      "Epoch [1/10], Step [2110/4063], Loss: 1.1465\n",
      "Epoch [1/10], Step [2120/4063], Loss: 0.9561\n",
      "Epoch [1/10], Step [2130/4063], Loss: 1.1041\n",
      "Epoch [1/10], Step [2140/4063], Loss: 0.7539\n",
      "Epoch [1/10], Step [2150/4063], Loss: 1.0644\n",
      "Epoch [1/10], Step [2160/4063], Loss: 1.2046\n",
      "Epoch [1/10], Step [2170/4063], Loss: 0.8503\n",
      "Epoch [1/10], Step [2180/4063], Loss: 0.7640\n",
      "Epoch [1/10], Step [2190/4063], Loss: 1.1020\n",
      "Epoch [1/10], Step [2200/4063], Loss: 0.9688\n",
      "Epoch [1/10], Step [2210/4063], Loss: 1.3168\n",
      "Epoch [1/10], Step [2220/4063], Loss: 1.2842\n",
      "Epoch [1/10], Step [2230/4063], Loss: 0.6090\n",
      "Epoch [1/10], Step [2240/4063], Loss: 0.6772\n",
      "Epoch [1/10], Step [2250/4063], Loss: 0.9499\n",
      "Epoch [1/10], Step [2260/4063], Loss: 1.0570\n",
      "Epoch [1/10], Step [2270/4063], Loss: 1.0148\n",
      "Epoch [1/10], Step [2280/4063], Loss: 0.8593\n",
      "Epoch [1/10], Step [2290/4063], Loss: 0.7886\n",
      "Epoch [1/10], Step [2300/4063], Loss: 0.8951\n",
      "Epoch [1/10], Step [2310/4063], Loss: 1.2421\n",
      "Epoch [1/10], Step [2320/4063], Loss: 1.3279\n",
      "Epoch [1/10], Step [2330/4063], Loss: 0.9628\n",
      "Epoch [1/10], Step [2340/4063], Loss: 1.0009\n",
      "Epoch [1/10], Step [2350/4063], Loss: 0.8501\n",
      "Epoch [1/10], Step [2360/4063], Loss: 0.9042\n",
      "Epoch [1/10], Step [2370/4063], Loss: 0.9367\n",
      "Epoch [1/10], Step [2380/4063], Loss: 0.8355\n",
      "Epoch [1/10], Step [2390/4063], Loss: 0.7341\n",
      "Epoch [1/10], Step [2400/4063], Loss: 1.2669\n",
      "Epoch [1/10], Step [2410/4063], Loss: 0.9822\n",
      "Epoch [1/10], Step [2420/4063], Loss: 0.7473\n",
      "Epoch [1/10], Step [2430/4063], Loss: 0.8914\n",
      "Epoch [1/10], Step [2440/4063], Loss: 1.0375\n",
      "Epoch [1/10], Step [2450/4063], Loss: 0.8023\n",
      "Epoch [1/10], Step [2460/4063], Loss: 0.9009\n",
      "Epoch [1/10], Step [2470/4063], Loss: 1.0431\n",
      "Epoch [1/10], Step [2480/4063], Loss: 0.8851\n",
      "Epoch [1/10], Step [2490/4063], Loss: 0.9728\n",
      "Epoch [1/10], Step [2500/4063], Loss: 0.7392\n",
      "Epoch [1/10], Step [2510/4063], Loss: 0.8288\n",
      "Epoch [1/10], Step [2520/4063], Loss: 0.9078\n",
      "Epoch [1/10], Step [2530/4063], Loss: 1.0006\n",
      "Epoch [1/10], Step [2540/4063], Loss: 1.3176\n",
      "Epoch [1/10], Step [2550/4063], Loss: 0.8658\n",
      "Epoch [1/10], Step [2560/4063], Loss: 0.9434\n",
      "Epoch [1/10], Step [2570/4063], Loss: 1.0696\n",
      "Epoch [1/10], Step [2580/4063], Loss: 0.5404\n",
      "Epoch [1/10], Step [2590/4063], Loss: 0.9277\n",
      "Epoch [1/10], Step [2600/4063], Loss: 1.0598\n",
      "Epoch [1/10], Step [2610/4063], Loss: 1.0844\n",
      "Epoch [1/10], Step [2620/4063], Loss: 0.9789\n",
      "Epoch [1/10], Step [2630/4063], Loss: 0.8858\n",
      "Epoch [1/10], Step [2640/4063], Loss: 0.9210\n",
      "Epoch [1/10], Step [2650/4063], Loss: 0.7128\n",
      "Epoch [1/10], Step [2660/4063], Loss: 0.8581\n",
      "Epoch [1/10], Step [2670/4063], Loss: 0.6690\n",
      "Epoch [1/10], Step [2680/4063], Loss: 0.8736\n",
      "Epoch [1/10], Step [2690/4063], Loss: 0.9973\n",
      "Epoch [1/10], Step [2700/4063], Loss: 1.2448\n",
      "Epoch [1/10], Step [2710/4063], Loss: 0.8071\n",
      "Epoch [1/10], Step [2720/4063], Loss: 0.8094\n",
      "Epoch [1/10], Step [2730/4063], Loss: 0.6620\n",
      "Epoch [1/10], Step [2740/4063], Loss: 0.7208\n",
      "Epoch [1/10], Step [2750/4063], Loss: 1.0168\n",
      "Epoch [1/10], Step [2760/4063], Loss: 0.8231\n",
      "Epoch [1/10], Step [2770/4063], Loss: 0.7304\n",
      "Epoch [1/10], Step [2780/4063], Loss: 1.1618\n",
      "Epoch [1/10], Step [2790/4063], Loss: 0.7004\n",
      "Epoch [1/10], Step [2800/4063], Loss: 0.9017\n",
      "Epoch [1/10], Step [2810/4063], Loss: 0.7959\n",
      "Epoch [1/10], Step [2820/4063], Loss: 0.9840\n",
      "Epoch [1/10], Step [2830/4063], Loss: 0.7723\n",
      "Epoch [1/10], Step [2840/4063], Loss: 1.1103\n",
      "Epoch [1/10], Step [2850/4063], Loss: 0.8790\n",
      "Epoch [1/10], Step [2860/4063], Loss: 1.0963\n",
      "Epoch [1/10], Step [2870/4063], Loss: 1.0605\n",
      "Epoch [1/10], Step [2880/4063], Loss: 0.7380\n",
      "Epoch [1/10], Step [2890/4063], Loss: 1.0323\n",
      "Epoch [1/10], Step [2900/4063], Loss: 1.1569\n",
      "Epoch [1/10], Step [2910/4063], Loss: 0.9027\n",
      "Epoch [1/10], Step [2920/4063], Loss: 1.1346\n",
      "Epoch [1/10], Step [2930/4063], Loss: 1.0389\n",
      "Epoch [1/10], Step [2940/4063], Loss: 1.0146\n",
      "Epoch [1/10], Step [2950/4063], Loss: 0.9750\n",
      "Epoch [1/10], Step [2960/4063], Loss: 0.9754\n",
      "Epoch [1/10], Step [2970/4063], Loss: 0.8404\n",
      "Epoch [1/10], Step [2980/4063], Loss: 0.8695\n",
      "Epoch [1/10], Step [2990/4063], Loss: 0.7072\n",
      "Epoch [1/10], Step [3000/4063], Loss: 0.8136\n",
      "Epoch [1/10], Step [3010/4063], Loss: 0.7986\n",
      "Epoch [1/10], Step [3020/4063], Loss: 0.9266\n",
      "Epoch [1/10], Step [3030/4063], Loss: 0.9591\n",
      "Epoch [1/10], Step [3040/4063], Loss: 0.6729\n",
      "Epoch [1/10], Step [3050/4063], Loss: 0.7086\n",
      "Epoch [1/10], Step [3060/4063], Loss: 1.3225\n",
      "Epoch [1/10], Step [3070/4063], Loss: 0.9611\n",
      "Epoch [1/10], Step [3080/4063], Loss: 0.7888\n",
      "Epoch [1/10], Step [3090/4063], Loss: 0.6361\n",
      "Epoch [1/10], Step [3100/4063], Loss: 1.2603\n",
      "Epoch [1/10], Step [3110/4063], Loss: 1.1203\n",
      "Epoch [1/10], Step [3120/4063], Loss: 1.2857\n",
      "Epoch [1/10], Step [3130/4063], Loss: 0.9752\n",
      "Epoch [1/10], Step [3140/4063], Loss: 1.0657\n",
      "Epoch [1/10], Step [3150/4063], Loss: 1.0974\n",
      "Epoch [1/10], Step [3160/4063], Loss: 0.7308\n",
      "Epoch [1/10], Step [3170/4063], Loss: 0.9091\n",
      "Epoch [1/10], Step [3180/4063], Loss: 0.9690\n",
      "Epoch [1/10], Step [3190/4063], Loss: 0.8477\n",
      "Epoch [1/10], Step [3200/4063], Loss: 0.7016\n",
      "Epoch [1/10], Step [3210/4063], Loss: 0.7571\n",
      "Epoch [1/10], Step [3220/4063], Loss: 1.1348\n",
      "Epoch [1/10], Step [3230/4063], Loss: 1.0370\n",
      "Epoch [1/10], Step [3240/4063], Loss: 1.2074\n",
      "Epoch [1/10], Step [3250/4063], Loss: 1.0513\n",
      "Epoch [1/10], Step [3260/4063], Loss: 1.1300\n",
      "Epoch [1/10], Step [3270/4063], Loss: 1.0447\n",
      "Epoch [1/10], Step [3280/4063], Loss: 1.1341\n",
      "Epoch [1/10], Step [3290/4063], Loss: 1.0697\n",
      "Epoch [1/10], Step [3300/4063], Loss: 0.8078\n",
      "Epoch [1/10], Step [3310/4063], Loss: 1.0760\n",
      "Epoch [1/10], Step [3320/4063], Loss: 0.9193\n",
      "Epoch [1/10], Step [3330/4063], Loss: 1.1541\n",
      "Epoch [1/10], Step [3340/4063], Loss: 1.1270\n",
      "Epoch [1/10], Step [3350/4063], Loss: 0.7787\n",
      "Epoch [1/10], Step [3360/4063], Loss: 0.5438\n",
      "Epoch [1/10], Step [3370/4063], Loss: 1.1156\n",
      "Epoch [1/10], Step [3380/4063], Loss: 1.0737\n",
      "Epoch [1/10], Step [3390/4063], Loss: 0.8900\n",
      "Epoch [1/10], Step [3400/4063], Loss: 1.0319\n",
      "Epoch [1/10], Step [3410/4063], Loss: 0.9003\n",
      "Epoch [1/10], Step [3420/4063], Loss: 0.9365\n",
      "Epoch [1/10], Step [3430/4063], Loss: 1.0130\n",
      "Epoch [1/10], Step [3440/4063], Loss: 0.9137\n",
      "Epoch [1/10], Step [3450/4063], Loss: 0.8611\n",
      "Epoch [1/10], Step [3460/4063], Loss: 1.2194\n",
      "Epoch [1/10], Step [3470/4063], Loss: 1.0390\n",
      "Epoch [1/10], Step [3480/4063], Loss: 0.9160\n",
      "Epoch [1/10], Step [3490/4063], Loss: 1.1884\n",
      "Epoch [1/10], Step [3500/4063], Loss: 1.1971\n",
      "Epoch [1/10], Step [3510/4063], Loss: 1.0350\n",
      "Epoch [1/10], Step [3520/4063], Loss: 0.7194\n",
      "Epoch [1/10], Step [3530/4063], Loss: 1.0654\n",
      "Epoch [1/10], Step [3540/4063], Loss: 0.9467\n",
      "Epoch [1/10], Step [3550/4063], Loss: 1.0383\n",
      "Epoch [1/10], Step [3560/4063], Loss: 1.0803\n",
      "Epoch [1/10], Step [3570/4063], Loss: 1.1960\n",
      "Epoch [1/10], Step [3580/4063], Loss: 0.7179\n",
      "Epoch [1/10], Step [3590/4063], Loss: 1.0678\n",
      "Epoch [1/10], Step [3600/4063], Loss: 0.8403\n",
      "Epoch [1/10], Step [3610/4063], Loss: 1.0629\n",
      "Epoch [1/10], Step [3620/4063], Loss: 0.8306\n",
      "Epoch [1/10], Step [3630/4063], Loss: 1.1857\n",
      "Epoch [1/10], Step [3640/4063], Loss: 1.1997\n",
      "Epoch [1/10], Step [3650/4063], Loss: 0.9958\n",
      "Epoch [1/10], Step [3660/4063], Loss: 1.0144\n",
      "Epoch [1/10], Step [3670/4063], Loss: 0.8476\n",
      "Epoch [1/10], Step [3680/4063], Loss: 0.9317\n",
      "Epoch [1/10], Step [3690/4063], Loss: 0.9505\n",
      "Epoch [1/10], Step [3700/4063], Loss: 0.9651\n",
      "Epoch [1/10], Step [3710/4063], Loss: 0.9869\n",
      "Epoch [1/10], Step [3720/4063], Loss: 0.9711\n",
      "Epoch [1/10], Step [3730/4063], Loss: 1.0750\n",
      "Epoch [1/10], Step [3740/4063], Loss: 1.2311\n",
      "Epoch [1/10], Step [3750/4063], Loss: 0.8949\n",
      "Epoch [1/10], Step [3760/4063], Loss: 0.7684\n",
      "Epoch [1/10], Step [3770/4063], Loss: 1.0170\n",
      "Epoch [1/10], Step [3780/4063], Loss: 1.0514\n",
      "Epoch [1/10], Step [3790/4063], Loss: 0.6228\n",
      "Epoch [1/10], Step [3800/4063], Loss: 0.8884\n",
      "Epoch [1/10], Step [3810/4063], Loss: 0.8702\n",
      "Epoch [1/10], Step [3820/4063], Loss: 0.7896\n",
      "Epoch [1/10], Step [3830/4063], Loss: 0.9585\n",
      "Epoch [1/10], Step [3840/4063], Loss: 1.0564\n",
      "Epoch [1/10], Step [3850/4063], Loss: 1.1111\n",
      "Epoch [1/10], Step [3860/4063], Loss: 0.8310\n",
      "Epoch [1/10], Step [3870/4063], Loss: 0.8921\n",
      "Epoch [1/10], Step [3880/4063], Loss: 0.9180\n",
      "Epoch [1/10], Step [3890/4063], Loss: 0.7164\n",
      "Epoch [1/10], Step [3900/4063], Loss: 1.0015\n",
      "Epoch [1/10], Step [3910/4063], Loss: 0.6620\n",
      "Epoch [1/10], Step [3920/4063], Loss: 0.7006\n",
      "Epoch [1/10], Step [3930/4063], Loss: 0.7845\n",
      "Epoch [1/10], Step [3940/4063], Loss: 0.7018\n",
      "Epoch [1/10], Step [3950/4063], Loss: 0.9386\n",
      "Epoch [1/10], Step [3960/4063], Loss: 0.9209\n",
      "Epoch [1/10], Step [3970/4063], Loss: 0.8112\n",
      "Epoch [1/10], Step [3980/4063], Loss: 0.9560\n",
      "Epoch [1/10], Step [3990/4063], Loss: 0.8203\n",
      "Epoch [1/10], Step [4000/4063], Loss: 0.8719\n",
      "Epoch [1/10], Step [4010/4063], Loss: 1.1234\n",
      "Epoch [1/10], Step [4020/4063], Loss: 1.1899\n",
      "Epoch [1/10], Step [4030/4063], Loss: 1.0656\n",
      "Epoch [1/10], Step [4040/4063], Loss: 0.7410\n",
      "Epoch [1/10], Step [4050/4063], Loss: 0.8921\n",
      "Epoch [1/10], Step [4060/4063], Loss: 0.7670\n",
      "Epoch [2/10], Step [10/4063], Loss: 0.8196\n",
      "Epoch [2/10], Step [20/4063], Loss: 1.0157\n",
      "Epoch [2/10], Step [30/4063], Loss: 0.7813\n",
      "Epoch [2/10], Step [40/4063], Loss: 0.8541\n",
      "Epoch [2/10], Step [50/4063], Loss: 0.9709\n",
      "Epoch [2/10], Step [60/4063], Loss: 0.8043\n",
      "Epoch [2/10], Step [70/4063], Loss: 0.9795\n",
      "Epoch [2/10], Step [80/4063], Loss: 0.9489\n",
      "Epoch [2/10], Step [90/4063], Loss: 1.1502\n",
      "Epoch [2/10], Step [100/4063], Loss: 0.8755\n",
      "Epoch [2/10], Step [110/4063], Loss: 1.0450\n",
      "Epoch [2/10], Step [120/4063], Loss: 1.0094\n",
      "Epoch [2/10], Step [130/4063], Loss: 0.8689\n",
      "Epoch [2/10], Step [140/4063], Loss: 0.9655\n",
      "Epoch [2/10], Step [150/4063], Loss: 0.9482\n",
      "Epoch [2/10], Step [160/4063], Loss: 0.7082\n",
      "Epoch [2/10], Step [170/4063], Loss: 0.9840\n",
      "Epoch [2/10], Step [180/4063], Loss: 0.9377\n",
      "Epoch [2/10], Step [190/4063], Loss: 1.0532\n",
      "Epoch [2/10], Step [200/4063], Loss: 0.6888\n",
      "Epoch [2/10], Step [210/4063], Loss: 0.7341\n",
      "Epoch [2/10], Step [220/4063], Loss: 0.6496\n",
      "Epoch [2/10], Step [230/4063], Loss: 0.6268\n",
      "Epoch [2/10], Step [240/4063], Loss: 1.1190\n",
      "Epoch [2/10], Step [250/4063], Loss: 0.9894\n",
      "Epoch [2/10], Step [260/4063], Loss: 0.7403\n",
      "Epoch [2/10], Step [270/4063], Loss: 1.2036\n",
      "Epoch [2/10], Step [280/4063], Loss: 1.1598\n",
      "Epoch [2/10], Step [290/4063], Loss: 1.2452\n",
      "Epoch [2/10], Step [300/4063], Loss: 0.7368\n",
      "Epoch [2/10], Step [310/4063], Loss: 1.1420\n",
      "Epoch [2/10], Step [320/4063], Loss: 1.1674\n",
      "Epoch [2/10], Step [330/4063], Loss: 0.9706\n",
      "Epoch [2/10], Step [340/4063], Loss: 0.5813\n",
      "Epoch [2/10], Step [350/4063], Loss: 0.9343\n",
      "Epoch [2/10], Step [360/4063], Loss: 0.8335\n",
      "Epoch [2/10], Step [370/4063], Loss: 1.0505\n",
      "Epoch [2/10], Step [380/4063], Loss: 0.9494\n",
      "Epoch [2/10], Step [390/4063], Loss: 0.8614\n",
      "Epoch [2/10], Step [400/4063], Loss: 0.7687\n",
      "Epoch [2/10], Step [410/4063], Loss: 1.2895\n",
      "Epoch [2/10], Step [420/4063], Loss: 1.0848\n",
      "Epoch [2/10], Step [430/4063], Loss: 0.7454\n",
      "Epoch [2/10], Step [440/4063], Loss: 1.0416\n",
      "Epoch [2/10], Step [450/4063], Loss: 0.5017\n",
      "Epoch [2/10], Step [460/4063], Loss: 0.5179\n",
      "Epoch [2/10], Step [470/4063], Loss: 1.1037\n",
      "Epoch [2/10], Step [480/4063], Loss: 0.6951\n",
      "Epoch [2/10], Step [490/4063], Loss: 0.9335\n",
      "Epoch [2/10], Step [500/4063], Loss: 1.1164\n",
      "Epoch [2/10], Step [510/4063], Loss: 0.8076\n",
      "Epoch [2/10], Step [520/4063], Loss: 0.7656\n",
      "Epoch [2/10], Step [530/4063], Loss: 1.1033\n",
      "Epoch [2/10], Step [540/4063], Loss: 0.8259\n",
      "Epoch [2/10], Step [550/4063], Loss: 1.0257\n",
      "Epoch [2/10], Step [560/4063], Loss: 0.9953\n",
      "Epoch [2/10], Step [570/4063], Loss: 1.2373\n",
      "Epoch [2/10], Step [580/4063], Loss: 0.7679\n",
      "Epoch [2/10], Step [590/4063], Loss: 0.6778\n",
      "Epoch [2/10], Step [600/4063], Loss: 0.8071\n",
      "Epoch [2/10], Step [610/4063], Loss: 1.0465\n",
      "Epoch [2/10], Step [620/4063], Loss: 0.9751\n",
      "Epoch [2/10], Step [630/4063], Loss: 0.5130\n",
      "Epoch [2/10], Step [640/4063], Loss: 1.1316\n",
      "Epoch [2/10], Step [650/4063], Loss: 0.6122\n",
      "Epoch [2/10], Step [660/4063], Loss: 0.5922\n",
      "Epoch [2/10], Step [670/4063], Loss: 0.9707\n",
      "Epoch [2/10], Step [680/4063], Loss: 1.1009\n",
      "Epoch [2/10], Step [690/4063], Loss: 1.0135\n",
      "Epoch [2/10], Step [700/4063], Loss: 1.0284\n",
      "Epoch [2/10], Step [710/4063], Loss: 0.8684\n",
      "Epoch [2/10], Step [720/4063], Loss: 0.9230\n",
      "Epoch [2/10], Step [730/4063], Loss: 1.3267\n",
      "Epoch [2/10], Step [740/4063], Loss: 1.1521\n",
      "Epoch [2/10], Step [750/4063], Loss: 0.7590\n",
      "Epoch [2/10], Step [760/4063], Loss: 0.9521\n",
      "Epoch [2/10], Step [770/4063], Loss: 1.4827\n",
      "Epoch [2/10], Step [780/4063], Loss: 1.2040\n",
      "Epoch [2/10], Step [790/4063], Loss: 1.2430\n",
      "Epoch [2/10], Step [800/4063], Loss: 0.9036\n",
      "Epoch [2/10], Step [810/4063], Loss: 1.0191\n",
      "Epoch [2/10], Step [820/4063], Loss: 0.7655\n",
      "Epoch [2/10], Step [830/4063], Loss: 1.2584\n",
      "Epoch [2/10], Step [840/4063], Loss: 0.7480\n",
      "Epoch [2/10], Step [850/4063], Loss: 0.6307\n",
      "Epoch [2/10], Step [860/4063], Loss: 0.9642\n",
      "Epoch [2/10], Step [870/4063], Loss: 0.8227\n",
      "Epoch [2/10], Step [880/4063], Loss: 0.8067\n",
      "Epoch [2/10], Step [890/4063], Loss: 0.8720\n",
      "Epoch [2/10], Step [900/4063], Loss: 0.6603\n",
      "Epoch [2/10], Step [910/4063], Loss: 0.6626\n",
      "Epoch [2/10], Step [920/4063], Loss: 1.2030\n",
      "Epoch [2/10], Step [930/4063], Loss: 1.0630\n",
      "Epoch [2/10], Step [940/4063], Loss: 0.9052\n",
      "Epoch [2/10], Step [950/4063], Loss: 0.9031\n",
      "Epoch [2/10], Step [960/4063], Loss: 0.8333\n",
      "Epoch [2/10], Step [970/4063], Loss: 0.8524\n",
      "Epoch [2/10], Step [980/4063], Loss: 0.9340\n",
      "Epoch [2/10], Step [990/4063], Loss: 0.9983\n",
      "Epoch [2/10], Step [1000/4063], Loss: 1.0985\n",
      "Epoch [2/10], Step [1010/4063], Loss: 0.9379\n",
      "Epoch [2/10], Step [1020/4063], Loss: 0.6951\n",
      "Epoch [2/10], Step [1030/4063], Loss: 0.5173\n",
      "Epoch [2/10], Step [1040/4063], Loss: 1.1181\n",
      "Epoch [2/10], Step [1050/4063], Loss: 1.1488\n",
      "Epoch [2/10], Step [1060/4063], Loss: 1.3288\n",
      "Epoch [2/10], Step [1070/4063], Loss: 0.8983\n",
      "Epoch [2/10], Step [1080/4063], Loss: 0.9742\n",
      "Epoch [2/10], Step [1090/4063], Loss: 1.2243\n",
      "Epoch [2/10], Step [1100/4063], Loss: 0.7661\n",
      "Epoch [2/10], Step [1110/4063], Loss: 0.9040\n",
      "Epoch [2/10], Step [1120/4063], Loss: 1.1927\n",
      "Epoch [2/10], Step [1130/4063], Loss: 0.5893\n",
      "Epoch [2/10], Step [1140/4063], Loss: 0.8085\n",
      "Epoch [2/10], Step [1150/4063], Loss: 1.1585\n",
      "Epoch [2/10], Step [1160/4063], Loss: 1.4761\n",
      "Epoch [2/10], Step [1170/4063], Loss: 0.9140\n",
      "Epoch [2/10], Step [1180/4063], Loss: 1.3224\n",
      "Epoch [2/10], Step [1190/4063], Loss: 1.1351\n",
      "Epoch [2/10], Step [1200/4063], Loss: 1.1999\n",
      "Epoch [2/10], Step [1210/4063], Loss: 1.1527\n",
      "Epoch [2/10], Step [1220/4063], Loss: 0.8920\n",
      "Epoch [2/10], Step [1230/4063], Loss: 0.8941\n",
      "Epoch [2/10], Step [1240/4063], Loss: 0.8300\n",
      "Epoch [2/10], Step [1250/4063], Loss: 1.2055\n",
      "Epoch [2/10], Step [1260/4063], Loss: 1.0320\n",
      "Epoch [2/10], Step [1270/4063], Loss: 1.1059\n",
      "Epoch [2/10], Step [1280/4063], Loss: 0.9270\n",
      "Epoch [2/10], Step [1290/4063], Loss: 0.6383\n",
      "Epoch [2/10], Step [1300/4063], Loss: 1.1120\n",
      "Epoch [2/10], Step [1310/4063], Loss: 1.1255\n",
      "Epoch [2/10], Step [1320/4063], Loss: 0.9017\n",
      "Epoch [2/10], Step [1330/4063], Loss: 0.8690\n",
      "Epoch [2/10], Step [1340/4063], Loss: 0.8880\n",
      "Epoch [2/10], Step [1350/4063], Loss: 0.8298\n",
      "Epoch [2/10], Step [1360/4063], Loss: 1.0960\n",
      "Epoch [2/10], Step [1370/4063], Loss: 0.9211\n",
      "Epoch [2/10], Step [1380/4063], Loss: 1.0744\n",
      "Epoch [2/10], Step [1390/4063], Loss: 0.9470\n",
      "Epoch [2/10], Step [1400/4063], Loss: 0.8765\n",
      "Epoch [2/10], Step [1410/4063], Loss: 0.8295\n",
      "Epoch [2/10], Step [1420/4063], Loss: 0.5744\n",
      "Epoch [2/10], Step [1430/4063], Loss: 1.0708\n",
      "Epoch [2/10], Step [1440/4063], Loss: 1.3475\n",
      "Epoch [2/10], Step [1450/4063], Loss: 0.7571\n",
      "Epoch [2/10], Step [1460/4063], Loss: 0.8020\n",
      "Epoch [2/10], Step [1470/4063], Loss: 0.9147\n",
      "Epoch [2/10], Step [1480/4063], Loss: 0.7308\n",
      "Epoch [2/10], Step [1490/4063], Loss: 1.1513\n",
      "Epoch [2/10], Step [1500/4063], Loss: 0.6895\n",
      "Epoch [2/10], Step [1510/4063], Loss: 0.7962\n",
      "Epoch [2/10], Step [1520/4063], Loss: 0.9711\n",
      "Epoch [2/10], Step [1530/4063], Loss: 1.1891\n",
      "Epoch [2/10], Step [1540/4063], Loss: 1.0883\n",
      "Epoch [2/10], Step [1550/4063], Loss: 0.9641\n",
      "Epoch [2/10], Step [1560/4063], Loss: 1.0132\n",
      "Epoch [2/10], Step [1570/4063], Loss: 0.8686\n",
      "Epoch [2/10], Step [1580/4063], Loss: 1.1416\n",
      "Epoch [2/10], Step [1590/4063], Loss: 1.0637\n",
      "Epoch [2/10], Step [1600/4063], Loss: 0.7561\n",
      "Epoch [2/10], Step [1610/4063], Loss: 0.9960\n",
      "Epoch [2/10], Step [1620/4063], Loss: 0.6485\n",
      "Epoch [2/10], Step [1630/4063], Loss: 0.7244\n",
      "Epoch [2/10], Step [1640/4063], Loss: 0.8044\n",
      "Epoch [2/10], Step [1650/4063], Loss: 0.8680\n",
      "Epoch [2/10], Step [1660/4063], Loss: 1.0293\n",
      "Epoch [2/10], Step [1670/4063], Loss: 0.8598\n",
      "Epoch [2/10], Step [1680/4063], Loss: 1.0636\n",
      "Epoch [2/10], Step [1690/4063], Loss: 0.6642\n",
      "Epoch [2/10], Step [1700/4063], Loss: 1.3541\n",
      "Epoch [2/10], Step [1710/4063], Loss: 0.9034\n",
      "Epoch [2/10], Step [1720/4063], Loss: 0.7017\n",
      "Epoch [2/10], Step [1730/4063], Loss: 0.7580\n",
      "Epoch [2/10], Step [1740/4063], Loss: 1.1949\n",
      "Epoch [2/10], Step [1750/4063], Loss: 1.3154\n",
      "Epoch [2/10], Step [1760/4063], Loss: 1.5472\n",
      "Epoch [2/10], Step [1770/4063], Loss: 0.8730\n",
      "Epoch [2/10], Step [1780/4063], Loss: 0.9614\n",
      "Epoch [2/10], Step [1790/4063], Loss: 0.6145\n",
      "Epoch [2/10], Step [1800/4063], Loss: 1.0555\n",
      "Epoch [2/10], Step [1810/4063], Loss: 1.0917\n",
      "Epoch [2/10], Step [1820/4063], Loss: 0.7631\n",
      "Epoch [2/10], Step [1830/4063], Loss: 1.3496\n",
      "Epoch [2/10], Step [1840/4063], Loss: 0.8989\n",
      "Epoch [2/10], Step [1850/4063], Loss: 1.2103\n",
      "Epoch [2/10], Step [1860/4063], Loss: 0.9267\n",
      "Epoch [2/10], Step [1870/4063], Loss: 0.4448\n",
      "Epoch [2/10], Step [1880/4063], Loss: 0.7780\n",
      "Epoch [2/10], Step [1890/4063], Loss: 1.2237\n",
      "Epoch [2/10], Step [1900/4063], Loss: 1.2185\n",
      "Epoch [2/10], Step [1910/4063], Loss: 0.9501\n",
      "Epoch [2/10], Step [1920/4063], Loss: 1.0074\n",
      "Epoch [2/10], Step [1930/4063], Loss: 1.0488\n",
      "Epoch [2/10], Step [1940/4063], Loss: 1.3485\n",
      "Epoch [2/10], Step [1950/4063], Loss: 0.9375\n",
      "Epoch [2/10], Step [1960/4063], Loss: 1.1411\n",
      "Epoch [2/10], Step [1970/4063], Loss: 0.7583\n",
      "Epoch [2/10], Step [1980/4063], Loss: 1.2637\n",
      "Epoch [2/10], Step [1990/4063], Loss: 0.9247\n",
      "Epoch [2/10], Step [2000/4063], Loss: 0.9222\n",
      "Epoch [2/10], Step [2010/4063], Loss: 0.6771\n",
      "Epoch [2/10], Step [2020/4063], Loss: 0.9275\n",
      "Epoch [2/10], Step [2030/4063], Loss: 0.7502\n",
      "Epoch [2/10], Step [2040/4063], Loss: 0.9334\n",
      "Epoch [2/10], Step [2050/4063], Loss: 0.9152\n",
      "Epoch [2/10], Step [2060/4063], Loss: 0.8566\n",
      "Epoch [2/10], Step [2070/4063], Loss: 1.1067\n",
      "Epoch [2/10], Step [2080/4063], Loss: 0.8163\n",
      "Epoch [2/10], Step [2090/4063], Loss: 0.8468\n",
      "Epoch [2/10], Step [2100/4063], Loss: 0.8357\n",
      "Epoch [2/10], Step [2110/4063], Loss: 1.0034\n",
      "Epoch [2/10], Step [2120/4063], Loss: 1.4184\n",
      "Epoch [2/10], Step [2130/4063], Loss: 0.6569\n",
      "Epoch [2/10], Step [2140/4063], Loss: 0.6977\n",
      "Epoch [2/10], Step [2150/4063], Loss: 1.0512\n",
      "Epoch [2/10], Step [2160/4063], Loss: 0.6475\n",
      "Epoch [2/10], Step [2170/4063], Loss: 0.8397\n",
      "Epoch [2/10], Step [2180/4063], Loss: 1.4835\n",
      "Epoch [2/10], Step [2190/4063], Loss: 0.6917\n",
      "Epoch [2/10], Step [2200/4063], Loss: 0.8955\n",
      "Epoch [2/10], Step [2210/4063], Loss: 0.8956\n",
      "Epoch [2/10], Step [2220/4063], Loss: 1.1911\n",
      "Epoch [2/10], Step [2230/4063], Loss: 1.0796\n",
      "Epoch [2/10], Step [2240/4063], Loss: 1.0010\n",
      "Epoch [2/10], Step [2250/4063], Loss: 0.9351\n",
      "Epoch [2/10], Step [2260/4063], Loss: 1.2245\n",
      "Epoch [2/10], Step [2270/4063], Loss: 1.0456\n",
      "Epoch [2/10], Step [2280/4063], Loss: 1.0363\n",
      "Epoch [2/10], Step [2290/4063], Loss: 1.3278\n",
      "Epoch [2/10], Step [2300/4063], Loss: 1.2229\n",
      "Epoch [2/10], Step [2310/4063], Loss: 0.8528\n",
      "Epoch [2/10], Step [2320/4063], Loss: 0.8209\n",
      "Epoch [2/10], Step [2330/4063], Loss: 0.6602\n",
      "Epoch [2/10], Step [2340/4063], Loss: 1.1740\n",
      "Epoch [2/10], Step [2350/4063], Loss: 0.7751\n",
      "Epoch [2/10], Step [2360/4063], Loss: 1.0052\n",
      "Epoch [2/10], Step [2370/4063], Loss: 0.9201\n",
      "Epoch [2/10], Step [2380/4063], Loss: 1.2520\n",
      "Epoch [2/10], Step [2390/4063], Loss: 0.7333\n",
      "Epoch [2/10], Step [2400/4063], Loss: 1.0262\n",
      "Epoch [2/10], Step [2410/4063], Loss: 0.9549\n",
      "Epoch [2/10], Step [2420/4063], Loss: 0.9597\n",
      "Epoch [2/10], Step [2430/4063], Loss: 0.9553\n",
      "Epoch [2/10], Step [2440/4063], Loss: 0.9436\n",
      "Epoch [2/10], Step [2450/4063], Loss: 0.9124\n",
      "Epoch [2/10], Step [2460/4063], Loss: 1.0478\n",
      "Epoch [2/10], Step [2470/4063], Loss: 1.1805\n",
      "Epoch [2/10], Step [2480/4063], Loss: 1.0960\n",
      "Epoch [2/10], Step [2490/4063], Loss: 0.8162\n",
      "Epoch [2/10], Step [2500/4063], Loss: 1.2270\n",
      "Epoch [2/10], Step [2510/4063], Loss: 1.0438\n",
      "Epoch [2/10], Step [2520/4063], Loss: 0.7026\n",
      "Epoch [2/10], Step [2530/4063], Loss: 0.8758\n",
      "Epoch [2/10], Step [2540/4063], Loss: 1.0750\n",
      "Epoch [2/10], Step [2550/4063], Loss: 1.2110\n",
      "Epoch [2/10], Step [2560/4063], Loss: 1.1015\n",
      "Epoch [2/10], Step [2570/4063], Loss: 0.7255\n",
      "Epoch [2/10], Step [2580/4063], Loss: 0.7271\n",
      "Epoch [2/10], Step [2590/4063], Loss: 1.3209\n",
      "Epoch [2/10], Step [2600/4063], Loss: 0.9620\n",
      "Epoch [2/10], Step [2610/4063], Loss: 0.9731\n",
      "Epoch [2/10], Step [2620/4063], Loss: 0.9692\n",
      "Epoch [2/10], Step [2630/4063], Loss: 0.8503\n",
      "Epoch [2/10], Step [2640/4063], Loss: 1.5662\n",
      "Epoch [2/10], Step [2650/4063], Loss: 1.1905\n",
      "Epoch [2/10], Step [2660/4063], Loss: 1.0385\n",
      "Epoch [2/10], Step [2670/4063], Loss: 0.8943\n",
      "Epoch [2/10], Step [2680/4063], Loss: 0.8307\n",
      "Epoch [2/10], Step [2690/4063], Loss: 1.0541\n",
      "Epoch [2/10], Step [2700/4063], Loss: 0.7973\n",
      "Epoch [2/10], Step [2710/4063], Loss: 0.8605\n",
      "Epoch [2/10], Step [2720/4063], Loss: 0.5702\n",
      "Epoch [2/10], Step [2730/4063], Loss: 0.8568\n",
      "Epoch [2/10], Step [2740/4063], Loss: 1.2901\n",
      "Epoch [2/10], Step [2750/4063], Loss: 0.8095\n",
      "Epoch [2/10], Step [2760/4063], Loss: 1.1520\n",
      "Epoch [2/10], Step [2770/4063], Loss: 0.9836\n",
      "Epoch [2/10], Step [2780/4063], Loss: 1.2549\n",
      "Epoch [2/10], Step [2790/4063], Loss: 0.4754\n",
      "Epoch [2/10], Step [2800/4063], Loss: 0.8489\n",
      "Epoch [2/10], Step [2810/4063], Loss: 1.1052\n",
      "Epoch [2/10], Step [2820/4063], Loss: 0.8569\n",
      "Epoch [2/10], Step [2830/4063], Loss: 0.7018\n",
      "Epoch [2/10], Step [2840/4063], Loss: 0.6549\n",
      "Epoch [2/10], Step [2850/4063], Loss: 1.1878\n",
      "Epoch [2/10], Step [2860/4063], Loss: 1.4741\n",
      "Epoch [2/10], Step [2870/4063], Loss: 1.1184\n",
      "Epoch [2/10], Step [2880/4063], Loss: 0.5315\n",
      "Epoch [2/10], Step [2890/4063], Loss: 0.8093\n",
      "Epoch [2/10], Step [2900/4063], Loss: 0.8277\n",
      "Epoch [2/10], Step [2910/4063], Loss: 0.6552\n",
      "Epoch [2/10], Step [2920/4063], Loss: 0.6992\n",
      "Epoch [2/10], Step [2930/4063], Loss: 1.2248\n",
      "Epoch [2/10], Step [2940/4063], Loss: 0.9525\n",
      "Epoch [2/10], Step [2950/4063], Loss: 1.0051\n",
      "Epoch [2/10], Step [2960/4063], Loss: 0.8784\n",
      "Epoch [2/10], Step [2970/4063], Loss: 0.7710\n",
      "Epoch [2/10], Step [2980/4063], Loss: 0.7767\n",
      "Epoch [2/10], Step [2990/4063], Loss: 0.7184\n",
      "Epoch [2/10], Step [3000/4063], Loss: 0.9293\n",
      "Epoch [2/10], Step [3010/4063], Loss: 1.6050\n",
      "Epoch [2/10], Step [3020/4063], Loss: 0.7749\n",
      "Epoch [2/10], Step [3030/4063], Loss: 1.4791\n",
      "Epoch [2/10], Step [3040/4063], Loss: 1.1356\n",
      "Epoch [2/10], Step [3050/4063], Loss: 0.8374\n",
      "Epoch [2/10], Step [3060/4063], Loss: 0.7752\n",
      "Epoch [2/10], Step [3070/4063], Loss: 0.9943\n",
      "Epoch [2/10], Step [3080/4063], Loss: 1.0270\n",
      "Epoch [2/10], Step [3090/4063], Loss: 1.4437\n",
      "Epoch [2/10], Step [3100/4063], Loss: 1.0377\n",
      "Epoch [2/10], Step [3110/4063], Loss: 0.7812\n",
      "Epoch [2/10], Step [3120/4063], Loss: 1.0974\n",
      "Epoch [2/10], Step [3130/4063], Loss: 0.6950\n",
      "Epoch [2/10], Step [3140/4063], Loss: 0.8489\n",
      "Epoch [2/10], Step [3150/4063], Loss: 1.2214\n",
      "Epoch [2/10], Step [3160/4063], Loss: 0.5900\n",
      "Epoch [2/10], Step [3170/4063], Loss: 0.8002\n",
      "Epoch [2/10], Step [3180/4063], Loss: 0.7339\n",
      "Epoch [2/10], Step [3190/4063], Loss: 0.8191\n",
      "Epoch [2/10], Step [3200/4063], Loss: 0.6580\n",
      "Epoch [2/10], Step [3210/4063], Loss: 0.9193\n",
      "Epoch [2/10], Step [3220/4063], Loss: 0.8339\n",
      "Epoch [2/10], Step [3230/4063], Loss: 0.7743\n",
      "Epoch [2/10], Step [3240/4063], Loss: 0.9906\n",
      "Epoch [2/10], Step [3250/4063], Loss: 0.9139\n",
      "Epoch [2/10], Step [3260/4063], Loss: 1.1389\n",
      "Epoch [2/10], Step [3270/4063], Loss: 0.7835\n",
      "Epoch [2/10], Step [3280/4063], Loss: 1.0287\n",
      "Epoch [2/10], Step [3290/4063], Loss: 0.7304\n",
      "Epoch [2/10], Step [3300/4063], Loss: 1.1414\n",
      "Epoch [2/10], Step [3310/4063], Loss: 0.8572\n",
      "Epoch [2/10], Step [3320/4063], Loss: 0.8133\n",
      "Epoch [2/10], Step [3330/4063], Loss: 0.9004\n",
      "Epoch [2/10], Step [3340/4063], Loss: 1.3149\n",
      "Epoch [2/10], Step [3350/4063], Loss: 0.9372\n",
      "Epoch [2/10], Step [3360/4063], Loss: 1.1676\n",
      "Epoch [2/10], Step [3370/4063], Loss: 0.7767\n",
      "Epoch [2/10], Step [3380/4063], Loss: 1.2270\n",
      "Epoch [2/10], Step [3390/4063], Loss: 0.7677\n",
      "Epoch [2/10], Step [3400/4063], Loss: 0.8700\n",
      "Epoch [2/10], Step [3410/4063], Loss: 0.9450\n",
      "Epoch [2/10], Step [3420/4063], Loss: 1.5983\n",
      "Epoch [2/10], Step [3430/4063], Loss: 1.2121\n",
      "Epoch [2/10], Step [3440/4063], Loss: 1.4703\n",
      "Epoch [2/10], Step [3450/4063], Loss: 1.1140\n",
      "Epoch [2/10], Step [3460/4063], Loss: 0.8575\n",
      "Epoch [2/10], Step [3470/4063], Loss: 1.2012\n",
      "Epoch [2/10], Step [3480/4063], Loss: 0.8441\n",
      "Epoch [2/10], Step [3490/4063], Loss: 0.8607\n",
      "Epoch [2/10], Step [3500/4063], Loss: 1.1702\n",
      "Epoch [2/10], Step [3510/4063], Loss: 1.2321\n",
      "Epoch [2/10], Step [3520/4063], Loss: 0.8379\n",
      "Epoch [2/10], Step [3530/4063], Loss: 0.8251\n",
      "Epoch [2/10], Step [3540/4063], Loss: 0.9312\n",
      "Epoch [2/10], Step [3550/4063], Loss: 0.6473\n",
      "Epoch [2/10], Step [3560/4063], Loss: 0.8420\n",
      "Epoch [2/10], Step [3570/4063], Loss: 0.9459\n",
      "Epoch [2/10], Step [3580/4063], Loss: 1.0647\n",
      "Epoch [2/10], Step [3590/4063], Loss: 0.7522\n",
      "Epoch [2/10], Step [3600/4063], Loss: 0.8171\n",
      "Epoch [2/10], Step [3610/4063], Loss: 0.9183\n",
      "Epoch [2/10], Step [3620/4063], Loss: 0.7452\n",
      "Epoch [2/10], Step [3630/4063], Loss: 1.3306\n",
      "Epoch [2/10], Step [3640/4063], Loss: 0.8284\n",
      "Epoch [2/10], Step [3650/4063], Loss: 0.4496\n",
      "Epoch [2/10], Step [3660/4063], Loss: 0.9594\n",
      "Epoch [2/10], Step [3670/4063], Loss: 0.6615\n",
      "Epoch [2/10], Step [3680/4063], Loss: 0.9385\n",
      "Epoch [2/10], Step [3690/4063], Loss: 1.2523\n",
      "Epoch [2/10], Step [3700/4063], Loss: 1.1752\n",
      "Epoch [2/10], Step [3710/4063], Loss: 1.2787\n",
      "Epoch [2/10], Step [3720/4063], Loss: 0.6844\n",
      "Epoch [2/10], Step [3730/4063], Loss: 0.8399\n",
      "Epoch [2/10], Step [3740/4063], Loss: 1.0763\n",
      "Epoch [2/10], Step [3750/4063], Loss: 1.3580\n",
      "Epoch [2/10], Step [3760/4063], Loss: 1.0130\n",
      "Epoch [2/10], Step [3770/4063], Loss: 1.2461\n",
      "Epoch [2/10], Step [3780/4063], Loss: 1.1168\n",
      "Epoch [2/10], Step [3790/4063], Loss: 1.3425\n",
      "Epoch [2/10], Step [3800/4063], Loss: 0.8234\n",
      "Epoch [2/10], Step [3810/4063], Loss: 1.2481\n",
      "Epoch [2/10], Step [3820/4063], Loss: 1.0719\n",
      "Epoch [2/10], Step [3830/4063], Loss: 1.0403\n",
      "Epoch [2/10], Step [3840/4063], Loss: 1.1284\n",
      "Epoch [2/10], Step [3850/4063], Loss: 0.6766\n",
      "Epoch [2/10], Step [3860/4063], Loss: 0.8340\n",
      "Epoch [2/10], Step [3870/4063], Loss: 0.9667\n",
      "Epoch [2/10], Step [3880/4063], Loss: 0.7194\n",
      "Epoch [2/10], Step [3890/4063], Loss: 0.6299\n",
      "Epoch [2/10], Step [3900/4063], Loss: 0.6980\n",
      "Epoch [2/10], Step [3910/4063], Loss: 0.8936\n",
      "Epoch [2/10], Step [3920/4063], Loss: 1.4294\n",
      "Epoch [2/10], Step [3930/4063], Loss: 1.0350\n",
      "Epoch [2/10], Step [3940/4063], Loss: 0.7150\n",
      "Epoch [2/10], Step [3950/4063], Loss: 0.7887\n",
      "Epoch [2/10], Step [3960/4063], Loss: 0.8317\n",
      "Epoch [2/10], Step [3970/4063], Loss: 1.3963\n",
      "Epoch [2/10], Step [3980/4063], Loss: 0.6401\n",
      "Epoch [2/10], Step [3990/4063], Loss: 0.7948\n",
      "Epoch [2/10], Step [4000/4063], Loss: 0.5580\n",
      "Epoch [2/10], Step [4010/4063], Loss: 0.9883\n",
      "Epoch [2/10], Step [4020/4063], Loss: 1.2525\n",
      "Epoch [2/10], Step [4030/4063], Loss: 0.7086\n",
      "Epoch [2/10], Step [4040/4063], Loss: 0.9212\n",
      "Epoch [2/10], Step [4050/4063], Loss: 1.1119\n",
      "Epoch [2/10], Step [4060/4063], Loss: 0.8682\n",
      "Epoch [3/10], Step [10/4063], Loss: 0.6704\n",
      "Epoch [3/10], Step [20/4063], Loss: 0.7678\n",
      "Epoch [3/10], Step [30/4063], Loss: 0.8110\n",
      "Epoch [3/10], Step [40/4063], Loss: 0.8616\n",
      "Epoch [3/10], Step [50/4063], Loss: 1.2746\n",
      "Epoch [3/10], Step [60/4063], Loss: 1.4387\n",
      "Epoch [3/10], Step [70/4063], Loss: 0.7902\n",
      "Epoch [3/10], Step [80/4063], Loss: 0.9070\n",
      "Epoch [3/10], Step [90/4063], Loss: 1.1673\n",
      "Epoch [3/10], Step [100/4063], Loss: 0.6276\n",
      "Epoch [3/10], Step [110/4063], Loss: 0.7507\n",
      "Epoch [3/10], Step [120/4063], Loss: 0.8061\n",
      "Epoch [3/10], Step [130/4063], Loss: 1.0214\n",
      "Epoch [3/10], Step [140/4063], Loss: 0.9249\n",
      "Epoch [3/10], Step [150/4063], Loss: 0.9617\n",
      "Epoch [3/10], Step [160/4063], Loss: 0.8521\n",
      "Epoch [3/10], Step [170/4063], Loss: 1.1118\n",
      "Epoch [3/10], Step [180/4063], Loss: 1.1606\n",
      "Epoch [3/10], Step [190/4063], Loss: 0.8899\n",
      "Epoch [3/10], Step [200/4063], Loss: 0.8697\n",
      "Epoch [3/10], Step [210/4063], Loss: 0.7869\n",
      "Epoch [3/10], Step [220/4063], Loss: 0.9726\n",
      "Epoch [3/10], Step [230/4063], Loss: 0.9544\n",
      "Epoch [3/10], Step [240/4063], Loss: 0.7450\n",
      "Epoch [3/10], Step [250/4063], Loss: 0.6750\n",
      "Epoch [3/10], Step [260/4063], Loss: 0.6397\n",
      "Epoch [3/10], Step [270/4063], Loss: 0.8856\n",
      "Epoch [3/10], Step [280/4063], Loss: 1.0562\n",
      "Epoch [3/10], Step [290/4063], Loss: 1.1812\n",
      "Epoch [3/10], Step [300/4063], Loss: 0.9553\n",
      "Epoch [3/10], Step [310/4063], Loss: 1.0261\n",
      "Epoch [3/10], Step [320/4063], Loss: 1.0679\n",
      "Epoch [3/10], Step [330/4063], Loss: 0.6651\n",
      "Epoch [3/10], Step [340/4063], Loss: 1.0333\n",
      "Epoch [3/10], Step [350/4063], Loss: 1.1593\n",
      "Epoch [3/10], Step [360/4063], Loss: 1.4912\n",
      "Epoch [3/10], Step [370/4063], Loss: 1.0972\n",
      "Epoch [3/10], Step [380/4063], Loss: 0.9633\n",
      "Epoch [3/10], Step [390/4063], Loss: 0.8268\n",
      "Epoch [3/10], Step [400/4063], Loss: 0.9637\n",
      "Epoch [3/10], Step [410/4063], Loss: 0.8389\n",
      "Epoch [3/10], Step [420/4063], Loss: 1.1925\n",
      "Epoch [3/10], Step [430/4063], Loss: 1.5727\n",
      "Epoch [3/10], Step [440/4063], Loss: 0.8380\n",
      "Epoch [3/10], Step [450/4063], Loss: 1.0534\n",
      "Epoch [3/10], Step [460/4063], Loss: 0.7428\n",
      "Epoch [3/10], Step [470/4063], Loss: 1.0848\n",
      "Epoch [3/10], Step [480/4063], Loss: 1.1419\n",
      "Epoch [3/10], Step [490/4063], Loss: 0.7743\n",
      "Epoch [3/10], Step [500/4063], Loss: 1.3786\n",
      "Epoch [3/10], Step [510/4063], Loss: 0.7927\n",
      "Epoch [3/10], Step [520/4063], Loss: 1.0492\n",
      "Epoch [3/10], Step [530/4063], Loss: 1.2137\n",
      "Epoch [3/10], Step [540/4063], Loss: 1.2223\n",
      "Epoch [3/10], Step [550/4063], Loss: 0.6755\n",
      "Epoch [3/10], Step [560/4063], Loss: 0.6351\n",
      "Epoch [3/10], Step [570/4063], Loss: 1.2222\n",
      "Epoch [3/10], Step [580/4063], Loss: 1.0243\n",
      "Epoch [3/10], Step [590/4063], Loss: 1.1415\n",
      "Epoch [3/10], Step [600/4063], Loss: 1.1332\n",
      "Epoch [3/10], Step [610/4063], Loss: 0.6228\n",
      "Epoch [3/10], Step [620/4063], Loss: 1.5548\n",
      "Epoch [3/10], Step [630/4063], Loss: 0.6598\n",
      "Epoch [3/10], Step [640/4063], Loss: 1.1310\n",
      "Epoch [3/10], Step [650/4063], Loss: 0.8448\n",
      "Epoch [3/10], Step [660/4063], Loss: 0.7911\n",
      "Epoch [3/10], Step [670/4063], Loss: 0.8776\n",
      "Epoch [3/10], Step [680/4063], Loss: 1.0375\n",
      "Epoch [3/10], Step [690/4063], Loss: 0.8207\n",
      "Epoch [3/10], Step [700/4063], Loss: 0.9640\n",
      "Epoch [3/10], Step [710/4063], Loss: 0.8656\n",
      "Epoch [3/10], Step [720/4063], Loss: 0.9801\n",
      "Epoch [3/10], Step [730/4063], Loss: 0.9548\n",
      "Epoch [3/10], Step [740/4063], Loss: 1.4890\n",
      "Epoch [3/10], Step [750/4063], Loss: 1.1501\n",
      "Epoch [3/10], Step [760/4063], Loss: 1.1624\n",
      "Epoch [3/10], Step [770/4063], Loss: 0.8922\n",
      "Epoch [3/10], Step [780/4063], Loss: 0.8202\n",
      "Epoch [3/10], Step [790/4063], Loss: 0.3912\n",
      "Epoch [3/10], Step [800/4063], Loss: 1.0777\n",
      "Epoch [3/10], Step [810/4063], Loss: 0.9439\n",
      "Epoch [3/10], Step [820/4063], Loss: 0.6353\n",
      "Epoch [3/10], Step [830/4063], Loss: 0.6669\n",
      "Epoch [3/10], Step [840/4063], Loss: 0.8033\n",
      "Epoch [3/10], Step [850/4063], Loss: 1.1876\n",
      "Epoch [3/10], Step [860/4063], Loss: 0.6732\n",
      "Epoch [3/10], Step [870/4063], Loss: 0.9062\n",
      "Epoch [3/10], Step [880/4063], Loss: 1.1518\n",
      "Epoch [3/10], Step [890/4063], Loss: 0.8585\n",
      "Epoch [3/10], Step [900/4063], Loss: 0.7169\n",
      "Epoch [3/10], Step [910/4063], Loss: 0.7163\n",
      "Epoch [3/10], Step [920/4063], Loss: 0.7588\n",
      "Epoch [3/10], Step [930/4063], Loss: 0.8591\n",
      "Epoch [3/10], Step [940/4063], Loss: 0.9914\n",
      "Epoch [3/10], Step [950/4063], Loss: 1.2695\n",
      "Epoch [3/10], Step [960/4063], Loss: 0.5500\n",
      "Epoch [3/10], Step [970/4063], Loss: 1.0077\n",
      "Epoch [3/10], Step [980/4063], Loss: 0.7157\n",
      "Epoch [3/10], Step [990/4063], Loss: 0.8401\n",
      "Epoch [3/10], Step [1000/4063], Loss: 1.0958\n",
      "Epoch [3/10], Step [1010/4063], Loss: 1.3504\n",
      "Epoch [3/10], Step [1020/4063], Loss: 0.7858\n",
      "Epoch [3/10], Step [1030/4063], Loss: 0.6073\n",
      "Epoch [3/10], Step [1040/4063], Loss: 0.7458\n",
      "Epoch [3/10], Step [1050/4063], Loss: 1.1063\n",
      "Epoch [3/10], Step [1060/4063], Loss: 0.8916\n",
      "Epoch [3/10], Step [1070/4063], Loss: 1.2297\n",
      "Epoch [3/10], Step [1080/4063], Loss: 0.8466\n",
      "Epoch [3/10], Step [1090/4063], Loss: 0.8827\n",
      "Epoch [3/10], Step [1100/4063], Loss: 0.6895\n",
      "Epoch [3/10], Step [1110/4063], Loss: 0.7608\n",
      "Epoch [3/10], Step [1120/4063], Loss: 0.7767\n",
      "Epoch [3/10], Step [1130/4063], Loss: 0.6685\n",
      "Epoch [3/10], Step [1140/4063], Loss: 1.2629\n",
      "Epoch [3/10], Step [1150/4063], Loss: 0.9214\n",
      "Epoch [3/10], Step [1160/4063], Loss: 0.9182\n",
      "Epoch [3/10], Step [1170/4063], Loss: 1.2803\n",
      "Epoch [3/10], Step [1180/4063], Loss: 1.0902\n",
      "Epoch [3/10], Step [1190/4063], Loss: 0.7542\n",
      "Epoch [3/10], Step [1200/4063], Loss: 0.8752\n",
      "Epoch [3/10], Step [1210/4063], Loss: 0.9676\n",
      "Epoch [3/10], Step [1220/4063], Loss: 0.9690\n",
      "Epoch [3/10], Step [1230/4063], Loss: 0.7239\n",
      "Epoch [3/10], Step [1240/4063], Loss: 0.7266\n",
      "Epoch [3/10], Step [1250/4063], Loss: 0.9159\n",
      "Epoch [3/10], Step [1260/4063], Loss: 0.9998\n",
      "Epoch [3/10], Step [1270/4063], Loss: 0.9556\n",
      "Epoch [3/10], Step [1280/4063], Loss: 0.6801\n",
      "Epoch [3/10], Step [1290/4063], Loss: 1.1599\n",
      "Epoch [3/10], Step [1300/4063], Loss: 1.0221\n",
      "Epoch [3/10], Step [1310/4063], Loss: 1.0281\n",
      "Epoch [3/10], Step [1320/4063], Loss: 1.1347\n",
      "Epoch [3/10], Step [1330/4063], Loss: 0.4395\n",
      "Epoch [3/10], Step [1340/4063], Loss: 0.7839\n",
      "Epoch [3/10], Step [1350/4063], Loss: 1.0802\n",
      "Epoch [3/10], Step [1360/4063], Loss: 0.5062\n",
      "Epoch [3/10], Step [1370/4063], Loss: 0.8156\n",
      "Epoch [3/10], Step [1380/4063], Loss: 0.9187\n",
      "Epoch [3/10], Step [1390/4063], Loss: 1.0102\n",
      "Epoch [3/10], Step [1400/4063], Loss: 0.5868\n",
      "Epoch [3/10], Step [1410/4063], Loss: 0.9501\n",
      "Epoch [3/10], Step [1420/4063], Loss: 0.9833\n",
      "Epoch [3/10], Step [1430/4063], Loss: 0.9027\n",
      "Epoch [3/10], Step [1440/4063], Loss: 0.2920\n",
      "Epoch [3/10], Step [1450/4063], Loss: 1.2212\n",
      "Epoch [3/10], Step [1460/4063], Loss: 0.5561\n",
      "Epoch [3/10], Step [1470/4063], Loss: 0.8083\n",
      "Epoch [3/10], Step [1480/4063], Loss: 0.8384\n",
      "Epoch [3/10], Step [1490/4063], Loss: 0.7509\n",
      "Epoch [3/10], Step [1500/4063], Loss: 1.1438\n",
      "Epoch [3/10], Step [1510/4063], Loss: 0.7742\n",
      "Epoch [3/10], Step [1520/4063], Loss: 0.7175\n",
      "Epoch [3/10], Step [1530/4063], Loss: 0.6004\n",
      "Epoch [3/10], Step [1540/4063], Loss: 0.9777\n",
      "Epoch [3/10], Step [1550/4063], Loss: 0.8483\n",
      "Epoch [3/10], Step [1560/4063], Loss: 0.8847\n",
      "Epoch [3/10], Step [1570/4063], Loss: 0.7061\n",
      "Epoch [3/10], Step [1580/4063], Loss: 0.5529\n",
      "Epoch [3/10], Step [1590/4063], Loss: 0.7921\n",
      "Epoch [3/10], Step [1600/4063], Loss: 1.0244\n",
      "Epoch [3/10], Step [1610/4063], Loss: 0.9348\n",
      "Epoch [3/10], Step [1620/4063], Loss: 1.2540\n",
      "Epoch [3/10], Step [1630/4063], Loss: 0.9237\n",
      "Epoch [3/10], Step [1640/4063], Loss: 0.7732\n",
      "Epoch [3/10], Step [1650/4063], Loss: 1.1648\n",
      "Epoch [3/10], Step [1660/4063], Loss: 1.4039\n",
      "Epoch [3/10], Step [1670/4063], Loss: 0.9020\n",
      "Epoch [3/10], Step [1680/4063], Loss: 1.2063\n",
      "Epoch [3/10], Step [1690/4063], Loss: 0.5984\n",
      "Epoch [3/10], Step [1700/4063], Loss: 1.3820\n",
      "Epoch [3/10], Step [1710/4063], Loss: 1.3092\n",
      "Epoch [3/10], Step [1720/4063], Loss: 1.0680\n",
      "Epoch [3/10], Step [1730/4063], Loss: 0.9755\n",
      "Epoch [3/10], Step [1740/4063], Loss: 0.8404\n",
      "Epoch [3/10], Step [1750/4063], Loss: 0.6191\n",
      "Epoch [3/10], Step [1760/4063], Loss: 0.8497\n",
      "Epoch [3/10], Step [1770/4063], Loss: 0.9141\n",
      "Epoch [3/10], Step [1780/4063], Loss: 0.9162\n",
      "Epoch [3/10], Step [1790/4063], Loss: 1.0198\n",
      "Epoch [3/10], Step [1800/4063], Loss: 1.3710\n",
      "Epoch [3/10], Step [1810/4063], Loss: 1.1025\n",
      "Epoch [3/10], Step [1820/4063], Loss: 0.7756\n",
      "Epoch [3/10], Step [1830/4063], Loss: 0.9744\n",
      "Epoch [3/10], Step [1840/4063], Loss: 1.0658\n",
      "Epoch [3/10], Step [1850/4063], Loss: 1.3632\n",
      "Epoch [3/10], Step [1860/4063], Loss: 0.7382\n",
      "Epoch [3/10], Step [1870/4063], Loss: 1.1784\n",
      "Epoch [3/10], Step [1880/4063], Loss: 1.0685\n",
      "Epoch [3/10], Step [1890/4063], Loss: 1.1013\n",
      "Epoch [3/10], Step [1900/4063], Loss: 0.9389\n",
      "Epoch [3/10], Step [1910/4063], Loss: 0.6971\n",
      "Epoch [3/10], Step [1920/4063], Loss: 0.7476\n",
      "Epoch [3/10], Step [1930/4063], Loss: 1.0433\n",
      "Epoch [3/10], Step [1940/4063], Loss: 1.0466\n",
      "Epoch [3/10], Step [1950/4063], Loss: 0.9961\n",
      "Epoch [3/10], Step [1960/4063], Loss: 0.9850\n",
      "Epoch [3/10], Step [1970/4063], Loss: 1.1399\n",
      "Epoch [3/10], Step [1980/4063], Loss: 1.0057\n",
      "Epoch [3/10], Step [1990/4063], Loss: 1.1589\n",
      "Epoch [3/10], Step [2000/4063], Loss: 1.2005\n",
      "Epoch [3/10], Step [2010/4063], Loss: 1.1932\n",
      "Epoch [3/10], Step [2020/4063], Loss: 0.7818\n",
      "Epoch [3/10], Step [2030/4063], Loss: 1.2266\n",
      "Epoch [3/10], Step [2040/4063], Loss: 0.9121\n",
      "Epoch [3/10], Step [2050/4063], Loss: 0.6652\n",
      "Epoch [3/10], Step [2060/4063], Loss: 1.0871\n",
      "Epoch [3/10], Step [2070/4063], Loss: 0.9289\n",
      "Epoch [3/10], Step [2080/4063], Loss: 0.8477\n",
      "Epoch [3/10], Step [2090/4063], Loss: 0.9406\n",
      "Epoch [3/10], Step [2100/4063], Loss: 1.4214\n",
      "Epoch [3/10], Step [2110/4063], Loss: 1.0028\n",
      "Epoch [3/10], Step [2120/4063], Loss: 0.8703\n",
      "Epoch [3/10], Step [2130/4063], Loss: 1.2477\n",
      "Epoch [3/10], Step [2140/4063], Loss: 0.6954\n",
      "Epoch [3/10], Step [2150/4063], Loss: 0.6180\n",
      "Epoch [3/10], Step [2160/4063], Loss: 0.7340\n",
      "Epoch [3/10], Step [2170/4063], Loss: 0.5146\n",
      "Epoch [3/10], Step [2180/4063], Loss: 1.3258\n",
      "Epoch [3/10], Step [2190/4063], Loss: 0.6594\n",
      "Epoch [3/10], Step [2200/4063], Loss: 0.9459\n",
      "Epoch [3/10], Step [2210/4063], Loss: 0.9098\n",
      "Epoch [3/10], Step [2220/4063], Loss: 0.9032\n",
      "Epoch [3/10], Step [2230/4063], Loss: 0.8274\n",
      "Epoch [3/10], Step [2240/4063], Loss: 0.6486\n",
      "Epoch [3/10], Step [2250/4063], Loss: 0.6190\n",
      "Epoch [3/10], Step [2260/4063], Loss: 0.8242\n",
      "Epoch [3/10], Step [2270/4063], Loss: 1.0296\n",
      "Epoch [3/10], Step [2280/4063], Loss: 0.6014\n",
      "Epoch [3/10], Step [2290/4063], Loss: 0.8968\n",
      "Epoch [3/10], Step [2300/4063], Loss: 1.0241\n",
      "Epoch [3/10], Step [2310/4063], Loss: 1.0958\n",
      "Epoch [3/10], Step [2320/4063], Loss: 0.6345\n",
      "Epoch [3/10], Step [2330/4063], Loss: 1.1210\n",
      "Epoch [3/10], Step [2340/4063], Loss: 0.7574\n",
      "Epoch [3/10], Step [2350/4063], Loss: 1.1572\n",
      "Epoch [3/10], Step [2360/4063], Loss: 1.1795\n",
      "Epoch [3/10], Step [2370/4063], Loss: 0.7512\n",
      "Epoch [3/10], Step [2380/4063], Loss: 1.1254\n",
      "Epoch [3/10], Step [2390/4063], Loss: 0.7929\n",
      "Epoch [3/10], Step [2400/4063], Loss: 0.6113\n",
      "Epoch [3/10], Step [2410/4063], Loss: 0.8629\n",
      "Epoch [3/10], Step [2420/4063], Loss: 1.1525\n",
      "Epoch [3/10], Step [2430/4063], Loss: 0.8113\n",
      "Epoch [3/10], Step [2440/4063], Loss: 1.2921\n",
      "Epoch [3/10], Step [2450/4063], Loss: 0.6566\n",
      "Epoch [3/10], Step [2460/4063], Loss: 0.9435\n",
      "Epoch [3/10], Step [2470/4063], Loss: 0.6185\n",
      "Epoch [3/10], Step [2480/4063], Loss: 1.1205\n",
      "Epoch [3/10], Step [2490/4063], Loss: 1.0966\n",
      "Epoch [3/10], Step [2500/4063], Loss: 0.8962\n",
      "Epoch [3/10], Step [2510/4063], Loss: 1.3903\n",
      "Epoch [3/10], Step [2520/4063], Loss: 0.5664\n",
      "Epoch [3/10], Step [2530/4063], Loss: 1.0980\n",
      "Epoch [3/10], Step [2540/4063], Loss: 1.2931\n",
      "Epoch [3/10], Step [2550/4063], Loss: 0.4879\n",
      "Epoch [3/10], Step [2560/4063], Loss: 0.6428\n",
      "Epoch [3/10], Step [2570/4063], Loss: 0.3873\n",
      "Epoch [3/10], Step [2580/4063], Loss: 1.0340\n",
      "Epoch [3/10], Step [2590/4063], Loss: 0.7084\n",
      "Epoch [3/10], Step [2600/4063], Loss: 1.2046\n",
      "Epoch [3/10], Step [2610/4063], Loss: 0.9827\n",
      "Epoch [3/10], Step [2620/4063], Loss: 0.7826\n",
      "Epoch [3/10], Step [2630/4063], Loss: 0.5797\n",
      "Epoch [3/10], Step [2640/4063], Loss: 0.6987\n",
      "Epoch [3/10], Step [2650/4063], Loss: 0.8500\n",
      "Epoch [3/10], Step [2660/4063], Loss: 1.2517\n",
      "Epoch [3/10], Step [2670/4063], Loss: 0.8767\n",
      "Epoch [3/10], Step [2680/4063], Loss: 0.7957\n",
      "Epoch [3/10], Step [2690/4063], Loss: 0.9124\n",
      "Epoch [3/10], Step [2700/4063], Loss: 0.8543\n",
      "Epoch [3/10], Step [2710/4063], Loss: 0.8963\n",
      "Epoch [3/10], Step [2720/4063], Loss: 0.7963\n",
      "Epoch [3/10], Step [2730/4063], Loss: 1.1711\n",
      "Epoch [3/10], Step [2740/4063], Loss: 0.6739\n",
      "Epoch [3/10], Step [2750/4063], Loss: 0.8840\n",
      "Epoch [3/10], Step [2760/4063], Loss: 1.1860\n",
      "Epoch [3/10], Step [2770/4063], Loss: 1.2202\n",
      "Epoch [3/10], Step [2780/4063], Loss: 0.9224\n",
      "Epoch [3/10], Step [2790/4063], Loss: 0.6816\n",
      "Epoch [3/10], Step [2800/4063], Loss: 1.1474\n",
      "Epoch [3/10], Step [2810/4063], Loss: 1.0195\n",
      "Epoch [3/10], Step [2820/4063], Loss: 0.9979\n",
      "Epoch [3/10], Step [2830/4063], Loss: 0.8321\n",
      "Epoch [3/10], Step [2840/4063], Loss: 1.2178\n",
      "Epoch [3/10], Step [2850/4063], Loss: 0.7910\n",
      "Epoch [3/10], Step [2860/4063], Loss: 0.9537\n",
      "Epoch [3/10], Step [2870/4063], Loss: 0.8248\n",
      "Epoch [3/10], Step [2880/4063], Loss: 1.0043\n",
      "Epoch [3/10], Step [2890/4063], Loss: 0.8867\n",
      "Epoch [3/10], Step [2900/4063], Loss: 0.9221\n",
      "Epoch [3/10], Step [2910/4063], Loss: 0.8564\n",
      "Epoch [3/10], Step [2920/4063], Loss: 0.4741\n",
      "Epoch [3/10], Step [2930/4063], Loss: 0.9958\n",
      "Epoch [3/10], Step [2940/4063], Loss: 0.7343\n",
      "Epoch [3/10], Step [2950/4063], Loss: 1.1270\n",
      "Epoch [3/10], Step [2960/4063], Loss: 1.1691\n",
      "Epoch [3/10], Step [2970/4063], Loss: 0.7522\n",
      "Epoch [3/10], Step [2980/4063], Loss: 0.5529\n",
      "Epoch [3/10], Step [2990/4063], Loss: 0.9531\n",
      "Epoch [3/10], Step [3000/4063], Loss: 0.8876\n",
      "Epoch [3/10], Step [3010/4063], Loss: 1.1164\n",
      "Epoch [3/10], Step [3020/4063], Loss: 0.8272\n",
      "Epoch [3/10], Step [3030/4063], Loss: 0.7020\n",
      "Epoch [3/10], Step [3040/4063], Loss: 0.9675\n",
      "Epoch [3/10], Step [3050/4063], Loss: 1.0596\n",
      "Epoch [3/10], Step [3060/4063], Loss: 0.9737\n",
      "Epoch [3/10], Step [3070/4063], Loss: 0.7465\n",
      "Epoch [3/10], Step [3080/4063], Loss: 0.7432\n",
      "Epoch [3/10], Step [3090/4063], Loss: 0.8099\n",
      "Epoch [3/10], Step [3100/4063], Loss: 1.4342\n",
      "Epoch [3/10], Step [3110/4063], Loss: 1.2815\n",
      "Epoch [3/10], Step [3120/4063], Loss: 1.0689\n",
      "Epoch [3/10], Step [3130/4063], Loss: 0.7456\n",
      "Epoch [3/10], Step [3140/4063], Loss: 0.8119\n",
      "Epoch [3/10], Step [3150/4063], Loss: 0.9704\n",
      "Epoch [3/10], Step [3160/4063], Loss: 0.9110\n",
      "Epoch [3/10], Step [3170/4063], Loss: 0.8866\n",
      "Epoch [3/10], Step [3180/4063], Loss: 1.3199\n",
      "Epoch [3/10], Step [3190/4063], Loss: 0.7771\n",
      "Epoch [3/10], Step [3200/4063], Loss: 0.6276\n",
      "Epoch [3/10], Step [3210/4063], Loss: 0.8217\n",
      "Epoch [3/10], Step [3220/4063], Loss: 0.7047\n",
      "Epoch [3/10], Step [3230/4063], Loss: 0.4287\n",
      "Epoch [3/10], Step [3240/4063], Loss: 1.0297\n",
      "Epoch [3/10], Step [3250/4063], Loss: 1.0722\n",
      "Epoch [3/10], Step [3260/4063], Loss: 0.7406\n",
      "Epoch [3/10], Step [3270/4063], Loss: 0.8626\n",
      "Epoch [3/10], Step [3280/4063], Loss: 0.7378\n",
      "Epoch [3/10], Step [3290/4063], Loss: 0.6919\n",
      "Epoch [3/10], Step [3300/4063], Loss: 0.5200\n",
      "Epoch [3/10], Step [3310/4063], Loss: 0.8799\n",
      "Epoch [3/10], Step [3320/4063], Loss: 0.6534\n",
      "Epoch [3/10], Step [3330/4063], Loss: 1.0876\n",
      "Epoch [3/10], Step [3340/4063], Loss: 0.9830\n",
      "Epoch [3/10], Step [3350/4063], Loss: 0.8049\n",
      "Epoch [3/10], Step [3360/4063], Loss: 0.8071\n",
      "Epoch [3/10], Step [3370/4063], Loss: 0.8112\n",
      "Epoch [3/10], Step [3380/4063], Loss: 1.2203\n",
      "Epoch [3/10], Step [3390/4063], Loss: 0.7275\n",
      "Epoch [3/10], Step [3400/4063], Loss: 1.0865\n",
      "Epoch [3/10], Step [3410/4063], Loss: 1.0051\n",
      "Epoch [3/10], Step [3420/4063], Loss: 0.6090\n",
      "Epoch [3/10], Step [3430/4063], Loss: 1.1625\n",
      "Epoch [3/10], Step [3440/4063], Loss: 0.8898\n",
      "Epoch [3/10], Step [3450/4063], Loss: 1.1292\n",
      "Epoch [3/10], Step [3460/4063], Loss: 0.9584\n",
      "Epoch [3/10], Step [3470/4063], Loss: 0.6844\n",
      "Epoch [3/10], Step [3480/4063], Loss: 1.1026\n",
      "Epoch [3/10], Step [3490/4063], Loss: 0.5513\n",
      "Epoch [3/10], Step [3500/4063], Loss: 0.5582\n",
      "Epoch [3/10], Step [3510/4063], Loss: 0.7643\n",
      "Epoch [3/10], Step [3520/4063], Loss: 0.7685\n",
      "Epoch [3/10], Step [3530/4063], Loss: 1.2096\n",
      "Epoch [3/10], Step [3540/4063], Loss: 0.9222\n",
      "Epoch [3/10], Step [3550/4063], Loss: 1.2748\n",
      "Epoch [3/10], Step [3560/4063], Loss: 0.8766\n",
      "Epoch [3/10], Step [3570/4063], Loss: 0.8323\n",
      "Epoch [3/10], Step [3580/4063], Loss: 0.5679\n",
      "Epoch [3/10], Step [3590/4063], Loss: 0.9829\n",
      "Epoch [3/10], Step [3600/4063], Loss: 0.9647\n",
      "Epoch [3/10], Step [3610/4063], Loss: 0.7135\n",
      "Epoch [3/10], Step [3620/4063], Loss: 0.8070\n",
      "Epoch [3/10], Step [3630/4063], Loss: 0.8830\n",
      "Epoch [3/10], Step [3640/4063], Loss: 1.0285\n",
      "Epoch [3/10], Step [3650/4063], Loss: 0.8412\n",
      "Epoch [3/10], Step [3660/4063], Loss: 0.7390\n",
      "Epoch [3/10], Step [3670/4063], Loss: 0.5008\n",
      "Epoch [3/10], Step [3680/4063], Loss: 0.6967\n",
      "Epoch [3/10], Step [3690/4063], Loss: 0.7247\n",
      "Epoch [3/10], Step [3700/4063], Loss: 0.8867\n",
      "Epoch [3/10], Step [3710/4063], Loss: 1.1266\n",
      "Epoch [3/10], Step [3720/4063], Loss: 0.6391\n",
      "Epoch [3/10], Step [3730/4063], Loss: 1.0624\n",
      "Epoch [3/10], Step [3740/4063], Loss: 0.7002\n",
      "Epoch [3/10], Step [3750/4063], Loss: 0.9085\n",
      "Epoch [3/10], Step [3760/4063], Loss: 1.3839\n",
      "Epoch [3/10], Step [3770/4063], Loss: 1.1128\n",
      "Epoch [3/10], Step [3780/4063], Loss: 0.8569\n",
      "Epoch [3/10], Step [3790/4063], Loss: 1.1091\n",
      "Epoch [3/10], Step [3800/4063], Loss: 0.9869\n",
      "Epoch [3/10], Step [3810/4063], Loss: 0.5517\n",
      "Epoch [3/10], Step [3820/4063], Loss: 1.5494\n",
      "Epoch [3/10], Step [3830/4063], Loss: 1.0789\n",
      "Epoch [3/10], Step [3840/4063], Loss: 0.9718\n",
      "Epoch [3/10], Step [3850/4063], Loss: 1.0644\n",
      "Epoch [3/10], Step [3860/4063], Loss: 0.8029\n",
      "Epoch [3/10], Step [3870/4063], Loss: 0.9207\n",
      "Epoch [3/10], Step [3880/4063], Loss: 0.8303\n",
      "Epoch [3/10], Step [3890/4063], Loss: 0.8072\n",
      "Epoch [3/10], Step [3900/4063], Loss: 1.1550\n",
      "Epoch [3/10], Step [3910/4063], Loss: 0.8767\n",
      "Epoch [3/10], Step [3920/4063], Loss: 0.8864\n",
      "Epoch [3/10], Step [3930/4063], Loss: 0.6160\n",
      "Epoch [3/10], Step [3940/4063], Loss: 0.8526\n",
      "Epoch [3/10], Step [3950/4063], Loss: 0.6284\n",
      "Epoch [3/10], Step [3960/4063], Loss: 0.9115\n",
      "Epoch [3/10], Step [3970/4063], Loss: 1.0457\n",
      "Epoch [3/10], Step [3980/4063], Loss: 0.7175\n",
      "Epoch [3/10], Step [3990/4063], Loss: 1.3204\n",
      "Epoch [3/10], Step [4000/4063], Loss: 1.1705\n",
      "Epoch [3/10], Step [4010/4063], Loss: 0.7155\n",
      "Epoch [3/10], Step [4020/4063], Loss: 0.5974\n",
      "Epoch [3/10], Step [4030/4063], Loss: 0.7316\n",
      "Epoch [3/10], Step [4040/4063], Loss: 0.9697\n",
      "Epoch [3/10], Step [4050/4063], Loss: 0.9123\n",
      "Epoch [3/10], Step [4060/4063], Loss: 0.8030\n",
      "Epoch [4/10], Step [10/4063], Loss: 0.8289\n",
      "Epoch [4/10], Step [20/4063], Loss: 0.8472\n",
      "Epoch [4/10], Step [30/4063], Loss: 0.8702\n",
      "Epoch [4/10], Step [40/4063], Loss: 0.5636\n",
      "Epoch [4/10], Step [50/4063], Loss: 1.2279\n",
      "Epoch [4/10], Step [60/4063], Loss: 0.7371\n",
      "Epoch [4/10], Step [70/4063], Loss: 0.9932\n",
      "Epoch [4/10], Step [80/4063], Loss: 1.1463\n",
      "Epoch [4/10], Step [90/4063], Loss: 1.1281\n",
      "Epoch [4/10], Step [100/4063], Loss: 0.8418\n",
      "Epoch [4/10], Step [110/4063], Loss: 0.6891\n",
      "Epoch [4/10], Step [120/4063], Loss: 0.8262\n",
      "Epoch [4/10], Step [130/4063], Loss: 0.7279\n",
      "Epoch [4/10], Step [140/4063], Loss: 0.6620\n",
      "Epoch [4/10], Step [150/4063], Loss: 0.6263\n",
      "Epoch [4/10], Step [160/4063], Loss: 0.8790\n",
      "Epoch [4/10], Step [170/4063], Loss: 0.9828\n",
      "Epoch [4/10], Step [180/4063], Loss: 0.8366\n",
      "Epoch [4/10], Step [190/4063], Loss: 0.7450\n",
      "Epoch [4/10], Step [200/4063], Loss: 1.0515\n",
      "Epoch [4/10], Step [210/4063], Loss: 1.1786\n",
      "Epoch [4/10], Step [220/4063], Loss: 0.9170\n",
      "Epoch [4/10], Step [230/4063], Loss: 1.0002\n",
      "Epoch [4/10], Step [240/4063], Loss: 0.9499\n",
      "Epoch [4/10], Step [250/4063], Loss: 0.7567\n",
      "Epoch [4/10], Step [260/4063], Loss: 0.6565\n",
      "Epoch [4/10], Step [270/4063], Loss: 0.7255\n",
      "Epoch [4/10], Step [280/4063], Loss: 0.6385\n",
      "Epoch [4/10], Step [290/4063], Loss: 1.0104\n",
      "Epoch [4/10], Step [300/4063], Loss: 0.6624\n",
      "Epoch [4/10], Step [310/4063], Loss: 0.7450\n",
      "Epoch [4/10], Step [320/4063], Loss: 1.6202\n",
      "Epoch [4/10], Step [330/4063], Loss: 1.3300\n",
      "Epoch [4/10], Step [340/4063], Loss: 0.8478\n",
      "Epoch [4/10], Step [350/4063], Loss: 0.7404\n",
      "Epoch [4/10], Step [360/4063], Loss: 0.9728\n",
      "Epoch [4/10], Step [370/4063], Loss: 1.1690\n",
      "Epoch [4/10], Step [380/4063], Loss: 1.1015\n",
      "Epoch [4/10], Step [390/4063], Loss: 0.4127\n",
      "Epoch [4/10], Step [400/4063], Loss: 1.0005\n",
      "Epoch [4/10], Step [410/4063], Loss: 0.7278\n",
      "Epoch [4/10], Step [420/4063], Loss: 0.9685\n",
      "Epoch [4/10], Step [430/4063], Loss: 0.8130\n",
      "Epoch [4/10], Step [440/4063], Loss: 0.8177\n",
      "Epoch [4/10], Step [450/4063], Loss: 0.7502\n",
      "Epoch [4/10], Step [460/4063], Loss: 1.1259\n",
      "Epoch [4/10], Step [470/4063], Loss: 0.7564\n",
      "Epoch [4/10], Step [480/4063], Loss: 1.0872\n",
      "Epoch [4/10], Step [490/4063], Loss: 1.1890\n",
      "Epoch [4/10], Step [500/4063], Loss: 0.7719\n",
      "Epoch [4/10], Step [510/4063], Loss: 1.0743\n",
      "Epoch [4/10], Step [520/4063], Loss: 1.0045\n",
      "Epoch [4/10], Step [530/4063], Loss: 0.7531\n",
      "Epoch [4/10], Step [540/4063], Loss: 1.2234\n",
      "Epoch [4/10], Step [550/4063], Loss: 0.7347\n",
      "Epoch [4/10], Step [560/4063], Loss: 0.9953\n",
      "Epoch [4/10], Step [570/4063], Loss: 1.0133\n",
      "Epoch [4/10], Step [580/4063], Loss: 0.8967\n",
      "Epoch [4/10], Step [590/4063], Loss: 0.8969\n",
      "Epoch [4/10], Step [600/4063], Loss: 1.1953\n",
      "Epoch [4/10], Step [610/4063], Loss: 0.7914\n",
      "Epoch [4/10], Step [620/4063], Loss: 0.6793\n",
      "Epoch [4/10], Step [630/4063], Loss: 0.8605\n",
      "Epoch [4/10], Step [640/4063], Loss: 0.6784\n",
      "Epoch [4/10], Step [650/4063], Loss: 0.9796\n",
      "Epoch [4/10], Step [660/4063], Loss: 0.8320\n",
      "Epoch [4/10], Step [670/4063], Loss: 0.8774\n",
      "Epoch [4/10], Step [680/4063], Loss: 0.7366\n",
      "Epoch [4/10], Step [690/4063], Loss: 0.7798\n",
      "Epoch [4/10], Step [700/4063], Loss: 1.2167\n",
      "Epoch [4/10], Step [710/4063], Loss: 0.7708\n",
      "Epoch [4/10], Step [720/4063], Loss: 0.9663\n",
      "Epoch [4/10], Step [730/4063], Loss: 1.1625\n",
      "Epoch [4/10], Step [740/4063], Loss: 0.8290\n",
      "Epoch [4/10], Step [750/4063], Loss: 0.7883\n",
      "Epoch [4/10], Step [760/4063], Loss: 0.6178\n",
      "Epoch [4/10], Step [770/4063], Loss: 0.5359\n",
      "Epoch [4/10], Step [780/4063], Loss: 0.9484\n",
      "Epoch [4/10], Step [790/4063], Loss: 1.1403\n",
      "Epoch [4/10], Step [800/4063], Loss: 0.9511\n",
      "Epoch [4/10], Step [810/4063], Loss: 0.8107\n",
      "Epoch [4/10], Step [820/4063], Loss: 0.8931\n",
      "Epoch [4/10], Step [830/4063], Loss: 1.1371\n",
      "Epoch [4/10], Step [840/4063], Loss: 1.2395\n",
      "Epoch [4/10], Step [850/4063], Loss: 0.9245\n",
      "Epoch [4/10], Step [860/4063], Loss: 0.7363\n",
      "Epoch [4/10], Step [870/4063], Loss: 0.9724\n",
      "Epoch [4/10], Step [880/4063], Loss: 1.1081\n",
      "Epoch [4/10], Step [890/4063], Loss: 1.1092\n",
      "Epoch [4/10], Step [900/4063], Loss: 1.3755\n",
      "Epoch [4/10], Step [910/4063], Loss: 0.9435\n",
      "Epoch [4/10], Step [920/4063], Loss: 0.9120\n",
      "Epoch [4/10], Step [930/4063], Loss: 0.6099\n",
      "Epoch [4/10], Step [940/4063], Loss: 0.6977\n",
      "Epoch [4/10], Step [950/4063], Loss: 0.8725\n",
      "Epoch [4/10], Step [960/4063], Loss: 0.7354\n",
      "Epoch [4/10], Step [970/4063], Loss: 0.7275\n",
      "Epoch [4/10], Step [980/4063], Loss: 0.7317\n",
      "Epoch [4/10], Step [990/4063], Loss: 0.7024\n",
      "Epoch [4/10], Step [1000/4063], Loss: 1.2017\n",
      "Epoch [4/10], Step [1010/4063], Loss: 1.0489\n",
      "Epoch [4/10], Step [1020/4063], Loss: 0.8238\n",
      "Epoch [4/10], Step [1030/4063], Loss: 0.8855\n",
      "Epoch [4/10], Step [1040/4063], Loss: 1.1614\n",
      "Epoch [4/10], Step [1050/4063], Loss: 1.2152\n",
      "Epoch [4/10], Step [1060/4063], Loss: 0.5568\n",
      "Epoch [4/10], Step [1070/4063], Loss: 0.7914\n",
      "Epoch [4/10], Step [1080/4063], Loss: 1.0476\n",
      "Epoch [4/10], Step [1090/4063], Loss: 1.2706\n",
      "Epoch [4/10], Step [1100/4063], Loss: 0.9755\n",
      "Epoch [4/10], Step [1110/4063], Loss: 0.9447\n",
      "Epoch [4/10], Step [1120/4063], Loss: 1.3030\n",
      "Epoch [4/10], Step [1130/4063], Loss: 0.5808\n",
      "Epoch [4/10], Step [1140/4063], Loss: 0.9387\n",
      "Epoch [4/10], Step [1150/4063], Loss: 0.6721\n",
      "Epoch [4/10], Step [1160/4063], Loss: 1.0233\n",
      "Epoch [4/10], Step [1170/4063], Loss: 0.8462\n",
      "Epoch [4/10], Step [1180/4063], Loss: 1.0596\n",
      "Epoch [4/10], Step [1190/4063], Loss: 0.8492\n",
      "Epoch [4/10], Step [1200/4063], Loss: 1.0396\n",
      "Epoch [4/10], Step [1210/4063], Loss: 1.1591\n",
      "Epoch [4/10], Step [1220/4063], Loss: 1.0531\n",
      "Epoch [4/10], Step [1230/4063], Loss: 0.6354\n",
      "Epoch [4/10], Step [1240/4063], Loss: 0.7297\n",
      "Epoch [4/10], Step [1250/4063], Loss: 1.3519\n",
      "Epoch [4/10], Step [1260/4063], Loss: 1.0252\n",
      "Epoch [4/10], Step [1270/4063], Loss: 0.7501\n",
      "Epoch [4/10], Step [1280/4063], Loss: 0.9150\n",
      "Epoch [4/10], Step [1290/4063], Loss: 1.0035\n",
      "Epoch [4/10], Step [1300/4063], Loss: 1.0012\n",
      "Epoch [4/10], Step [1310/4063], Loss: 0.7989\n",
      "Epoch [4/10], Step [1320/4063], Loss: 0.6403\n",
      "Epoch [4/10], Step [1330/4063], Loss: 1.0645\n",
      "Epoch [4/10], Step [1340/4063], Loss: 0.7642\n",
      "Epoch [4/10], Step [1350/4063], Loss: 0.8091\n",
      "Epoch [4/10], Step [1360/4063], Loss: 1.1560\n",
      "Epoch [4/10], Step [1370/4063], Loss: 0.6461\n",
      "Epoch [4/10], Step [1380/4063], Loss: 1.3421\n",
      "Epoch [4/10], Step [1390/4063], Loss: 0.7127\n",
      "Epoch [4/10], Step [1400/4063], Loss: 0.9787\n",
      "Epoch [4/10], Step [1410/4063], Loss: 1.5034\n",
      "Epoch [4/10], Step [1420/4063], Loss: 0.9322\n",
      "Epoch [4/10], Step [1430/4063], Loss: 1.0627\n",
      "Epoch [4/10], Step [1440/4063], Loss: 0.6959\n",
      "Epoch [4/10], Step [1450/4063], Loss: 0.6546\n",
      "Epoch [4/10], Step [1460/4063], Loss: 1.0802\n",
      "Epoch [4/10], Step [1470/4063], Loss: 0.7432\n",
      "Epoch [4/10], Step [1480/4063], Loss: 0.7622\n",
      "Epoch [4/10], Step [1490/4063], Loss: 0.6811\n",
      "Epoch [4/10], Step [1500/4063], Loss: 0.6858\n",
      "Epoch [4/10], Step [1510/4063], Loss: 0.4460\n",
      "Epoch [4/10], Step [1520/4063], Loss: 1.0977\n",
      "Epoch [4/10], Step [1530/4063], Loss: 1.0242\n",
      "Epoch [4/10], Step [1540/4063], Loss: 0.8329\n",
      "Epoch [4/10], Step [1550/4063], Loss: 0.9658\n",
      "Epoch [4/10], Step [1560/4063], Loss: 1.0076\n",
      "Epoch [4/10], Step [1570/4063], Loss: 1.0598\n",
      "Epoch [4/10], Step [1580/4063], Loss: 0.7297\n",
      "Epoch [4/10], Step [1590/4063], Loss: 1.1509\n",
      "Epoch [4/10], Step [1600/4063], Loss: 1.2485\n",
      "Epoch [4/10], Step [1610/4063], Loss: 0.9940\n",
      "Epoch [4/10], Step [1620/4063], Loss: 0.9388\n",
      "Epoch [4/10], Step [1630/4063], Loss: 0.4290\n",
      "Epoch [4/10], Step [1640/4063], Loss: 1.6182\n",
      "Epoch [4/10], Step [1650/4063], Loss: 1.3355\n",
      "Epoch [4/10], Step [1660/4063], Loss: 1.0080\n",
      "Epoch [4/10], Step [1670/4063], Loss: 0.9159\n",
      "Epoch [4/10], Step [1680/4063], Loss: 0.9745\n",
      "Epoch [4/10], Step [1690/4063], Loss: 1.0688\n",
      "Epoch [4/10], Step [1700/4063], Loss: 0.8355\n",
      "Epoch [4/10], Step [1710/4063], Loss: 0.7703\n",
      "Epoch [4/10], Step [1720/4063], Loss: 0.8681\n",
      "Epoch [4/10], Step [1730/4063], Loss: 1.2755\n",
      "Epoch [4/10], Step [1740/4063], Loss: 0.6091\n",
      "Epoch [4/10], Step [1750/4063], Loss: 0.8403\n",
      "Epoch [4/10], Step [1760/4063], Loss: 0.9241\n",
      "Epoch [4/10], Step [1770/4063], Loss: 0.8391\n",
      "Epoch [4/10], Step [1780/4063], Loss: 0.9751\n",
      "Epoch [4/10], Step [1790/4063], Loss: 1.1777\n",
      "Epoch [4/10], Step [1800/4063], Loss: 0.6449\n",
      "Epoch [4/10], Step [1810/4063], Loss: 0.9809\n",
      "Epoch [4/10], Step [1820/4063], Loss: 1.0507\n",
      "Epoch [4/10], Step [1830/4063], Loss: 0.5548\n",
      "Epoch [4/10], Step [1840/4063], Loss: 1.0199\n",
      "Epoch [4/10], Step [1850/4063], Loss: 0.8428\n",
      "Epoch [4/10], Step [1860/4063], Loss: 0.6720\n",
      "Epoch [4/10], Step [1870/4063], Loss: 1.3202\n",
      "Epoch [4/10], Step [1880/4063], Loss: 1.0484\n",
      "Epoch [4/10], Step [1890/4063], Loss: 0.8095\n",
      "Epoch [4/10], Step [1900/4063], Loss: 0.9842\n",
      "Epoch [4/10], Step [1910/4063], Loss: 0.9130\n",
      "Epoch [4/10], Step [1920/4063], Loss: 0.9149\n",
      "Epoch [4/10], Step [1930/4063], Loss: 0.9620\n",
      "Epoch [4/10], Step [1940/4063], Loss: 0.8075\n",
      "Epoch [4/10], Step [1950/4063], Loss: 1.0320\n",
      "Epoch [4/10], Step [1960/4063], Loss: 0.8069\n",
      "Epoch [4/10], Step [1970/4063], Loss: 1.2523\n",
      "Epoch [4/10], Step [1980/4063], Loss: 1.1696\n",
      "Epoch [4/10], Step [1990/4063], Loss: 0.8252\n",
      "Epoch [4/10], Step [2000/4063], Loss: 1.3398\n",
      "Epoch [4/10], Step [2010/4063], Loss: 0.6767\n",
      "Epoch [4/10], Step [2020/4063], Loss: 0.8881\n",
      "Epoch [4/10], Step [2030/4063], Loss: 0.8187\n",
      "Epoch [4/10], Step [2040/4063], Loss: 1.0879\n",
      "Epoch [4/10], Step [2050/4063], Loss: 0.9629\n",
      "Epoch [4/10], Step [2060/4063], Loss: 1.0639\n",
      "Epoch [4/10], Step [2070/4063], Loss: 0.9522\n",
      "Epoch [4/10], Step [2080/4063], Loss: 0.8744\n",
      "Epoch [4/10], Step [2090/4063], Loss: 0.8147\n",
      "Epoch [4/10], Step [2100/4063], Loss: 0.7500\n",
      "Epoch [4/10], Step [2110/4063], Loss: 1.3869\n",
      "Epoch [4/10], Step [2120/4063], Loss: 1.2775\n",
      "Epoch [4/10], Step [2130/4063], Loss: 0.8170\n",
      "Epoch [4/10], Step [2140/4063], Loss: 1.1212\n",
      "Epoch [4/10], Step [2150/4063], Loss: 0.7266\n",
      "Epoch [4/10], Step [2160/4063], Loss: 0.5128\n",
      "Epoch [4/10], Step [2170/4063], Loss: 0.8817\n",
      "Epoch [4/10], Step [2180/4063], Loss: 1.0954\n",
      "Epoch [4/10], Step [2190/4063], Loss: 0.9348\n",
      "Epoch [4/10], Step [2200/4063], Loss: 0.8383\n",
      "Epoch [4/10], Step [2210/4063], Loss: 1.4649\n",
      "Epoch [4/10], Step [2220/4063], Loss: 0.8108\n",
      "Epoch [4/10], Step [2230/4063], Loss: 1.2433\n",
      "Epoch [4/10], Step [2240/4063], Loss: 0.8533\n",
      "Epoch [4/10], Step [2250/4063], Loss: 0.8085\n",
      "Epoch [4/10], Step [2260/4063], Loss: 0.9251\n",
      "Epoch [4/10], Step [2270/4063], Loss: 0.6804\n",
      "Epoch [4/10], Step [2280/4063], Loss: 0.7371\n",
      "Epoch [4/10], Step [2290/4063], Loss: 0.8054\n",
      "Epoch [4/10], Step [2300/4063], Loss: 0.8789\n",
      "Epoch [4/10], Step [2310/4063], Loss: 0.7297\n",
      "Epoch [4/10], Step [2320/4063], Loss: 0.8374\n",
      "Epoch [4/10], Step [2330/4063], Loss: 0.6901\n",
      "Epoch [4/10], Step [2340/4063], Loss: 0.7834\n",
      "Epoch [4/10], Step [2350/4063], Loss: 0.8995\n",
      "Epoch [4/10], Step [2360/4063], Loss: 0.5824\n",
      "Epoch [4/10], Step [2370/4063], Loss: 1.2982\n",
      "Epoch [4/10], Step [2380/4063], Loss: 0.8008\n",
      "Epoch [4/10], Step [2390/4063], Loss: 1.2630\n",
      "Epoch [4/10], Step [2400/4063], Loss: 1.3391\n",
      "Epoch [4/10], Step [2410/4063], Loss: 1.3841\n",
      "Epoch [4/10], Step [2420/4063], Loss: 0.6803\n",
      "Epoch [4/10], Step [2430/4063], Loss: 1.0689\n",
      "Epoch [4/10], Step [2440/4063], Loss: 1.1739\n",
      "Epoch [4/10], Step [2450/4063], Loss: 1.0677\n",
      "Epoch [4/10], Step [2460/4063], Loss: 0.7399\n",
      "Epoch [4/10], Step [2470/4063], Loss: 1.1404\n",
      "Epoch [4/10], Step [2480/4063], Loss: 0.7384\n",
      "Epoch [4/10], Step [2490/4063], Loss: 1.0931\n",
      "Epoch [4/10], Step [2500/4063], Loss: 0.6077\n",
      "Epoch [4/10], Step [2510/4063], Loss: 1.1836\n",
      "Epoch [4/10], Step [2520/4063], Loss: 1.0059\n",
      "Epoch [4/10], Step [2530/4063], Loss: 0.9202\n",
      "Epoch [4/10], Step [2540/4063], Loss: 1.1926\n",
      "Epoch [4/10], Step [2550/4063], Loss: 0.7902\n",
      "Epoch [4/10], Step [2560/4063], Loss: 0.9801\n",
      "Epoch [4/10], Step [2570/4063], Loss: 0.8376\n",
      "Epoch [4/10], Step [2580/4063], Loss: 1.3528\n",
      "Epoch [4/10], Step [2590/4063], Loss: 0.7023\n",
      "Epoch [4/10], Step [2600/4063], Loss: 0.9057\n",
      "Epoch [4/10], Step [2610/4063], Loss: 0.8854\n",
      "Epoch [4/10], Step [2620/4063], Loss: 0.7405\n",
      "Epoch [4/10], Step [2630/4063], Loss: 0.8384\n",
      "Epoch [4/10], Step [2640/4063], Loss: 0.8092\n",
      "Epoch [4/10], Step [2650/4063], Loss: 1.1007\n",
      "Epoch [4/10], Step [2660/4063], Loss: 0.8673\n",
      "Epoch [4/10], Step [2670/4063], Loss: 0.6665\n",
      "Epoch [4/10], Step [2680/4063], Loss: 1.1240\n",
      "Epoch [4/10], Step [2690/4063], Loss: 0.8038\n",
      "Epoch [4/10], Step [2700/4063], Loss: 1.0126\n",
      "Epoch [4/10], Step [2710/4063], Loss: 0.8740\n",
      "Epoch [4/10], Step [2720/4063], Loss: 0.8402\n",
      "Epoch [4/10], Step [2730/4063], Loss: 1.2713\n",
      "Epoch [4/10], Step [2740/4063], Loss: 1.0728\n",
      "Epoch [4/10], Step [2750/4063], Loss: 0.6537\n",
      "Epoch [4/10], Step [2760/4063], Loss: 1.0564\n",
      "Epoch [4/10], Step [2770/4063], Loss: 0.9484\n",
      "Epoch [4/10], Step [2780/4063], Loss: 0.7237\n",
      "Epoch [4/10], Step [2790/4063], Loss: 1.0604\n",
      "Epoch [4/10], Step [2800/4063], Loss: 0.8265\n",
      "Epoch [4/10], Step [2810/4063], Loss: 1.1199\n",
      "Epoch [4/10], Step [2820/4063], Loss: 0.8504\n",
      "Epoch [4/10], Step [2830/4063], Loss: 0.8585\n",
      "Epoch [4/10], Step [2840/4063], Loss: 0.9903\n",
      "Epoch [4/10], Step [2850/4063], Loss: 0.9411\n",
      "Epoch [4/10], Step [2860/4063], Loss: 1.2120\n",
      "Epoch [4/10], Step [2870/4063], Loss: 0.9264\n",
      "Epoch [4/10], Step [2880/4063], Loss: 0.8441\n",
      "Epoch [4/10], Step [2890/4063], Loss: 0.9912\n",
      "Epoch [4/10], Step [2900/4063], Loss: 0.5603\n",
      "Epoch [4/10], Step [2910/4063], Loss: 1.1150\n",
      "Epoch [4/10], Step [2920/4063], Loss: 0.9606\n",
      "Epoch [4/10], Step [2930/4063], Loss: 0.8809\n",
      "Epoch [4/10], Step [2940/4063], Loss: 0.8411\n",
      "Epoch [4/10], Step [2950/4063], Loss: 0.8944\n",
      "Epoch [4/10], Step [2960/4063], Loss: 0.7268\n",
      "Epoch [4/10], Step [2970/4063], Loss: 0.9513\n",
      "Epoch [4/10], Step [2980/4063], Loss: 0.9720\n",
      "Epoch [4/10], Step [2990/4063], Loss: 1.0264\n",
      "Epoch [4/10], Step [3000/4063], Loss: 1.0578\n",
      "Epoch [4/10], Step [3010/4063], Loss: 0.8148\n",
      "Epoch [4/10], Step [3020/4063], Loss: 0.5461\n",
      "Epoch [4/10], Step [3030/4063], Loss: 0.8272\n",
      "Epoch [4/10], Step [3040/4063], Loss: 0.9183\n",
      "Epoch [4/10], Step [3050/4063], Loss: 1.0810\n",
      "Epoch [4/10], Step [3060/4063], Loss: 1.1318\n",
      "Epoch [4/10], Step [3070/4063], Loss: 1.0131\n",
      "Epoch [4/10], Step [3080/4063], Loss: 0.7566\n",
      "Epoch [4/10], Step [3090/4063], Loss: 1.5917\n",
      "Epoch [4/10], Step [3100/4063], Loss: 0.4244\n",
      "Epoch [4/10], Step [3110/4063], Loss: 1.3900\n",
      "Epoch [4/10], Step [3120/4063], Loss: 0.7588\n",
      "Epoch [4/10], Step [3130/4063], Loss: 1.0874\n",
      "Epoch [4/10], Step [3140/4063], Loss: 1.1477\n",
      "Epoch [4/10], Step [3150/4063], Loss: 1.0400\n",
      "Epoch [4/10], Step [3160/4063], Loss: 0.7096\n",
      "Epoch [4/10], Step [3170/4063], Loss: 0.9804\n",
      "Epoch [4/10], Step [3180/4063], Loss: 1.1785\n",
      "Epoch [4/10], Step [3190/4063], Loss: 1.0071\n",
      "Epoch [4/10], Step [3200/4063], Loss: 1.1728\n",
      "Epoch [4/10], Step [3210/4063], Loss: 0.7604\n",
      "Epoch [4/10], Step [3220/4063], Loss: 0.8150\n",
      "Epoch [4/10], Step [3230/4063], Loss: 1.3480\n",
      "Epoch [4/10], Step [3240/4063], Loss: 0.8986\n",
      "Epoch [4/10], Step [3250/4063], Loss: 0.9997\n",
      "Epoch [4/10], Step [3260/4063], Loss: 1.0320\n",
      "Epoch [4/10], Step [3270/4063], Loss: 0.9666\n",
      "Epoch [4/10], Step [3280/4063], Loss: 0.9297\n",
      "Epoch [4/10], Step [3290/4063], Loss: 0.8910\n",
      "Epoch [4/10], Step [3300/4063], Loss: 0.7899\n",
      "Epoch [4/10], Step [3310/4063], Loss: 0.5666\n",
      "Epoch [4/10], Step [3320/4063], Loss: 0.7573\n",
      "Epoch [4/10], Step [3330/4063], Loss: 0.9748\n",
      "Epoch [4/10], Step [3340/4063], Loss: 1.1777\n",
      "Epoch [4/10], Step [3350/4063], Loss: 0.8016\n",
      "Epoch [4/10], Step [3360/4063], Loss: 1.1650\n",
      "Epoch [4/10], Step [3370/4063], Loss: 0.7102\n",
      "Epoch [4/10], Step [3380/4063], Loss: 0.8373\n",
      "Epoch [4/10], Step [3390/4063], Loss: 0.9495\n",
      "Epoch [4/10], Step [3400/4063], Loss: 0.5176\n",
      "Epoch [4/10], Step [3410/4063], Loss: 0.9997\n",
      "Epoch [4/10], Step [3420/4063], Loss: 1.0836\n",
      "Epoch [4/10], Step [3430/4063], Loss: 0.7399\n",
      "Epoch [4/10], Step [3440/4063], Loss: 0.9675\n",
      "Epoch [4/10], Step [3450/4063], Loss: 1.1275\n",
      "Epoch [4/10], Step [3460/4063], Loss: 0.9257\n",
      "Epoch [4/10], Step [3470/4063], Loss: 1.0084\n",
      "Epoch [4/10], Step [3480/4063], Loss: 1.0310\n",
      "Epoch [4/10], Step [3490/4063], Loss: 0.5882\n",
      "Epoch [4/10], Step [3500/4063], Loss: 1.2772\n",
      "Epoch [4/10], Step [3510/4063], Loss: 1.0531\n",
      "Epoch [4/10], Step [3520/4063], Loss: 0.8022\n",
      "Epoch [4/10], Step [3530/4063], Loss: 0.4446\n",
      "Epoch [4/10], Step [3540/4063], Loss: 0.7211\n",
      "Epoch [4/10], Step [3550/4063], Loss: 0.8506\n",
      "Epoch [4/10], Step [3560/4063], Loss: 0.4909\n",
      "Epoch [4/10], Step [3570/4063], Loss: 0.8071\n",
      "Epoch [4/10], Step [3580/4063], Loss: 1.0366\n",
      "Epoch [4/10], Step [3590/4063], Loss: 1.0501\n",
      "Epoch [4/10], Step [3600/4063], Loss: 1.0630\n",
      "Epoch [4/10], Step [3610/4063], Loss: 0.9336\n",
      "Epoch [4/10], Step [3620/4063], Loss: 1.0308\n",
      "Epoch [4/10], Step [3630/4063], Loss: 0.5689\n",
      "Epoch [4/10], Step [3640/4063], Loss: 0.6179\n",
      "Epoch [4/10], Step [3650/4063], Loss: 0.6047\n",
      "Epoch [4/10], Step [3660/4063], Loss: 1.0777\n",
      "Epoch [4/10], Step [3670/4063], Loss: 1.1284\n",
      "Epoch [4/10], Step [3680/4063], Loss: 0.7039\n",
      "Epoch [4/10], Step [3690/4063], Loss: 0.9345\n",
      "Epoch [4/10], Step [3700/4063], Loss: 0.7694\n",
      "Epoch [4/10], Step [3710/4063], Loss: 1.1833\n",
      "Epoch [4/10], Step [3720/4063], Loss: 1.0388\n",
      "Epoch [4/10], Step [3730/4063], Loss: 1.1546\n",
      "Epoch [4/10], Step [3740/4063], Loss: 1.0444\n",
      "Epoch [4/10], Step [3750/4063], Loss: 0.9867\n",
      "Epoch [4/10], Step [3760/4063], Loss: 1.1525\n",
      "Epoch [4/10], Step [3770/4063], Loss: 1.1208\n",
      "Epoch [4/10], Step [3780/4063], Loss: 0.8180\n",
      "Epoch [4/10], Step [3790/4063], Loss: 0.8274\n",
      "Epoch [4/10], Step [3800/4063], Loss: 0.6235\n",
      "Epoch [4/10], Step [3810/4063], Loss: 1.2515\n",
      "Epoch [4/10], Step [3820/4063], Loss: 0.8848\n",
      "Epoch [4/10], Step [3830/4063], Loss: 0.6508\n",
      "Epoch [4/10], Step [3840/4063], Loss: 0.4876\n",
      "Epoch [4/10], Step [3850/4063], Loss: 0.7296\n",
      "Epoch [4/10], Step [3860/4063], Loss: 0.7782\n",
      "Epoch [4/10], Step [3870/4063], Loss: 1.1009\n",
      "Epoch [4/10], Step [3880/4063], Loss: 0.8837\n",
      "Epoch [4/10], Step [3890/4063], Loss: 0.9711\n",
      "Epoch [4/10], Step [3900/4063], Loss: 0.7544\n",
      "Epoch [4/10], Step [3910/4063], Loss: 0.9960\n",
      "Epoch [4/10], Step [3920/4063], Loss: 0.7368\n",
      "Epoch [4/10], Step [3930/4063], Loss: 1.1822\n",
      "Epoch [4/10], Step [3940/4063], Loss: 0.6736\n",
      "Epoch [4/10], Step [3950/4063], Loss: 0.6945\n",
      "Epoch [4/10], Step [3960/4063], Loss: 0.7415\n",
      "Epoch [4/10], Step [3970/4063], Loss: 0.9489\n",
      "Epoch [4/10], Step [3980/4063], Loss: 0.7416\n",
      "Epoch [4/10], Step [3990/4063], Loss: 1.0285\n",
      "Epoch [4/10], Step [4000/4063], Loss: 0.9899\n",
      "Epoch [4/10], Step [4010/4063], Loss: 0.9085\n",
      "Epoch [4/10], Step [4020/4063], Loss: 1.1080\n",
      "Epoch [4/10], Step [4030/4063], Loss: 0.8109\n",
      "Epoch [4/10], Step [4040/4063], Loss: 0.9064\n",
      "Epoch [4/10], Step [4050/4063], Loss: 0.7124\n",
      "Epoch [4/10], Step [4060/4063], Loss: 0.7236\n",
      "Epoch [5/10], Step [10/4063], Loss: 1.6807\n",
      "Epoch [5/10], Step [20/4063], Loss: 0.7929\n",
      "Epoch [5/10], Step [30/4063], Loss: 0.7873\n",
      "Epoch [5/10], Step [40/4063], Loss: 1.3042\n",
      "Epoch [5/10], Step [50/4063], Loss: 1.0534\n",
      "Epoch [5/10], Step [60/4063], Loss: 0.8520\n",
      "Epoch [5/10], Step [70/4063], Loss: 1.1483\n",
      "Epoch [5/10], Step [80/4063], Loss: 0.8604\n",
      "Epoch [5/10], Step [90/4063], Loss: 0.7097\n",
      "Epoch [5/10], Step [100/4063], Loss: 0.7858\n",
      "Epoch [5/10], Step [110/4063], Loss: 0.8539\n",
      "Epoch [5/10], Step [120/4063], Loss: 0.7929\n",
      "Epoch [5/10], Step [130/4063], Loss: 1.1106\n",
      "Epoch [5/10], Step [140/4063], Loss: 0.8797\n",
      "Epoch [5/10], Step [150/4063], Loss: 1.0851\n",
      "Epoch [5/10], Step [160/4063], Loss: 1.1537\n",
      "Epoch [5/10], Step [170/4063], Loss: 1.2403\n",
      "Epoch [5/10], Step [180/4063], Loss: 0.7850\n",
      "Epoch [5/10], Step [190/4063], Loss: 1.0348\n",
      "Epoch [5/10], Step [200/4063], Loss: 0.7036\n",
      "Epoch [5/10], Step [210/4063], Loss: 1.1075\n",
      "Epoch [5/10], Step [220/4063], Loss: 1.2272\n",
      "Epoch [5/10], Step [230/4063], Loss: 1.0535\n",
      "Epoch [5/10], Step [240/4063], Loss: 1.2003\n",
      "Epoch [5/10], Step [250/4063], Loss: 0.7529\n",
      "Epoch [5/10], Step [260/4063], Loss: 0.7027\n",
      "Epoch [5/10], Step [270/4063], Loss: 1.2444\n",
      "Epoch [5/10], Step [280/4063], Loss: 0.6936\n",
      "Epoch [5/10], Step [290/4063], Loss: 0.7901\n",
      "Epoch [5/10], Step [300/4063], Loss: 1.1918\n",
      "Epoch [5/10], Step [310/4063], Loss: 0.8381\n",
      "Epoch [5/10], Step [320/4063], Loss: 0.6301\n",
      "Epoch [5/10], Step [330/4063], Loss: 0.8014\n",
      "Epoch [5/10], Step [340/4063], Loss: 0.9264\n",
      "Epoch [5/10], Step [350/4063], Loss: 1.2767\n",
      "Epoch [5/10], Step [360/4063], Loss: 0.9114\n",
      "Epoch [5/10], Step [370/4063], Loss: 1.0829\n",
      "Epoch [5/10], Step [380/4063], Loss: 0.6322\n",
      "Epoch [5/10], Step [390/4063], Loss: 1.3646\n",
      "Epoch [5/10], Step [400/4063], Loss: 0.9846\n",
      "Epoch [5/10], Step [410/4063], Loss: 1.0148\n",
      "Epoch [5/10], Step [420/4063], Loss: 0.8090\n",
      "Epoch [5/10], Step [430/4063], Loss: 0.8297\n",
      "Epoch [5/10], Step [440/4063], Loss: 1.2747\n",
      "Epoch [5/10], Step [450/4063], Loss: 1.1110\n",
      "Epoch [5/10], Step [460/4063], Loss: 0.7764\n",
      "Epoch [5/10], Step [470/4063], Loss: 1.0792\n",
      "Epoch [5/10], Step [480/4063], Loss: 0.6095\n",
      "Epoch [5/10], Step [490/4063], Loss: 1.1848\n",
      "Epoch [5/10], Step [500/4063], Loss: 1.1655\n",
      "Epoch [5/10], Step [510/4063], Loss: 0.8518\n",
      "Epoch [5/10], Step [520/4063], Loss: 1.1941\n",
      "Epoch [5/10], Step [530/4063], Loss: 1.3034\n",
      "Epoch [5/10], Step [540/4063], Loss: 1.1478\n",
      "Epoch [5/10], Step [550/4063], Loss: 0.6779\n",
      "Epoch [5/10], Step [560/4063], Loss: 0.8879\n",
      "Epoch [5/10], Step [570/4063], Loss: 0.7176\n",
      "Epoch [5/10], Step [580/4063], Loss: 0.8277\n",
      "Epoch [5/10], Step [590/4063], Loss: 0.9178\n",
      "Epoch [5/10], Step [600/4063], Loss: 0.9937\n",
      "Epoch [5/10], Step [610/4063], Loss: 1.1056\n",
      "Epoch [5/10], Step [620/4063], Loss: 0.6617\n",
      "Epoch [5/10], Step [630/4063], Loss: 0.7779\n",
      "Epoch [5/10], Step [640/4063], Loss: 1.0898\n",
      "Epoch [5/10], Step [650/4063], Loss: 0.9440\n",
      "Epoch [5/10], Step [660/4063], Loss: 0.9454\n",
      "Epoch [5/10], Step [670/4063], Loss: 0.7789\n",
      "Epoch [5/10], Step [680/4063], Loss: 1.3293\n",
      "Epoch [5/10], Step [690/4063], Loss: 1.1260\n",
      "Epoch [5/10], Step [700/4063], Loss: 0.7891\n",
      "Epoch [5/10], Step [710/4063], Loss: 0.9466\n",
      "Epoch [5/10], Step [720/4063], Loss: 1.1973\n",
      "Epoch [5/10], Step [730/4063], Loss: 0.7001\n",
      "Epoch [5/10], Step [740/4063], Loss: 0.8525\n",
      "Epoch [5/10], Step [750/4063], Loss: 1.0676\n",
      "Epoch [5/10], Step [760/4063], Loss: 0.7502\n",
      "Epoch [5/10], Step [770/4063], Loss: 0.9117\n",
      "Epoch [5/10], Step [780/4063], Loss: 1.0968\n",
      "Epoch [5/10], Step [790/4063], Loss: 1.0426\n",
      "Epoch [5/10], Step [800/4063], Loss: 1.1377\n",
      "Epoch [5/10], Step [810/4063], Loss: 0.6564\n",
      "Epoch [5/10], Step [820/4063], Loss: 1.0923\n",
      "Epoch [5/10], Step [830/4063], Loss: 1.2755\n",
      "Epoch [5/10], Step [840/4063], Loss: 0.7669\n",
      "Epoch [5/10], Step [850/4063], Loss: 0.7247\n",
      "Epoch [5/10], Step [860/4063], Loss: 0.8928\n",
      "Epoch [5/10], Step [870/4063], Loss: 0.7486\n",
      "Epoch [5/10], Step [880/4063], Loss: 1.0037\n",
      "Epoch [5/10], Step [890/4063], Loss: 0.8804\n",
      "Epoch [5/10], Step [900/4063], Loss: 0.7213\n",
      "Epoch [5/10], Step [910/4063], Loss: 1.1331\n",
      "Epoch [5/10], Step [920/4063], Loss: 0.9589\n",
      "Epoch [5/10], Step [930/4063], Loss: 1.0849\n",
      "Epoch [5/10], Step [940/4063], Loss: 0.6339\n",
      "Epoch [5/10], Step [950/4063], Loss: 0.7754\n",
      "Epoch [5/10], Step [960/4063], Loss: 0.9572\n",
      "Epoch [5/10], Step [970/4063], Loss: 0.6900\n",
      "Epoch [5/10], Step [980/4063], Loss: 1.0243\n",
      "Epoch [5/10], Step [990/4063], Loss: 0.7429\n",
      "Epoch [5/10], Step [1000/4063], Loss: 1.0913\n",
      "Epoch [5/10], Step [1010/4063], Loss: 1.0063\n",
      "Epoch [5/10], Step [1020/4063], Loss: 1.1590\n",
      "Epoch [5/10], Step [1030/4063], Loss: 0.4640\n",
      "Epoch [5/10], Step [1040/4063], Loss: 0.5979\n",
      "Epoch [5/10], Step [1050/4063], Loss: 1.2542\n",
      "Epoch [5/10], Step [1060/4063], Loss: 0.9002\n",
      "Epoch [5/10], Step [1070/4063], Loss: 0.8983\n",
      "Epoch [5/10], Step [1080/4063], Loss: 0.9554\n",
      "Epoch [5/10], Step [1090/4063], Loss: 0.9175\n",
      "Epoch [5/10], Step [1100/4063], Loss: 0.7267\n",
      "Epoch [5/10], Step [1110/4063], Loss: 1.0437\n",
      "Epoch [5/10], Step [1120/4063], Loss: 0.7697\n",
      "Epoch [5/10], Step [1130/4063], Loss: 0.8339\n",
      "Epoch [5/10], Step [1140/4063], Loss: 1.0714\n",
      "Epoch [5/10], Step [1150/4063], Loss: 0.9069\n",
      "Epoch [5/10], Step [1160/4063], Loss: 0.6084\n",
      "Epoch [5/10], Step [1170/4063], Loss: 0.5816\n",
      "Epoch [5/10], Step [1180/4063], Loss: 0.8684\n",
      "Epoch [5/10], Step [1190/4063], Loss: 0.9799\n",
      "Epoch [5/10], Step [1200/4063], Loss: 0.9161\n",
      "Epoch [5/10], Step [1210/4063], Loss: 1.0829\n",
      "Epoch [5/10], Step [1220/4063], Loss: 0.9167\n",
      "Epoch [5/10], Step [1230/4063], Loss: 1.1212\n",
      "Epoch [5/10], Step [1240/4063], Loss: 0.9972\n",
      "Epoch [5/10], Step [1250/4063], Loss: 1.0591\n",
      "Epoch [5/10], Step [1260/4063], Loss: 1.2114\n",
      "Epoch [5/10], Step [1270/4063], Loss: 0.7133\n",
      "Epoch [5/10], Step [1280/4063], Loss: 0.7779\n",
      "Epoch [5/10], Step [1290/4063], Loss: 0.7025\n",
      "Epoch [5/10], Step [1300/4063], Loss: 1.2057\n",
      "Epoch [5/10], Step [1310/4063], Loss: 0.7267\n",
      "Epoch [5/10], Step [1320/4063], Loss: 1.0257\n",
      "Epoch [5/10], Step [1330/4063], Loss: 0.9331\n",
      "Epoch [5/10], Step [1340/4063], Loss: 1.3094\n",
      "Epoch [5/10], Step [1350/4063], Loss: 0.4133\n",
      "Epoch [5/10], Step [1360/4063], Loss: 0.6784\n",
      "Epoch [5/10], Step [1370/4063], Loss: 0.6911\n",
      "Epoch [5/10], Step [1380/4063], Loss: 0.7307\n",
      "Epoch [5/10], Step [1390/4063], Loss: 0.8812\n",
      "Epoch [5/10], Step [1400/4063], Loss: 1.0833\n",
      "Epoch [5/10], Step [1410/4063], Loss: 0.4852\n",
      "Epoch [5/10], Step [1420/4063], Loss: 1.1217\n",
      "Epoch [5/10], Step [1430/4063], Loss: 1.3436\n",
      "Epoch [5/10], Step [1440/4063], Loss: 0.7541\n",
      "Epoch [5/10], Step [1450/4063], Loss: 1.1588\n",
      "Epoch [5/10], Step [1460/4063], Loss: 0.7093\n",
      "Epoch [5/10], Step [1470/4063], Loss: 0.7585\n",
      "Epoch [5/10], Step [1480/4063], Loss: 1.0058\n",
      "Epoch [5/10], Step [1490/4063], Loss: 0.4095\n",
      "Epoch [5/10], Step [1500/4063], Loss: 0.5302\n",
      "Epoch [5/10], Step [1510/4063], Loss: 0.7839\n",
      "Epoch [5/10], Step [1520/4063], Loss: 0.6737\n",
      "Epoch [5/10], Step [1530/4063], Loss: 0.7140\n",
      "Epoch [5/10], Step [1540/4063], Loss: 0.8630\n",
      "Epoch [5/10], Step [1550/4063], Loss: 0.5584\n",
      "Epoch [5/10], Step [1560/4063], Loss: 0.9181\n",
      "Epoch [5/10], Step [1570/4063], Loss: 0.7980\n",
      "Epoch [5/10], Step [1580/4063], Loss: 0.8844\n",
      "Epoch [5/10], Step [1590/4063], Loss: 0.4999\n",
      "Epoch [5/10], Step [1600/4063], Loss: 1.3758\n",
      "Epoch [5/10], Step [1610/4063], Loss: 0.8079\n",
      "Epoch [5/10], Step [1620/4063], Loss: 0.6020\n",
      "Epoch [5/10], Step [1630/4063], Loss: 0.9809\n",
      "Epoch [5/10], Step [1640/4063], Loss: 0.7861\n",
      "Epoch [5/10], Step [1650/4063], Loss: 1.0384\n",
      "Epoch [5/10], Step [1660/4063], Loss: 1.0366\n",
      "Epoch [5/10], Step [1670/4063], Loss: 0.4990\n",
      "Epoch [5/10], Step [1680/4063], Loss: 0.5232\n",
      "Epoch [5/10], Step [1690/4063], Loss: 1.1430\n",
      "Epoch [5/10], Step [1700/4063], Loss: 0.8829\n",
      "Epoch [5/10], Step [1710/4063], Loss: 1.0905\n",
      "Epoch [5/10], Step [1720/4063], Loss: 1.2762\n",
      "Epoch [5/10], Step [1730/4063], Loss: 0.5942\n",
      "Epoch [5/10], Step [1740/4063], Loss: 1.1102\n",
      "Epoch [5/10], Step [1750/4063], Loss: 0.9172\n",
      "Epoch [5/10], Step [1760/4063], Loss: 1.0361\n",
      "Epoch [5/10], Step [1770/4063], Loss: 0.5846\n",
      "Epoch [5/10], Step [1780/4063], Loss: 0.8050\n",
      "Epoch [5/10], Step [1790/4063], Loss: 0.8178\n",
      "Epoch [5/10], Step [1800/4063], Loss: 0.8316\n",
      "Epoch [5/10], Step [1810/4063], Loss: 0.6131\n",
      "Epoch [5/10], Step [1820/4063], Loss: 0.8903\n",
      "Epoch [5/10], Step [1830/4063], Loss: 1.1564\n",
      "Epoch [5/10], Step [1840/4063], Loss: 0.8305\n",
      "Epoch [5/10], Step [1850/4063], Loss: 1.0738\n",
      "Epoch [5/10], Step [1860/4063], Loss: 0.9522\n",
      "Epoch [5/10], Step [1870/4063], Loss: 0.6276\n",
      "Epoch [5/10], Step [1880/4063], Loss: 1.2026\n",
      "Epoch [5/10], Step [1890/4063], Loss: 1.1130\n",
      "Epoch [5/10], Step [1900/4063], Loss: 0.9851\n",
      "Epoch [5/10], Step [1910/4063], Loss: 0.5233\n",
      "Epoch [5/10], Step [1920/4063], Loss: 1.0292\n",
      "Epoch [5/10], Step [1930/4063], Loss: 1.0070\n",
      "Epoch [5/10], Step [1940/4063], Loss: 0.9324\n",
      "Epoch [5/10], Step [1950/4063], Loss: 0.8042\n",
      "Epoch [5/10], Step [1960/4063], Loss: 0.8896\n",
      "Epoch [5/10], Step [1970/4063], Loss: 0.7949\n",
      "Epoch [5/10], Step [1980/4063], Loss: 0.8400\n",
      "Epoch [5/10], Step [1990/4063], Loss: 0.7547\n",
      "Epoch [5/10], Step [2000/4063], Loss: 1.0683\n",
      "Epoch [5/10], Step [2010/4063], Loss: 1.0947\n",
      "Epoch [5/10], Step [2020/4063], Loss: 0.6492\n",
      "Epoch [5/10], Step [2030/4063], Loss: 1.0925\n",
      "Epoch [5/10], Step [2040/4063], Loss: 1.2893\n",
      "Epoch [5/10], Step [2050/4063], Loss: 1.1416\n",
      "Epoch [5/10], Step [2060/4063], Loss: 0.5319\n",
      "Epoch [5/10], Step [2070/4063], Loss: 1.0300\n",
      "Epoch [5/10], Step [2080/4063], Loss: 0.9437\n",
      "Epoch [5/10], Step [2090/4063], Loss: 0.5603\n",
      "Epoch [5/10], Step [2100/4063], Loss: 0.9217\n",
      "Epoch [5/10], Step [2110/4063], Loss: 1.0232\n",
      "Epoch [5/10], Step [2120/4063], Loss: 1.4987\n",
      "Epoch [5/10], Step [2130/4063], Loss: 0.9211\n",
      "Epoch [5/10], Step [2140/4063], Loss: 1.2721\n",
      "Epoch [5/10], Step [2150/4063], Loss: 0.9123\n",
      "Epoch [5/10], Step [2160/4063], Loss: 1.0024\n",
      "Epoch [5/10], Step [2170/4063], Loss: 0.9083\n",
      "Epoch [5/10], Step [2180/4063], Loss: 0.7390\n",
      "Epoch [5/10], Step [2190/4063], Loss: 0.6999\n",
      "Epoch [5/10], Step [2200/4063], Loss: 0.6705\n",
      "Epoch [5/10], Step [2210/4063], Loss: 0.9918\n",
      "Epoch [5/10], Step [2220/4063], Loss: 1.0521\n",
      "Epoch [5/10], Step [2230/4063], Loss: 0.7126\n",
      "Epoch [5/10], Step [2240/4063], Loss: 0.6494\n",
      "Epoch [5/10], Step [2250/4063], Loss: 0.7386\n",
      "Epoch [5/10], Step [2260/4063], Loss: 1.1972\n",
      "Epoch [5/10], Step [2270/4063], Loss: 0.9827\n",
      "Epoch [5/10], Step [2280/4063], Loss: 1.1451\n",
      "Epoch [5/10], Step [2290/4063], Loss: 1.0453\n",
      "Epoch [5/10], Step [2300/4063], Loss: 0.4809\n",
      "Epoch [5/10], Step [2310/4063], Loss: 1.0407\n",
      "Epoch [5/10], Step [2320/4063], Loss: 0.8455\n",
      "Epoch [5/10], Step [2330/4063], Loss: 0.5576\n",
      "Epoch [5/10], Step [2340/4063], Loss: 1.0207\n",
      "Epoch [5/10], Step [2350/4063], Loss: 1.1893\n",
      "Epoch [5/10], Step [2360/4063], Loss: 0.8099\n",
      "Epoch [5/10], Step [2370/4063], Loss: 0.9091\n",
      "Epoch [5/10], Step [2380/4063], Loss: 1.2078\n",
      "Epoch [5/10], Step [2390/4063], Loss: 1.1306\n",
      "Epoch [5/10], Step [2400/4063], Loss: 0.8789\n",
      "Epoch [5/10], Step [2410/4063], Loss: 0.8950\n",
      "Epoch [5/10], Step [2420/4063], Loss: 0.6243\n",
      "Epoch [5/10], Step [2430/4063], Loss: 0.6669\n",
      "Epoch [5/10], Step [2440/4063], Loss: 0.9546\n",
      "Epoch [5/10], Step [2450/4063], Loss: 0.8135\n",
      "Epoch [5/10], Step [2460/4063], Loss: 1.1436\n",
      "Epoch [5/10], Step [2470/4063], Loss: 0.5016\n",
      "Epoch [5/10], Step [2480/4063], Loss: 0.7849\n",
      "Epoch [5/10], Step [2490/4063], Loss: 0.8783\n",
      "Epoch [5/10], Step [2500/4063], Loss: 1.1234\n",
      "Epoch [5/10], Step [2510/4063], Loss: 0.7701\n",
      "Epoch [5/10], Step [2520/4063], Loss: 0.8578\n",
      "Epoch [5/10], Step [2530/4063], Loss: 0.6414\n",
      "Epoch [5/10], Step [2540/4063], Loss: 0.9122\n",
      "Epoch [5/10], Step [2550/4063], Loss: 0.8208\n",
      "Epoch [5/10], Step [2560/4063], Loss: 1.1080\n",
      "Epoch [5/10], Step [2570/4063], Loss: 0.9145\n",
      "Epoch [5/10], Step [2580/4063], Loss: 0.9433\n",
      "Epoch [5/10], Step [2590/4063], Loss: 0.7050\n",
      "Epoch [5/10], Step [2600/4063], Loss: 0.7034\n",
      "Epoch [5/10], Step [2610/4063], Loss: 0.8707\n",
      "Epoch [5/10], Step [2620/4063], Loss: 1.2826\n",
      "Epoch [5/10], Step [2630/4063], Loss: 0.9291\n",
      "Epoch [5/10], Step [2640/4063], Loss: 0.9488\n",
      "Epoch [5/10], Step [2650/4063], Loss: 0.6690\n",
      "Epoch [5/10], Step [2660/4063], Loss: 1.1328\n",
      "Epoch [5/10], Step [2670/4063], Loss: 0.7695\n",
      "Epoch [5/10], Step [2680/4063], Loss: 0.7887\n",
      "Epoch [5/10], Step [2690/4063], Loss: 0.8947\n",
      "Epoch [5/10], Step [2700/4063], Loss: 0.7099\n",
      "Epoch [5/10], Step [2710/4063], Loss: 1.1937\n",
      "Epoch [5/10], Step [2720/4063], Loss: 0.7868\n",
      "Epoch [5/10], Step [2730/4063], Loss: 1.4109\n",
      "Epoch [5/10], Step [2740/4063], Loss: 1.0353\n",
      "Epoch [5/10], Step [2750/4063], Loss: 0.6909\n",
      "Epoch [5/10], Step [2760/4063], Loss: 0.7680\n",
      "Epoch [5/10], Step [2770/4063], Loss: 0.8664\n",
      "Epoch [5/10], Step [2780/4063], Loss: 0.6908\n",
      "Epoch [5/10], Step [2790/4063], Loss: 0.5772\n",
      "Epoch [5/10], Step [2800/4063], Loss: 0.6474\n",
      "Epoch [5/10], Step [2810/4063], Loss: 0.8708\n",
      "Epoch [5/10], Step [2820/4063], Loss: 0.5386\n",
      "Epoch [5/10], Step [2830/4063], Loss: 0.9656\n",
      "Epoch [5/10], Step [2840/4063], Loss: 0.8970\n",
      "Epoch [5/10], Step [2850/4063], Loss: 1.0287\n",
      "Epoch [5/10], Step [2860/4063], Loss: 1.3526\n",
      "Epoch [5/10], Step [2870/4063], Loss: 1.3398\n",
      "Epoch [5/10], Step [2880/4063], Loss: 0.8586\n",
      "Epoch [5/10], Step [2890/4063], Loss: 0.6341\n",
      "Epoch [5/10], Step [2900/4063], Loss: 1.1784\n",
      "Epoch [5/10], Step [2910/4063], Loss: 0.6538\n",
      "Epoch [5/10], Step [2920/4063], Loss: 1.1739\n",
      "Epoch [5/10], Step [2930/4063], Loss: 0.8698\n",
      "Epoch [5/10], Step [2940/4063], Loss: 0.7040\n",
      "Epoch [5/10], Step [2950/4063], Loss: 0.8528\n",
      "Epoch [5/10], Step [2960/4063], Loss: 0.9971\n",
      "Epoch [5/10], Step [2970/4063], Loss: 0.9796\n",
      "Epoch [5/10], Step [2980/4063], Loss: 0.9250\n",
      "Epoch [5/10], Step [2990/4063], Loss: 1.0329\n",
      "Epoch [5/10], Step [3000/4063], Loss: 0.7176\n",
      "Epoch [5/10], Step [3010/4063], Loss: 0.6085\n",
      "Epoch [5/10], Step [3020/4063], Loss: 0.4589\n",
      "Epoch [5/10], Step [3030/4063], Loss: 1.0616\n",
      "Epoch [5/10], Step [3040/4063], Loss: 0.9254\n",
      "Epoch [5/10], Step [3050/4063], Loss: 0.6099\n",
      "Epoch [5/10], Step [3060/4063], Loss: 0.8429\n",
      "Epoch [5/10], Step [3070/4063], Loss: 1.2021\n",
      "Epoch [5/10], Step [3080/4063], Loss: 0.9348\n",
      "Epoch [5/10], Step [3090/4063], Loss: 0.9870\n",
      "Epoch [5/10], Step [3100/4063], Loss: 1.1308\n",
      "Epoch [5/10], Step [3110/4063], Loss: 0.7805\n",
      "Epoch [5/10], Step [3120/4063], Loss: 0.6040\n",
      "Epoch [5/10], Step [3130/4063], Loss: 0.6081\n",
      "Epoch [5/10], Step [3140/4063], Loss: 0.3636\n",
      "Epoch [5/10], Step [3150/4063], Loss: 0.9175\n",
      "Epoch [5/10], Step [3160/4063], Loss: 0.8724\n",
      "Epoch [5/10], Step [3170/4063], Loss: 0.8769\n",
      "Epoch [5/10], Step [3180/4063], Loss: 1.4767\n",
      "Epoch [5/10], Step [3190/4063], Loss: 1.1151\n",
      "Epoch [5/10], Step [3200/4063], Loss: 0.5743\n",
      "Epoch [5/10], Step [3210/4063], Loss: 0.7872\n",
      "Epoch [5/10], Step [3220/4063], Loss: 0.7718\n",
      "Epoch [5/10], Step [3230/4063], Loss: 0.9134\n",
      "Epoch [5/10], Step [3240/4063], Loss: 1.2536\n",
      "Epoch [5/10], Step [3250/4063], Loss: 0.8130\n",
      "Epoch [5/10], Step [3260/4063], Loss: 0.9808\n",
      "Epoch [5/10], Step [3270/4063], Loss: 0.8553\n",
      "Epoch [5/10], Step [3280/4063], Loss: 1.0629\n",
      "Epoch [5/10], Step [3290/4063], Loss: 0.7416\n",
      "Epoch [5/10], Step [3300/4063], Loss: 0.6719\n",
      "Epoch [5/10], Step [3310/4063], Loss: 0.7705\n",
      "Epoch [5/10], Step [3320/4063], Loss: 0.9908\n",
      "Epoch [5/10], Step [3330/4063], Loss: 0.7796\n",
      "Epoch [5/10], Step [3340/4063], Loss: 1.0474\n",
      "Epoch [5/10], Step [3350/4063], Loss: 0.8873\n",
      "Epoch [5/10], Step [3360/4063], Loss: 1.1357\n",
      "Epoch [5/10], Step [3370/4063], Loss: 0.9756\n",
      "Epoch [5/10], Step [3380/4063], Loss: 0.5074\n",
      "Epoch [5/10], Step [3390/4063], Loss: 1.0089\n",
      "Epoch [5/10], Step [3400/4063], Loss: 1.1190\n",
      "Epoch [5/10], Step [3410/4063], Loss: 1.1413\n",
      "Epoch [5/10], Step [3420/4063], Loss: 0.5954\n",
      "Epoch [5/10], Step [3430/4063], Loss: 0.5853\n",
      "Epoch [5/10], Step [3440/4063], Loss: 0.6213\n",
      "Epoch [5/10], Step [3450/4063], Loss: 0.8600\n",
      "Epoch [5/10], Step [3460/4063], Loss: 1.2339\n",
      "Epoch [5/10], Step [3470/4063], Loss: 0.8918\n",
      "Epoch [5/10], Step [3480/4063], Loss: 0.6774\n",
      "Epoch [5/10], Step [3490/4063], Loss: 1.0268\n",
      "Epoch [5/10], Step [3500/4063], Loss: 0.9652\n",
      "Epoch [5/10], Step [3510/4063], Loss: 0.5693\n",
      "Epoch [5/10], Step [3520/4063], Loss: 1.2990\n",
      "Epoch [5/10], Step [3530/4063], Loss: 1.1737\n",
      "Epoch [5/10], Step [3540/4063], Loss: 0.5563\n",
      "Epoch [5/10], Step [3550/4063], Loss: 0.8763\n",
      "Epoch [5/10], Step [3560/4063], Loss: 0.8234\n",
      "Epoch [5/10], Step [3570/4063], Loss: 1.1277\n",
      "Epoch [5/10], Step [3580/4063], Loss: 1.0809\n",
      "Epoch [5/10], Step [3590/4063], Loss: 1.3129\n",
      "Epoch [5/10], Step [3600/4063], Loss: 0.5264\n",
      "Epoch [5/10], Step [3610/4063], Loss: 0.7433\n",
      "Epoch [5/10], Step [3620/4063], Loss: 0.7123\n",
      "Epoch [5/10], Step [3630/4063], Loss: 0.6001\n",
      "Epoch [5/10], Step [3640/4063], Loss: 0.4850\n",
      "Epoch [5/10], Step [3650/4063], Loss: 1.0431\n",
      "Epoch [5/10], Step [3660/4063], Loss: 0.7178\n",
      "Epoch [5/10], Step [3670/4063], Loss: 0.8092\n",
      "Epoch [5/10], Step [3680/4063], Loss: 0.7979\n",
      "Epoch [5/10], Step [3690/4063], Loss: 1.2307\n",
      "Epoch [5/10], Step [3700/4063], Loss: 0.6196\n",
      "Epoch [5/10], Step [3710/4063], Loss: 1.1889\n",
      "Epoch [5/10], Step [3720/4063], Loss: 0.8462\n",
      "Epoch [5/10], Step [3730/4063], Loss: 1.2006\n",
      "Epoch [5/10], Step [3740/4063], Loss: 0.8033\n",
      "Epoch [5/10], Step [3750/4063], Loss: 0.5943\n",
      "Epoch [5/10], Step [3760/4063], Loss: 0.7160\n",
      "Epoch [5/10], Step [3770/4063], Loss: 0.7830\n",
      "Epoch [5/10], Step [3780/4063], Loss: 1.0554\n",
      "Epoch [5/10], Step [3790/4063], Loss: 1.2889\n",
      "Epoch [5/10], Step [3800/4063], Loss: 0.9134\n",
      "Epoch [5/10], Step [3810/4063], Loss: 0.8977\n",
      "Epoch [5/10], Step [3820/4063], Loss: 1.0714\n",
      "Epoch [5/10], Step [3830/4063], Loss: 0.8925\n",
      "Epoch [5/10], Step [3840/4063], Loss: 0.9938\n",
      "Epoch [5/10], Step [3850/4063], Loss: 0.8950\n",
      "Epoch [5/10], Step [3860/4063], Loss: 1.1077\n",
      "Epoch [5/10], Step [3870/4063], Loss: 0.7625\n",
      "Epoch [5/10], Step [3880/4063], Loss: 1.0102\n",
      "Epoch [5/10], Step [3890/4063], Loss: 0.7240\n",
      "Epoch [5/10], Step [3900/4063], Loss: 0.8606\n",
      "Epoch [5/10], Step [3910/4063], Loss: 1.1138\n",
      "Epoch [5/10], Step [3920/4063], Loss: 0.8355\n",
      "Epoch [5/10], Step [3930/4063], Loss: 1.4012\n",
      "Epoch [5/10], Step [3940/4063], Loss: 1.0889\n",
      "Epoch [5/10], Step [3950/4063], Loss: 0.7672\n",
      "Epoch [5/10], Step [3960/4063], Loss: 0.5900\n",
      "Epoch [5/10], Step [3970/4063], Loss: 0.8978\n",
      "Epoch [5/10], Step [3980/4063], Loss: 1.1414\n",
      "Epoch [5/10], Step [3990/4063], Loss: 0.8866\n",
      "Epoch [5/10], Step [4000/4063], Loss: 0.7837\n",
      "Epoch [5/10], Step [4010/4063], Loss: 0.9923\n",
      "Epoch [5/10], Step [4020/4063], Loss: 0.8738\n",
      "Epoch [5/10], Step [4030/4063], Loss: 1.2353\n",
      "Epoch [5/10], Step [4040/4063], Loss: 0.6642\n",
      "Epoch [5/10], Step [4050/4063], Loss: 0.8477\n",
      "Epoch [5/10], Step [4060/4063], Loss: 0.7148\n",
      "Epoch [6/10], Step [10/4063], Loss: 0.6289\n",
      "Epoch [6/10], Step [20/4063], Loss: 0.6916\n",
      "Epoch [6/10], Step [30/4063], Loss: 0.3046\n",
      "Epoch [6/10], Step [40/4063], Loss: 0.8280\n",
      "Epoch [6/10], Step [50/4063], Loss: 0.6709\n",
      "Epoch [6/10], Step [60/4063], Loss: 0.6250\n",
      "Epoch [6/10], Step [70/4063], Loss: 0.8265\n",
      "Epoch [6/10], Step [80/4063], Loss: 0.8850\n",
      "Epoch [6/10], Step [90/4063], Loss: 1.2457\n",
      "Epoch [6/10], Step [100/4063], Loss: 1.0647\n",
      "Epoch [6/10], Step [110/4063], Loss: 0.5895\n",
      "Epoch [6/10], Step [120/4063], Loss: 0.6017\n",
      "Epoch [6/10], Step [130/4063], Loss: 0.8055\n",
      "Epoch [6/10], Step [140/4063], Loss: 0.7662\n",
      "Epoch [6/10], Step [150/4063], Loss: 0.7847\n",
      "Epoch [6/10], Step [160/4063], Loss: 0.9306\n",
      "Epoch [6/10], Step [170/4063], Loss: 0.9799\n",
      "Epoch [6/10], Step [180/4063], Loss: 0.7625\n",
      "Epoch [6/10], Step [190/4063], Loss: 0.9704\n",
      "Epoch [6/10], Step [200/4063], Loss: 0.8983\n",
      "Epoch [6/10], Step [210/4063], Loss: 1.0707\n",
      "Epoch [6/10], Step [220/4063], Loss: 0.5922\n",
      "Epoch [6/10], Step [230/4063], Loss: 0.7187\n",
      "Epoch [6/10], Step [240/4063], Loss: 0.7988\n",
      "Epoch [6/10], Step [250/4063], Loss: 0.8708\n",
      "Epoch [6/10], Step [260/4063], Loss: 0.9322\n",
      "Epoch [6/10], Step [270/4063], Loss: 1.2853\n",
      "Epoch [6/10], Step [280/4063], Loss: 1.1079\n",
      "Epoch [6/10], Step [290/4063], Loss: 1.0135\n",
      "Epoch [6/10], Step [300/4063], Loss: 1.5863\n",
      "Epoch [6/10], Step [310/4063], Loss: 0.6850\n",
      "Epoch [6/10], Step [320/4063], Loss: 0.9340\n",
      "Epoch [6/10], Step [330/4063], Loss: 0.4715\n",
      "Epoch [6/10], Step [340/4063], Loss: 0.9221\n",
      "Epoch [6/10], Step [350/4063], Loss: 1.0996\n",
      "Epoch [6/10], Step [360/4063], Loss: 1.4574\n",
      "Epoch [6/10], Step [370/4063], Loss: 1.1703\n",
      "Epoch [6/10], Step [380/4063], Loss: 0.6033\n",
      "Epoch [6/10], Step [390/4063], Loss: 0.9743\n",
      "Epoch [6/10], Step [400/4063], Loss: 0.8067\n",
      "Epoch [6/10], Step [410/4063], Loss: 0.9071\n",
      "Epoch [6/10], Step [420/4063], Loss: 0.7882\n",
      "Epoch [6/10], Step [430/4063], Loss: 0.9591\n",
      "Epoch [6/10], Step [440/4063], Loss: 0.9325\n",
      "Epoch [6/10], Step [450/4063], Loss: 0.9665\n",
      "Epoch [6/10], Step [460/4063], Loss: 1.5214\n",
      "Epoch [6/10], Step [470/4063], Loss: 0.6458\n",
      "Epoch [6/10], Step [480/4063], Loss: 0.7480\n",
      "Epoch [6/10], Step [490/4063], Loss: 0.7810\n",
      "Epoch [6/10], Step [500/4063], Loss: 1.2620\n",
      "Epoch [6/10], Step [510/4063], Loss: 1.3566\n",
      "Epoch [6/10], Step [520/4063], Loss: 0.9408\n",
      "Epoch [6/10], Step [530/4063], Loss: 0.9931\n",
      "Epoch [6/10], Step [540/4063], Loss: 0.5787\n",
      "Epoch [6/10], Step [550/4063], Loss: 0.6508\n",
      "Epoch [6/10], Step [560/4063], Loss: 0.7050\n",
      "Epoch [6/10], Step [570/4063], Loss: 0.9405\n",
      "Epoch [6/10], Step [580/4063], Loss: 0.8756\n",
      "Epoch [6/10], Step [590/4063], Loss: 0.7778\n",
      "Epoch [6/10], Step [600/4063], Loss: 0.8188\n",
      "Epoch [6/10], Step [610/4063], Loss: 0.6587\n",
      "Epoch [6/10], Step [620/4063], Loss: 0.8186\n",
      "Epoch [6/10], Step [630/4063], Loss: 1.0035\n",
      "Epoch [6/10], Step [640/4063], Loss: 0.9542\n",
      "Epoch [6/10], Step [650/4063], Loss: 1.4042\n",
      "Epoch [6/10], Step [660/4063], Loss: 0.7301\n",
      "Epoch [6/10], Step [670/4063], Loss: 1.0151\n",
      "Epoch [6/10], Step [680/4063], Loss: 1.6840\n",
      "Epoch [6/10], Step [690/4063], Loss: 1.2627\n",
      "Epoch [6/10], Step [700/4063], Loss: 1.1150\n",
      "Epoch [6/10], Step [710/4063], Loss: 1.0516\n",
      "Epoch [6/10], Step [720/4063], Loss: 0.7416\n",
      "Epoch [6/10], Step [730/4063], Loss: 0.7421\n",
      "Epoch [6/10], Step [740/4063], Loss: 0.9922\n",
      "Epoch [6/10], Step [750/4063], Loss: 1.2481\n",
      "Epoch [6/10], Step [760/4063], Loss: 0.6221\n",
      "Epoch [6/10], Step [770/4063], Loss: 1.1920\n",
      "Epoch [6/10], Step [780/4063], Loss: 0.8597\n",
      "Epoch [6/10], Step [790/4063], Loss: 0.7912\n",
      "Epoch [6/10], Step [800/4063], Loss: 0.6423\n",
      "Epoch [6/10], Step [810/4063], Loss: 1.2467\n",
      "Epoch [6/10], Step [820/4063], Loss: 0.6539\n",
      "Epoch [6/10], Step [830/4063], Loss: 0.7131\n",
      "Epoch [6/10], Step [840/4063], Loss: 0.7982\n",
      "Epoch [6/10], Step [850/4063], Loss: 1.1928\n",
      "Epoch [6/10], Step [860/4063], Loss: 1.0941\n",
      "Epoch [6/10], Step [870/4063], Loss: 1.0248\n",
      "Epoch [6/10], Step [880/4063], Loss: 0.8542\n",
      "Epoch [6/10], Step [890/4063], Loss: 1.1659\n",
      "Epoch [6/10], Step [900/4063], Loss: 1.3320\n",
      "Epoch [6/10], Step [910/4063], Loss: 0.7840\n",
      "Epoch [6/10], Step [920/4063], Loss: 0.9858\n",
      "Epoch [6/10], Step [930/4063], Loss: 0.9763\n",
      "Epoch [6/10], Step [940/4063], Loss: 1.0153\n",
      "Epoch [6/10], Step [950/4063], Loss: 1.0371\n",
      "Epoch [6/10], Step [960/4063], Loss: 0.7354\n",
      "Epoch [6/10], Step [970/4063], Loss: 1.1012\n",
      "Epoch [6/10], Step [980/4063], Loss: 1.1975\n",
      "Epoch [6/10], Step [990/4063], Loss: 0.7340\n",
      "Epoch [6/10], Step [1000/4063], Loss: 0.6938\n",
      "Epoch [6/10], Step [1010/4063], Loss: 1.3239\n",
      "Epoch [6/10], Step [1020/4063], Loss: 0.6770\n",
      "Epoch [6/10], Step [1030/4063], Loss: 0.9693\n",
      "Epoch [6/10], Step [1040/4063], Loss: 0.7078\n",
      "Epoch [6/10], Step [1050/4063], Loss: 0.9573\n",
      "Epoch [6/10], Step [1060/4063], Loss: 0.8120\n",
      "Epoch [6/10], Step [1070/4063], Loss: 0.7036\n",
      "Epoch [6/10], Step [1080/4063], Loss: 1.2965\n",
      "Epoch [6/10], Step [1090/4063], Loss: 0.6011\n",
      "Epoch [6/10], Step [1100/4063], Loss: 0.9515\n",
      "Epoch [6/10], Step [1110/4063], Loss: 0.8280\n",
      "Epoch [6/10], Step [1120/4063], Loss: 1.2022\n",
      "Epoch [6/10], Step [1130/4063], Loss: 0.8508\n",
      "Epoch [6/10], Step [1140/4063], Loss: 1.3486\n",
      "Epoch [6/10], Step [1150/4063], Loss: 1.1363\n",
      "Epoch [6/10], Step [1160/4063], Loss: 0.8623\n",
      "Epoch [6/10], Step [1170/4063], Loss: 0.7554\n",
      "Epoch [6/10], Step [1180/4063], Loss: 0.8998\n",
      "Epoch [6/10], Step [1190/4063], Loss: 1.0789\n",
      "Epoch [6/10], Step [1200/4063], Loss: 1.0251\n",
      "Epoch [6/10], Step [1210/4063], Loss: 0.7095\n",
      "Epoch [6/10], Step [1220/4063], Loss: 0.6784\n",
      "Epoch [6/10], Step [1230/4063], Loss: 1.3024\n",
      "Epoch [6/10], Step [1240/4063], Loss: 0.8786\n",
      "Epoch [6/10], Step [1250/4063], Loss: 0.9187\n",
      "Epoch [6/10], Step [1260/4063], Loss: 0.7888\n",
      "Epoch [6/10], Step [1270/4063], Loss: 0.7350\n",
      "Epoch [6/10], Step [1280/4063], Loss: 0.9730\n",
      "Epoch [6/10], Step [1290/4063], Loss: 0.9001\n",
      "Epoch [6/10], Step [1300/4063], Loss: 0.9813\n",
      "Epoch [6/10], Step [1310/4063], Loss: 0.7628\n",
      "Epoch [6/10], Step [1320/4063], Loss: 1.3284\n",
      "Epoch [6/10], Step [1330/4063], Loss: 1.2997\n",
      "Epoch [6/10], Step [1340/4063], Loss: 0.9352\n",
      "Epoch [6/10], Step [1350/4063], Loss: 1.4138\n",
      "Epoch [6/10], Step [1360/4063], Loss: 1.1784\n",
      "Epoch [6/10], Step [1370/4063], Loss: 1.2138\n",
      "Epoch [6/10], Step [1380/4063], Loss: 0.9168\n",
      "Epoch [6/10], Step [1390/4063], Loss: 1.3005\n",
      "Epoch [6/10], Step [1400/4063], Loss: 1.6795\n",
      "Epoch [6/10], Step [1410/4063], Loss: 0.9836\n",
      "Epoch [6/10], Step [1420/4063], Loss: 0.5128\n",
      "Epoch [6/10], Step [1430/4063], Loss: 1.1283\n",
      "Epoch [6/10], Step [1440/4063], Loss: 1.1192\n",
      "Epoch [6/10], Step [1450/4063], Loss: 0.9290\n",
      "Epoch [6/10], Step [1460/4063], Loss: 0.7967\n",
      "Epoch [6/10], Step [1470/4063], Loss: 1.1023\n",
      "Epoch [6/10], Step [1480/4063], Loss: 1.2088\n",
      "Epoch [6/10], Step [1490/4063], Loss: 0.8352\n",
      "Epoch [6/10], Step [1500/4063], Loss: 1.2331\n",
      "Epoch [6/10], Step [1510/4063], Loss: 0.9176\n",
      "Epoch [6/10], Step [1520/4063], Loss: 0.9844\n",
      "Epoch [6/10], Step [1530/4063], Loss: 0.9604\n",
      "Epoch [6/10], Step [1540/4063], Loss: 0.7965\n",
      "Epoch [6/10], Step [1550/4063], Loss: 0.5215\n",
      "Epoch [6/10], Step [1560/4063], Loss: 0.9651\n",
      "Epoch [6/10], Step [1570/4063], Loss: 0.7083\n",
      "Epoch [6/10], Step [1580/4063], Loss: 0.7887\n",
      "Epoch [6/10], Step [1590/4063], Loss: 0.6445\n",
      "Epoch [6/10], Step [1600/4063], Loss: 0.8890\n",
      "Epoch [6/10], Step [1610/4063], Loss: 0.7451\n",
      "Epoch [6/10], Step [1620/4063], Loss: 1.0637\n",
      "Epoch [6/10], Step [1630/4063], Loss: 1.1167\n",
      "Epoch [6/10], Step [1640/4063], Loss: 1.0500\n",
      "Epoch [6/10], Step [1650/4063], Loss: 0.8297\n",
      "Epoch [6/10], Step [1660/4063], Loss: 0.8257\n",
      "Epoch [6/10], Step [1670/4063], Loss: 0.6758\n",
      "Epoch [6/10], Step [1680/4063], Loss: 1.4254\n",
      "Epoch [6/10], Step [1690/4063], Loss: 0.5400\n",
      "Epoch [6/10], Step [1700/4063], Loss: 0.8776\n",
      "Epoch [6/10], Step [1710/4063], Loss: 0.8441\n",
      "Epoch [6/10], Step [1720/4063], Loss: 0.9091\n",
      "Epoch [6/10], Step [1730/4063], Loss: 0.4978\n",
      "Epoch [6/10], Step [1740/4063], Loss: 0.7133\n",
      "Epoch [6/10], Step [1750/4063], Loss: 0.8436\n",
      "Epoch [6/10], Step [1760/4063], Loss: 1.1137\n",
      "Epoch [6/10], Step [1770/4063], Loss: 0.9620\n",
      "Epoch [6/10], Step [1780/4063], Loss: 0.4906\n",
      "Epoch [6/10], Step [1790/4063], Loss: 1.0797\n",
      "Epoch [6/10], Step [1800/4063], Loss: 0.8256\n",
      "Epoch [6/10], Step [1810/4063], Loss: 0.9818\n",
      "Epoch [6/10], Step [1820/4063], Loss: 1.0318\n",
      "Epoch [6/10], Step [1830/4063], Loss: 1.1204\n",
      "Epoch [6/10], Step [1840/4063], Loss: 0.8607\n",
      "Epoch [6/10], Step [1850/4063], Loss: 0.7718\n",
      "Epoch [6/10], Step [1860/4063], Loss: 0.8410\n",
      "Epoch [6/10], Step [1870/4063], Loss: 1.1438\n",
      "Epoch [6/10], Step [1880/4063], Loss: 1.0376\n",
      "Epoch [6/10], Step [1890/4063], Loss: 0.7583\n",
      "Epoch [6/10], Step [1900/4063], Loss: 0.8073\n",
      "Epoch [6/10], Step [1910/4063], Loss: 1.0069\n",
      "Epoch [6/10], Step [1920/4063], Loss: 1.0696\n",
      "Epoch [6/10], Step [1930/4063], Loss: 0.9568\n",
      "Epoch [6/10], Step [1940/4063], Loss: 1.2009\n",
      "Epoch [6/10], Step [1950/4063], Loss: 0.7895\n",
      "Epoch [6/10], Step [1960/4063], Loss: 1.1515\n",
      "Epoch [6/10], Step [1970/4063], Loss: 0.7381\n",
      "Epoch [6/10], Step [1980/4063], Loss: 1.3128\n",
      "Epoch [6/10], Step [1990/4063], Loss: 0.6534\n",
      "Epoch [6/10], Step [2000/4063], Loss: 0.5879\n",
      "Epoch [6/10], Step [2010/4063], Loss: 0.9333\n",
      "Epoch [6/10], Step [2020/4063], Loss: 0.8896\n",
      "Epoch [6/10], Step [2030/4063], Loss: 1.0406\n",
      "Epoch [6/10], Step [2040/4063], Loss: 0.6560\n",
      "Epoch [6/10], Step [2050/4063], Loss: 0.8213\n",
      "Epoch [6/10], Step [2060/4063], Loss: 0.8604\n",
      "Epoch [6/10], Step [2070/4063], Loss: 0.8210\n",
      "Epoch [6/10], Step [2080/4063], Loss: 0.7607\n",
      "Epoch [6/10], Step [2090/4063], Loss: 0.7110\n",
      "Epoch [6/10], Step [2100/4063], Loss: 1.0350\n",
      "Epoch [6/10], Step [2110/4063], Loss: 0.6145\n",
      "Epoch [6/10], Step [2120/4063], Loss: 1.1615\n",
      "Epoch [6/10], Step [2130/4063], Loss: 0.8182\n",
      "Epoch [6/10], Step [2140/4063], Loss: 1.0712\n",
      "Epoch [6/10], Step [2150/4063], Loss: 0.7717\n",
      "Epoch [6/10], Step [2160/4063], Loss: 0.8437\n",
      "Epoch [6/10], Step [2170/4063], Loss: 0.8834\n",
      "Epoch [6/10], Step [2180/4063], Loss: 1.0245\n",
      "Epoch [6/10], Step [2190/4063], Loss: 1.0310\n",
      "Epoch [6/10], Step [2200/4063], Loss: 1.0445\n",
      "Epoch [6/10], Step [2210/4063], Loss: 1.1090\n",
      "Epoch [6/10], Step [2220/4063], Loss: 1.1723\n",
      "Epoch [6/10], Step [2230/4063], Loss: 1.1583\n",
      "Epoch [6/10], Step [2240/4063], Loss: 0.9738\n",
      "Epoch [6/10], Step [2250/4063], Loss: 0.8630\n",
      "Epoch [6/10], Step [2260/4063], Loss: 1.1505\n",
      "Epoch [6/10], Step [2270/4063], Loss: 1.0019\n",
      "Epoch [6/10], Step [2280/4063], Loss: 1.0485\n",
      "Epoch [6/10], Step [2290/4063], Loss: 0.8054\n",
      "Epoch [6/10], Step [2300/4063], Loss: 1.0798\n",
      "Epoch [6/10], Step [2310/4063], Loss: 0.7624\n",
      "Epoch [6/10], Step [2320/4063], Loss: 1.3884\n",
      "Epoch [6/10], Step [2330/4063], Loss: 1.0134\n",
      "Epoch [6/10], Step [2340/4063], Loss: 0.8125\n",
      "Epoch [6/10], Step [2350/4063], Loss: 1.2959\n",
      "Epoch [6/10], Step [2360/4063], Loss: 0.5144\n",
      "Epoch [6/10], Step [2370/4063], Loss: 0.5994\n",
      "Epoch [6/10], Step [2380/4063], Loss: 0.5992\n",
      "Epoch [6/10], Step [2390/4063], Loss: 0.6911\n",
      "Epoch [6/10], Step [2400/4063], Loss: 0.5570\n",
      "Epoch [6/10], Step [2410/4063], Loss: 0.8467\n",
      "Epoch [6/10], Step [2420/4063], Loss: 0.8513\n",
      "Epoch [6/10], Step [2430/4063], Loss: 0.7132\n",
      "Epoch [6/10], Step [2440/4063], Loss: 1.0080\n",
      "Epoch [6/10], Step [2450/4063], Loss: 0.7531\n",
      "Epoch [6/10], Step [2460/4063], Loss: 0.6583\n",
      "Epoch [6/10], Step [2470/4063], Loss: 0.9011\n",
      "Epoch [6/10], Step [2480/4063], Loss: 0.5292\n",
      "Epoch [6/10], Step [2490/4063], Loss: 0.6609\n",
      "Epoch [6/10], Step [2500/4063], Loss: 0.9983\n",
      "Epoch [6/10], Step [2510/4063], Loss: 1.3342\n",
      "Epoch [6/10], Step [2520/4063], Loss: 0.9860\n",
      "Epoch [6/10], Step [2530/4063], Loss: 0.6051\n",
      "Epoch [6/10], Step [2540/4063], Loss: 0.9153\n",
      "Epoch [6/10], Step [2550/4063], Loss: 0.9648\n",
      "Epoch [6/10], Step [2560/4063], Loss: 1.0516\n",
      "Epoch [6/10], Step [2570/4063], Loss: 0.6116\n",
      "Epoch [6/10], Step [2580/4063], Loss: 0.5902\n",
      "Epoch [6/10], Step [2590/4063], Loss: 0.9011\n",
      "Epoch [6/10], Step [2600/4063], Loss: 1.0812\n",
      "Epoch [6/10], Step [2610/4063], Loss: 0.8854\n",
      "Epoch [6/10], Step [2620/4063], Loss: 0.7637\n",
      "Epoch [6/10], Step [2630/4063], Loss: 0.8672\n",
      "Epoch [6/10], Step [2640/4063], Loss: 0.9570\n",
      "Epoch [6/10], Step [2650/4063], Loss: 1.1326\n",
      "Epoch [6/10], Step [2660/4063], Loss: 1.2349\n",
      "Epoch [6/10], Step [2670/4063], Loss: 0.6554\n",
      "Epoch [6/10], Step [2680/4063], Loss: 0.5921\n",
      "Epoch [6/10], Step [2690/4063], Loss: 0.8222\n",
      "Epoch [6/10], Step [2700/4063], Loss: 0.9892\n",
      "Epoch [6/10], Step [2710/4063], Loss: 0.6448\n",
      "Epoch [6/10], Step [2720/4063], Loss: 0.4523\n",
      "Epoch [6/10], Step [2730/4063], Loss: 0.7624\n",
      "Epoch [6/10], Step [2740/4063], Loss: 1.2595\n",
      "Epoch [6/10], Step [2750/4063], Loss: 0.5043\n",
      "Epoch [6/10], Step [2760/4063], Loss: 0.9637\n",
      "Epoch [6/10], Step [2770/4063], Loss: 0.7240\n",
      "Epoch [6/10], Step [2780/4063], Loss: 1.0480\n",
      "Epoch [6/10], Step [2790/4063], Loss: 0.7758\n",
      "Epoch [6/10], Step [2800/4063], Loss: 0.7592\n",
      "Epoch [6/10], Step [2810/4063], Loss: 0.5563\n",
      "Epoch [6/10], Step [2820/4063], Loss: 0.5082\n",
      "Epoch [6/10], Step [2830/4063], Loss: 0.7699\n",
      "Epoch [6/10], Step [2840/4063], Loss: 0.9267\n",
      "Epoch [6/10], Step [2850/4063], Loss: 1.0777\n",
      "Epoch [6/10], Step [2860/4063], Loss: 1.0730\n",
      "Epoch [6/10], Step [2870/4063], Loss: 0.7237\n",
      "Epoch [6/10], Step [2880/4063], Loss: 0.7463\n",
      "Epoch [6/10], Step [2890/4063], Loss: 0.6495\n",
      "Epoch [6/10], Step [2900/4063], Loss: 1.1206\n",
      "Epoch [6/10], Step [2910/4063], Loss: 0.6931\n",
      "Epoch [6/10], Step [2920/4063], Loss: 1.1994\n",
      "Epoch [6/10], Step [2930/4063], Loss: 0.6638\n",
      "Epoch [6/10], Step [2940/4063], Loss: 0.9820\n",
      "Epoch [6/10], Step [2950/4063], Loss: 1.1973\n",
      "Epoch [6/10], Step [2960/4063], Loss: 1.4058\n",
      "Epoch [6/10], Step [2970/4063], Loss: 1.6359\n",
      "Epoch [6/10], Step [2980/4063], Loss: 1.4679\n",
      "Epoch [6/10], Step [2990/4063], Loss: 0.9458\n",
      "Epoch [6/10], Step [3000/4063], Loss: 0.6190\n",
      "Epoch [6/10], Step [3010/4063], Loss: 0.8829\n",
      "Epoch [6/10], Step [3020/4063], Loss: 0.7378\n",
      "Epoch [6/10], Step [3030/4063], Loss: 1.0578\n",
      "Epoch [6/10], Step [3040/4063], Loss: 0.5436\n",
      "Epoch [6/10], Step [3050/4063], Loss: 0.7303\n",
      "Epoch [6/10], Step [3060/4063], Loss: 1.3209\n",
      "Epoch [6/10], Step [3070/4063], Loss: 0.8491\n",
      "Epoch [6/10], Step [3080/4063], Loss: 1.0555\n",
      "Epoch [6/10], Step [3090/4063], Loss: 0.9494\n",
      "Epoch [6/10], Step [3100/4063], Loss: 0.7393\n",
      "Epoch [6/10], Step [3110/4063], Loss: 0.8690\n",
      "Epoch [6/10], Step [3120/4063], Loss: 0.6010\n",
      "Epoch [6/10], Step [3130/4063], Loss: 1.2468\n",
      "Epoch [6/10], Step [3140/4063], Loss: 1.0744\n",
      "Epoch [6/10], Step [3150/4063], Loss: 1.2950\n",
      "Epoch [6/10], Step [3160/4063], Loss: 1.0559\n",
      "Epoch [6/10], Step [3170/4063], Loss: 1.0724\n",
      "Epoch [6/10], Step [3180/4063], Loss: 1.0391\n",
      "Epoch [6/10], Step [3190/4063], Loss: 0.4768\n",
      "Epoch [6/10], Step [3200/4063], Loss: 0.8009\n",
      "Epoch [6/10], Step [3210/4063], Loss: 0.8497\n",
      "Epoch [6/10], Step [3220/4063], Loss: 1.1438\n",
      "Epoch [6/10], Step [3230/4063], Loss: 0.7778\n",
      "Epoch [6/10], Step [3240/4063], Loss: 1.3832\n",
      "Epoch [6/10], Step [3250/4063], Loss: 0.8844\n",
      "Epoch [6/10], Step [3260/4063], Loss: 0.7861\n",
      "Epoch [6/10], Step [3270/4063], Loss: 0.8053\n",
      "Epoch [6/10], Step [3280/4063], Loss: 0.5675\n",
      "Epoch [6/10], Step [3290/4063], Loss: 1.1065\n",
      "Epoch [6/10], Step [3300/4063], Loss: 0.8279\n",
      "Epoch [6/10], Step [3310/4063], Loss: 0.7767\n",
      "Epoch [6/10], Step [3320/4063], Loss: 0.6599\n",
      "Epoch [6/10], Step [3330/4063], Loss: 1.0191\n",
      "Epoch [6/10], Step [3340/4063], Loss: 1.0453\n",
      "Epoch [6/10], Step [3350/4063], Loss: 0.7760\n",
      "Epoch [6/10], Step [3360/4063], Loss: 0.7087\n",
      "Epoch [6/10], Step [3370/4063], Loss: 0.9329\n",
      "Epoch [6/10], Step [3380/4063], Loss: 1.1802\n",
      "Epoch [6/10], Step [3390/4063], Loss: 0.9135\n",
      "Epoch [6/10], Step [3400/4063], Loss: 1.1206\n",
      "Epoch [6/10], Step [3410/4063], Loss: 0.9375\n",
      "Epoch [6/10], Step [3420/4063], Loss: 0.6638\n",
      "Epoch [6/10], Step [3430/4063], Loss: 1.1072\n",
      "Epoch [6/10], Step [3440/4063], Loss: 1.1881\n",
      "Epoch [6/10], Step [3450/4063], Loss: 0.6218\n",
      "Epoch [6/10], Step [3460/4063], Loss: 1.0810\n",
      "Epoch [6/10], Step [3470/4063], Loss: 0.9676\n",
      "Epoch [6/10], Step [3480/4063], Loss: 1.1559\n",
      "Epoch [6/10], Step [3490/4063], Loss: 0.8074\n",
      "Epoch [6/10], Step [3500/4063], Loss: 0.7133\n",
      "Epoch [6/10], Step [3510/4063], Loss: 0.9356\n",
      "Epoch [6/10], Step [3520/4063], Loss: 0.5623\n",
      "Epoch [6/10], Step [3530/4063], Loss: 1.1204\n",
      "Epoch [6/10], Step [3540/4063], Loss: 1.0278\n",
      "Epoch [6/10], Step [3550/4063], Loss: 0.8775\n",
      "Epoch [6/10], Step [3560/4063], Loss: 0.9089\n",
      "Epoch [6/10], Step [3570/4063], Loss: 1.1653\n",
      "Epoch [6/10], Step [3580/4063], Loss: 0.5405\n",
      "Epoch [6/10], Step [3590/4063], Loss: 0.8907\n",
      "Epoch [6/10], Step [3600/4063], Loss: 1.1409\n",
      "Epoch [6/10], Step [3610/4063], Loss: 0.7472\n",
      "Epoch [6/10], Step [3620/4063], Loss: 0.7758\n",
      "Epoch [6/10], Step [3630/4063], Loss: 0.6160\n",
      "Epoch [6/10], Step [3640/4063], Loss: 0.9788\n",
      "Epoch [6/10], Step [3650/4063], Loss: 1.1036\n",
      "Epoch [6/10], Step [3660/4063], Loss: 0.9838\n",
      "Epoch [6/10], Step [3670/4063], Loss: 0.8311\n",
      "Epoch [6/10], Step [3680/4063], Loss: 0.9584\n",
      "Epoch [6/10], Step [3690/4063], Loss: 0.9018\n",
      "Epoch [6/10], Step [3700/4063], Loss: 1.0926\n",
      "Epoch [6/10], Step [3710/4063], Loss: 0.8968\n",
      "Epoch [6/10], Step [3720/4063], Loss: 1.4464\n",
      "Epoch [6/10], Step [3730/4063], Loss: 1.1409\n",
      "Epoch [6/10], Step [3740/4063], Loss: 0.7166\n",
      "Epoch [6/10], Step [3750/4063], Loss: 0.5665\n",
      "Epoch [6/10], Step [3760/4063], Loss: 0.8257\n",
      "Epoch [6/10], Step [3770/4063], Loss: 0.7742\n",
      "Epoch [6/10], Step [3780/4063], Loss: 0.8947\n",
      "Epoch [6/10], Step [3790/4063], Loss: 0.7381\n",
      "Epoch [6/10], Step [3800/4063], Loss: 0.7686\n",
      "Epoch [6/10], Step [3810/4063], Loss: 0.8444\n",
      "Epoch [6/10], Step [3820/4063], Loss: 0.9190\n",
      "Epoch [6/10], Step [3830/4063], Loss: 1.1034\n",
      "Epoch [6/10], Step [3840/4063], Loss: 0.8773\n",
      "Epoch [6/10], Step [3850/4063], Loss: 0.9586\n",
      "Epoch [6/10], Step [3860/4063], Loss: 0.8695\n",
      "Epoch [6/10], Step [3870/4063], Loss: 0.8376\n",
      "Epoch [6/10], Step [3880/4063], Loss: 0.8322\n",
      "Epoch [6/10], Step [3890/4063], Loss: 0.8853\n",
      "Epoch [6/10], Step [3900/4063], Loss: 0.9657\n",
      "Epoch [6/10], Step [3910/4063], Loss: 0.4987\n",
      "Epoch [6/10], Step [3920/4063], Loss: 0.5957\n",
      "Epoch [6/10], Step [3930/4063], Loss: 1.0223\n",
      "Epoch [6/10], Step [3940/4063], Loss: 1.4327\n",
      "Epoch [6/10], Step [3950/4063], Loss: 0.9414\n",
      "Epoch [6/10], Step [3960/4063], Loss: 0.6869\n",
      "Epoch [6/10], Step [3970/4063], Loss: 0.7892\n",
      "Epoch [6/10], Step [3980/4063], Loss: 1.2150\n",
      "Epoch [6/10], Step [3990/4063], Loss: 0.9955\n",
      "Epoch [6/10], Step [4000/4063], Loss: 0.7744\n",
      "Epoch [6/10], Step [4010/4063], Loss: 0.9740\n",
      "Epoch [6/10], Step [4020/4063], Loss: 0.9630\n",
      "Epoch [6/10], Step [4030/4063], Loss: 0.5936\n",
      "Epoch [6/10], Step [4040/4063], Loss: 0.9169\n",
      "Epoch [6/10], Step [4050/4063], Loss: 0.6058\n",
      "Epoch [6/10], Step [4060/4063], Loss: 0.8944\n",
      "Epoch [7/10], Step [10/4063], Loss: 0.6845\n",
      "Epoch [7/10], Step [20/4063], Loss: 0.7745\n",
      "Epoch [7/10], Step [30/4063], Loss: 0.8056\n",
      "Epoch [7/10], Step [40/4063], Loss: 1.0449\n",
      "Epoch [7/10], Step [50/4063], Loss: 1.2170\n",
      "Epoch [7/10], Step [60/4063], Loss: 0.8249\n",
      "Epoch [7/10], Step [70/4063], Loss: 1.0579\n",
      "Epoch [7/10], Step [80/4063], Loss: 1.1325\n",
      "Epoch [7/10], Step [90/4063], Loss: 1.0143\n",
      "Epoch [7/10], Step [100/4063], Loss: 0.6983\n",
      "Epoch [7/10], Step [110/4063], Loss: 0.8129\n",
      "Epoch [7/10], Step [120/4063], Loss: 0.6558\n",
      "Epoch [7/10], Step [130/4063], Loss: 0.8284\n",
      "Epoch [7/10], Step [140/4063], Loss: 0.7279\n",
      "Epoch [7/10], Step [150/4063], Loss: 1.0593\n",
      "Epoch [7/10], Step [160/4063], Loss: 0.8984\n",
      "Epoch [7/10], Step [170/4063], Loss: 0.6903\n",
      "Epoch [7/10], Step [180/4063], Loss: 0.7803\n",
      "Epoch [7/10], Step [190/4063], Loss: 1.0359\n",
      "Epoch [7/10], Step [200/4063], Loss: 1.1511\n",
      "Epoch [7/10], Step [210/4063], Loss: 0.8141\n",
      "Epoch [7/10], Step [220/4063], Loss: 0.7682\n",
      "Epoch [7/10], Step [230/4063], Loss: 0.7851\n",
      "Epoch [7/10], Step [240/4063], Loss: 0.6831\n",
      "Epoch [7/10], Step [250/4063], Loss: 0.9341\n",
      "Epoch [7/10], Step [260/4063], Loss: 1.0553\n",
      "Epoch [7/10], Step [270/4063], Loss: 0.8029\n",
      "Epoch [7/10], Step [280/4063], Loss: 0.7327\n",
      "Epoch [7/10], Step [290/4063], Loss: 0.8802\n",
      "Epoch [7/10], Step [300/4063], Loss: 0.9751\n",
      "Epoch [7/10], Step [310/4063], Loss: 0.7523\n",
      "Epoch [7/10], Step [320/4063], Loss: 0.7139\n",
      "Epoch [7/10], Step [330/4063], Loss: 0.9746\n",
      "Epoch [7/10], Step [340/4063], Loss: 0.4738\n",
      "Epoch [7/10], Step [350/4063], Loss: 0.6786\n",
      "Epoch [7/10], Step [360/4063], Loss: 0.8409\n",
      "Epoch [7/10], Step [370/4063], Loss: 0.7476\n",
      "Epoch [7/10], Step [380/4063], Loss: 1.0748\n",
      "Epoch [7/10], Step [390/4063], Loss: 0.9313\n",
      "Epoch [7/10], Step [400/4063], Loss: 0.9347\n",
      "Epoch [7/10], Step [410/4063], Loss: 0.9219\n",
      "Epoch [7/10], Step [420/4063], Loss: 0.7108\n",
      "Epoch [7/10], Step [430/4063], Loss: 0.8852\n",
      "Epoch [7/10], Step [440/4063], Loss: 0.9866\n",
      "Epoch [7/10], Step [450/4063], Loss: 1.2948\n",
      "Epoch [7/10], Step [460/4063], Loss: 0.6028\n",
      "Epoch [7/10], Step [470/4063], Loss: 0.7347\n",
      "Epoch [7/10], Step [480/4063], Loss: 0.7604\n",
      "Epoch [7/10], Step [490/4063], Loss: 0.5951\n",
      "Epoch [7/10], Step [500/4063], Loss: 0.8977\n",
      "Epoch [7/10], Step [510/4063], Loss: 1.0486\n",
      "Epoch [7/10], Step [520/4063], Loss: 1.0326\n",
      "Epoch [7/10], Step [530/4063], Loss: 1.2454\n",
      "Epoch [7/10], Step [540/4063], Loss: 1.0173\n",
      "Epoch [7/10], Step [550/4063], Loss: 0.9507\n",
      "Epoch [7/10], Step [560/4063], Loss: 0.8511\n",
      "Epoch [7/10], Step [570/4063], Loss: 0.9576\n",
      "Epoch [7/10], Step [580/4063], Loss: 0.7590\n",
      "Epoch [7/10], Step [590/4063], Loss: 1.0795\n",
      "Epoch [7/10], Step [600/4063], Loss: 0.8552\n",
      "Epoch [7/10], Step [610/4063], Loss: 0.6670\n",
      "Epoch [7/10], Step [620/4063], Loss: 0.7258\n",
      "Epoch [7/10], Step [630/4063], Loss: 1.0185\n",
      "Epoch [7/10], Step [640/4063], Loss: 1.1609\n",
      "Epoch [7/10], Step [650/4063], Loss: 0.9674\n",
      "Epoch [7/10], Step [660/4063], Loss: 0.7694\n",
      "Epoch [7/10], Step [670/4063], Loss: 1.3112\n",
      "Epoch [7/10], Step [680/4063], Loss: 0.9795\n",
      "Epoch [7/10], Step [690/4063], Loss: 0.9690\n",
      "Epoch [7/10], Step [700/4063], Loss: 0.6128\n",
      "Epoch [7/10], Step [710/4063], Loss: 0.5531\n",
      "Epoch [7/10], Step [720/4063], Loss: 0.6886\n",
      "Epoch [7/10], Step [730/4063], Loss: 0.5926\n",
      "Epoch [7/10], Step [740/4063], Loss: 0.8066\n",
      "Epoch [7/10], Step [750/4063], Loss: 0.8181\n",
      "Epoch [7/10], Step [760/4063], Loss: 0.8776\n",
      "Epoch [7/10], Step [770/4063], Loss: 1.0032\n",
      "Epoch [7/10], Step [780/4063], Loss: 1.0476\n",
      "Epoch [7/10], Step [790/4063], Loss: 1.1418\n",
      "Epoch [7/10], Step [800/4063], Loss: 1.1233\n",
      "Epoch [7/10], Step [810/4063], Loss: 0.5845\n",
      "Epoch [7/10], Step [820/4063], Loss: 0.9257\n",
      "Epoch [7/10], Step [830/4063], Loss: 0.7945\n",
      "Epoch [7/10], Step [840/4063], Loss: 0.7585\n",
      "Epoch [7/10], Step [850/4063], Loss: 1.0789\n",
      "Epoch [7/10], Step [860/4063], Loss: 0.5053\n",
      "Epoch [7/10], Step [870/4063], Loss: 0.6794\n",
      "Epoch [7/10], Step [880/4063], Loss: 0.7828\n",
      "Epoch [7/10], Step [890/4063], Loss: 0.9686\n",
      "Epoch [7/10], Step [900/4063], Loss: 0.7024\n",
      "Epoch [7/10], Step [910/4063], Loss: 0.5116\n",
      "Epoch [7/10], Step [920/4063], Loss: 0.9308\n",
      "Epoch [7/10], Step [930/4063], Loss: 0.8727\n",
      "Epoch [7/10], Step [940/4063], Loss: 1.1815\n",
      "Epoch [7/10], Step [950/4063], Loss: 0.8706\n",
      "Epoch [7/10], Step [960/4063], Loss: 0.9168\n",
      "Epoch [7/10], Step [970/4063], Loss: 0.6277\n",
      "Epoch [7/10], Step [980/4063], Loss: 0.9961\n",
      "Epoch [7/10], Step [990/4063], Loss: 1.1137\n",
      "Epoch [7/10], Step [1000/4063], Loss: 1.0201\n",
      "Epoch [7/10], Step [1010/4063], Loss: 0.9232\n",
      "Epoch [7/10], Step [1020/4063], Loss: 0.7749\n",
      "Epoch [7/10], Step [1030/4063], Loss: 1.3705\n",
      "Epoch [7/10], Step [1040/4063], Loss: 0.6167\n",
      "Epoch [7/10], Step [1050/4063], Loss: 0.9006\n",
      "Epoch [7/10], Step [1060/4063], Loss: 0.8701\n",
      "Epoch [7/10], Step [1070/4063], Loss: 0.7091\n",
      "Epoch [7/10], Step [1080/4063], Loss: 1.0850\n",
      "Epoch [7/10], Step [1090/4063], Loss: 0.7059\n",
      "Epoch [7/10], Step [1100/4063], Loss: 0.9814\n",
      "Epoch [7/10], Step [1110/4063], Loss: 1.0597\n",
      "Epoch [7/10], Step [1120/4063], Loss: 0.7844\n",
      "Epoch [7/10], Step [1130/4063], Loss: 0.9454\n",
      "Epoch [7/10], Step [1140/4063], Loss: 0.6785\n",
      "Epoch [7/10], Step [1150/4063], Loss: 0.7291\n",
      "Epoch [7/10], Step [1160/4063], Loss: 0.8815\n",
      "Epoch [7/10], Step [1170/4063], Loss: 0.6469\n",
      "Epoch [7/10], Step [1180/4063], Loss: 0.8824\n",
      "Epoch [7/10], Step [1190/4063], Loss: 0.5211\n",
      "Epoch [7/10], Step [1200/4063], Loss: 0.8896\n",
      "Epoch [7/10], Step [1210/4063], Loss: 1.0827\n",
      "Epoch [7/10], Step [1220/4063], Loss: 0.6967\n",
      "Epoch [7/10], Step [1230/4063], Loss: 0.8715\n",
      "Epoch [7/10], Step [1240/4063], Loss: 0.9289\n",
      "Epoch [7/10], Step [1250/4063], Loss: 0.6331\n",
      "Epoch [7/10], Step [1260/4063], Loss: 0.9748\n",
      "Epoch [7/10], Step [1270/4063], Loss: 0.5132\n",
      "Epoch [7/10], Step [1280/4063], Loss: 0.9675\n",
      "Epoch [7/10], Step [1290/4063], Loss: 0.7543\n",
      "Epoch [7/10], Step [1300/4063], Loss: 0.7835\n",
      "Epoch [7/10], Step [1310/4063], Loss: 0.7163\n",
      "Epoch [7/10], Step [1320/4063], Loss: 1.0545\n",
      "Epoch [7/10], Step [1330/4063], Loss: 0.9899\n",
      "Epoch [7/10], Step [1340/4063], Loss: 0.9175\n",
      "Epoch [7/10], Step [1350/4063], Loss: 0.8759\n",
      "Epoch [7/10], Step [1360/4063], Loss: 0.8723\n",
      "Epoch [7/10], Step [1370/4063], Loss: 0.7922\n",
      "Epoch [7/10], Step [1380/4063], Loss: 0.9700\n",
      "Epoch [7/10], Step [1390/4063], Loss: 0.6248\n",
      "Epoch [7/10], Step [1400/4063], Loss: 0.6938\n",
      "Epoch [7/10], Step [1410/4063], Loss: 0.8254\n",
      "Epoch [7/10], Step [1420/4063], Loss: 0.8263\n",
      "Epoch [7/10], Step [1430/4063], Loss: 0.5661\n",
      "Epoch [7/10], Step [1440/4063], Loss: 1.0496\n",
      "Epoch [7/10], Step [1450/4063], Loss: 0.7361\n",
      "Epoch [7/10], Step [1460/4063], Loss: 1.0698\n",
      "Epoch [7/10], Step [1470/4063], Loss: 0.7393\n",
      "Epoch [7/10], Step [1480/4063], Loss: 1.0890\n",
      "Epoch [7/10], Step [1490/4063], Loss: 0.9167\n",
      "Epoch [7/10], Step [1500/4063], Loss: 0.9302\n",
      "Epoch [7/10], Step [1510/4063], Loss: 0.8106\n",
      "Epoch [7/10], Step [1520/4063], Loss: 1.0325\n",
      "Epoch [7/10], Step [1530/4063], Loss: 0.9715\n",
      "Epoch [7/10], Step [1540/4063], Loss: 0.8827\n",
      "Epoch [7/10], Step [1550/4063], Loss: 1.0013\n",
      "Epoch [7/10], Step [1560/4063], Loss: 0.8794\n",
      "Epoch [7/10], Step [1570/4063], Loss: 1.2104\n",
      "Epoch [7/10], Step [1580/4063], Loss: 0.6627\n",
      "Epoch [7/10], Step [1590/4063], Loss: 0.7143\n",
      "Epoch [7/10], Step [1600/4063], Loss: 0.7227\n",
      "Epoch [7/10], Step [1610/4063], Loss: 0.8458\n",
      "Epoch [7/10], Step [1620/4063], Loss: 1.0577\n",
      "Epoch [7/10], Step [1630/4063], Loss: 0.7506\n",
      "Epoch [7/10], Step [1640/4063], Loss: 0.6706\n",
      "Epoch [7/10], Step [1650/4063], Loss: 0.9619\n",
      "Epoch [7/10], Step [1660/4063], Loss: 0.7677\n",
      "Epoch [7/10], Step [1670/4063], Loss: 0.8344\n",
      "Epoch [7/10], Step [1680/4063], Loss: 0.6673\n",
      "Epoch [7/10], Step [1690/4063], Loss: 0.7167\n",
      "Epoch [7/10], Step [1700/4063], Loss: 1.3375\n",
      "Epoch [7/10], Step [1710/4063], Loss: 0.7110\n",
      "Epoch [7/10], Step [1720/4063], Loss: 0.7567\n",
      "Epoch [7/10], Step [1730/4063], Loss: 0.6354\n",
      "Epoch [7/10], Step [1740/4063], Loss: 1.2057\n",
      "Epoch [7/10], Step [1750/4063], Loss: 0.7047\n",
      "Epoch [7/10], Step [1760/4063], Loss: 0.7828\n",
      "Epoch [7/10], Step [1770/4063], Loss: 0.8043\n",
      "Epoch [7/10], Step [1780/4063], Loss: 1.0285\n",
      "Epoch [7/10], Step [1790/4063], Loss: 0.8883\n",
      "Epoch [7/10], Step [1800/4063], Loss: 0.8508\n",
      "Epoch [7/10], Step [1810/4063], Loss: 1.0583\n",
      "Epoch [7/10], Step [1820/4063], Loss: 0.9183\n",
      "Epoch [7/10], Step [1830/4063], Loss: 0.7839\n",
      "Epoch [7/10], Step [1840/4063], Loss: 0.6767\n",
      "Epoch [7/10], Step [1850/4063], Loss: 1.1095\n",
      "Epoch [7/10], Step [1860/4063], Loss: 1.4696\n",
      "Epoch [7/10], Step [1870/4063], Loss: 0.7409\n",
      "Epoch [7/10], Step [1880/4063], Loss: 0.5258\n",
      "Epoch [7/10], Step [1890/4063], Loss: 0.7806\n",
      "Epoch [7/10], Step [1900/4063], Loss: 0.8373\n",
      "Epoch [7/10], Step [1910/4063], Loss: 0.9594\n",
      "Epoch [7/10], Step [1920/4063], Loss: 0.9191\n",
      "Epoch [7/10], Step [1930/4063], Loss: 0.8346\n",
      "Epoch [7/10], Step [1940/4063], Loss: 1.0331\n",
      "Epoch [7/10], Step [1950/4063], Loss: 1.0606\n",
      "Epoch [7/10], Step [1960/4063], Loss: 0.6820\n",
      "Epoch [7/10], Step [1970/4063], Loss: 0.7732\n",
      "Epoch [7/10], Step [1980/4063], Loss: 0.9437\n",
      "Epoch [7/10], Step [1990/4063], Loss: 1.3830\n",
      "Epoch [7/10], Step [2000/4063], Loss: 0.9411\n",
      "Epoch [7/10], Step [2010/4063], Loss: 0.7504\n",
      "Epoch [7/10], Step [2020/4063], Loss: 0.8015\n",
      "Epoch [7/10], Step [2030/4063], Loss: 0.9664\n",
      "Epoch [7/10], Step [2040/4063], Loss: 1.4048\n",
      "Epoch [7/10], Step [2050/4063], Loss: 0.8026\n",
      "Epoch [7/10], Step [2060/4063], Loss: 0.9281\n",
      "Epoch [7/10], Step [2070/4063], Loss: 0.9398\n",
      "Epoch [7/10], Step [2080/4063], Loss: 1.0438\n",
      "Epoch [7/10], Step [2090/4063], Loss: 0.5438\n",
      "Epoch [7/10], Step [2100/4063], Loss: 1.0239\n",
      "Epoch [7/10], Step [2110/4063], Loss: 1.0361\n",
      "Epoch [7/10], Step [2120/4063], Loss: 0.7819\n",
      "Epoch [7/10], Step [2130/4063], Loss: 1.1608\n",
      "Epoch [7/10], Step [2140/4063], Loss: 1.0471\n",
      "Epoch [7/10], Step [2150/4063], Loss: 0.9326\n",
      "Epoch [7/10], Step [2160/4063], Loss: 0.8793\n",
      "Epoch [7/10], Step [2170/4063], Loss: 0.8403\n",
      "Epoch [7/10], Step [2180/4063], Loss: 1.4059\n",
      "Epoch [7/10], Step [2190/4063], Loss: 0.6427\n",
      "Epoch [7/10], Step [2200/4063], Loss: 0.6782\n",
      "Epoch [7/10], Step [2210/4063], Loss: 1.2613\n",
      "Epoch [7/10], Step [2220/4063], Loss: 0.9187\n",
      "Epoch [7/10], Step [2230/4063], Loss: 1.0437\n",
      "Epoch [7/10], Step [2240/4063], Loss: 0.9584\n",
      "Epoch [7/10], Step [2250/4063], Loss: 1.2862\n",
      "Epoch [7/10], Step [2260/4063], Loss: 0.8504\n",
      "Epoch [7/10], Step [2270/4063], Loss: 1.3437\n",
      "Epoch [7/10], Step [2280/4063], Loss: 1.1287\n",
      "Epoch [7/10], Step [2290/4063], Loss: 1.1650\n",
      "Epoch [7/10], Step [2300/4063], Loss: 1.1353\n",
      "Epoch [7/10], Step [2310/4063], Loss: 0.8197\n",
      "Epoch [7/10], Step [2320/4063], Loss: 0.6595\n",
      "Epoch [7/10], Step [2330/4063], Loss: 0.9059\n",
      "Epoch [7/10], Step [2340/4063], Loss: 0.8448\n",
      "Epoch [7/10], Step [2350/4063], Loss: 0.8669\n",
      "Epoch [7/10], Step [2360/4063], Loss: 0.6811\n",
      "Epoch [7/10], Step [2370/4063], Loss: 0.8379\n",
      "Epoch [7/10], Step [2380/4063], Loss: 1.1306\n",
      "Epoch [7/10], Step [2390/4063], Loss: 1.0777\n",
      "Epoch [7/10], Step [2400/4063], Loss: 0.9881\n",
      "Epoch [7/10], Step [2410/4063], Loss: 0.7901\n",
      "Epoch [7/10], Step [2420/4063], Loss: 1.0009\n",
      "Epoch [7/10], Step [2430/4063], Loss: 0.5524\n",
      "Epoch [7/10], Step [2440/4063], Loss: 0.9635\n",
      "Epoch [7/10], Step [2450/4063], Loss: 0.6051\n",
      "Epoch [7/10], Step [2460/4063], Loss: 0.9830\n",
      "Epoch [7/10], Step [2470/4063], Loss: 0.8605\n",
      "Epoch [7/10], Step [2480/4063], Loss: 0.8681\n",
      "Epoch [7/10], Step [2490/4063], Loss: 1.6133\n",
      "Epoch [7/10], Step [2500/4063], Loss: 0.5056\n",
      "Epoch [7/10], Step [2510/4063], Loss: 0.6383\n",
      "Epoch [7/10], Step [2520/4063], Loss: 0.8783\n",
      "Epoch [7/10], Step [2530/4063], Loss: 0.8103\n",
      "Epoch [7/10], Step [2540/4063], Loss: 0.9029\n",
      "Epoch [7/10], Step [2550/4063], Loss: 1.4719\n",
      "Epoch [7/10], Step [2560/4063], Loss: 1.0444\n",
      "Epoch [7/10], Step [2570/4063], Loss: 0.9165\n",
      "Epoch [7/10], Step [2580/4063], Loss: 0.6725\n",
      "Epoch [7/10], Step [2590/4063], Loss: 1.1510\n",
      "Epoch [7/10], Step [2600/4063], Loss: 0.8523\n",
      "Epoch [7/10], Step [2610/4063], Loss: 1.1432\n",
      "Epoch [7/10], Step [2620/4063], Loss: 0.9826\n",
      "Epoch [7/10], Step [2630/4063], Loss: 0.7198\n",
      "Epoch [7/10], Step [2640/4063], Loss: 0.7102\n",
      "Epoch [7/10], Step [2650/4063], Loss: 0.7399\n",
      "Epoch [7/10], Step [2660/4063], Loss: 1.1720\n",
      "Epoch [7/10], Step [2670/4063], Loss: 0.9556\n",
      "Epoch [7/10], Step [2680/4063], Loss: 0.8956\n",
      "Epoch [7/10], Step [2690/4063], Loss: 0.6951\n",
      "Epoch [7/10], Step [2700/4063], Loss: 0.8294\n",
      "Epoch [7/10], Step [2710/4063], Loss: 1.0698\n",
      "Epoch [7/10], Step [2720/4063], Loss: 0.7957\n",
      "Epoch [7/10], Step [2730/4063], Loss: 1.3691\n",
      "Epoch [7/10], Step [2740/4063], Loss: 1.0220\n",
      "Epoch [7/10], Step [2750/4063], Loss: 0.7815\n",
      "Epoch [7/10], Step [2760/4063], Loss: 0.7846\n",
      "Epoch [7/10], Step [2770/4063], Loss: 0.8165\n",
      "Epoch [7/10], Step [2780/4063], Loss: 0.7412\n",
      "Epoch [7/10], Step [2790/4063], Loss: 0.8646\n",
      "Epoch [7/10], Step [2800/4063], Loss: 0.7418\n",
      "Epoch [7/10], Step [2810/4063], Loss: 1.0429\n",
      "Epoch [7/10], Step [2820/4063], Loss: 0.8712\n",
      "Epoch [7/10], Step [2830/4063], Loss: 1.0929\n",
      "Epoch [7/10], Step [2840/4063], Loss: 1.1277\n",
      "Epoch [7/10], Step [2850/4063], Loss: 1.0117\n",
      "Epoch [7/10], Step [2860/4063], Loss: 0.8816\n",
      "Epoch [7/10], Step [2870/4063], Loss: 0.6863\n",
      "Epoch [7/10], Step [2880/4063], Loss: 1.0106\n",
      "Epoch [7/10], Step [2890/4063], Loss: 0.5112\n",
      "Epoch [7/10], Step [2900/4063], Loss: 0.9284\n",
      "Epoch [7/10], Step [2910/4063], Loss: 0.8869\n",
      "Epoch [7/10], Step [2920/4063], Loss: 0.8864\n",
      "Epoch [7/10], Step [2930/4063], Loss: 0.8201\n",
      "Epoch [7/10], Step [2940/4063], Loss: 1.1290\n",
      "Epoch [7/10], Step [2950/4063], Loss: 0.9975\n",
      "Epoch [7/10], Step [2960/4063], Loss: 0.8781\n",
      "Epoch [7/10], Step [2970/4063], Loss: 0.6291\n",
      "Epoch [7/10], Step [2980/4063], Loss: 1.1085\n",
      "Epoch [7/10], Step [2990/4063], Loss: 0.9008\n",
      "Epoch [7/10], Step [3000/4063], Loss: 0.7856\n",
      "Epoch [7/10], Step [3010/4063], Loss: 0.7333\n",
      "Epoch [7/10], Step [3020/4063], Loss: 1.1075\n",
      "Epoch [7/10], Step [3030/4063], Loss: 0.7182\n",
      "Epoch [7/10], Step [3040/4063], Loss: 1.0639\n",
      "Epoch [7/10], Step [3050/4063], Loss: 0.8507\n",
      "Epoch [7/10], Step [3060/4063], Loss: 1.1648\n",
      "Epoch [7/10], Step [3070/4063], Loss: 0.7685\n",
      "Epoch [7/10], Step [3080/4063], Loss: 1.1419\n",
      "Epoch [7/10], Step [3090/4063], Loss: 0.8979\n",
      "Epoch [7/10], Step [3100/4063], Loss: 1.2176\n",
      "Epoch [7/10], Step [3110/4063], Loss: 1.3245\n",
      "Epoch [7/10], Step [3120/4063], Loss: 0.9104\n",
      "Epoch [7/10], Step [3130/4063], Loss: 0.8472\n",
      "Epoch [7/10], Step [3140/4063], Loss: 0.9420\n",
      "Epoch [7/10], Step [3150/4063], Loss: 0.9042\n",
      "Epoch [7/10], Step [3160/4063], Loss: 1.1759\n",
      "Epoch [7/10], Step [3170/4063], Loss: 1.0258\n",
      "Epoch [7/10], Step [3180/4063], Loss: 0.7020\n",
      "Epoch [7/10], Step [3190/4063], Loss: 1.5416\n",
      "Epoch [7/10], Step [3200/4063], Loss: 1.0551\n",
      "Epoch [7/10], Step [3210/4063], Loss: 1.1431\n",
      "Epoch [7/10], Step [3220/4063], Loss: 0.8646\n",
      "Epoch [7/10], Step [3230/4063], Loss: 1.3170\n",
      "Epoch [7/10], Step [3240/4063], Loss: 0.8953\n",
      "Epoch [7/10], Step [3250/4063], Loss: 0.9942\n",
      "Epoch [7/10], Step [3260/4063], Loss: 1.3862\n",
      "Epoch [7/10], Step [3270/4063], Loss: 1.2863\n",
      "Epoch [7/10], Step [3280/4063], Loss: 0.7288\n",
      "Epoch [7/10], Step [3290/4063], Loss: 0.5179\n",
      "Epoch [7/10], Step [3300/4063], Loss: 0.8661\n",
      "Epoch [7/10], Step [3310/4063], Loss: 0.9584\n",
      "Epoch [7/10], Step [3320/4063], Loss: 1.0062\n",
      "Epoch [7/10], Step [3330/4063], Loss: 0.9458\n",
      "Epoch [7/10], Step [3340/4063], Loss: 0.8541\n",
      "Epoch [7/10], Step [3350/4063], Loss: 0.9653\n",
      "Epoch [7/10], Step [3360/4063], Loss: 0.7599\n",
      "Epoch [7/10], Step [3370/4063], Loss: 0.7280\n",
      "Epoch [7/10], Step [3380/4063], Loss: 0.9184\n",
      "Epoch [7/10], Step [3390/4063], Loss: 0.8258\n",
      "Epoch [7/10], Step [3400/4063], Loss: 0.7841\n",
      "Epoch [7/10], Step [3410/4063], Loss: 0.7060\n",
      "Epoch [7/10], Step [3420/4063], Loss: 0.9749\n",
      "Epoch [7/10], Step [3430/4063], Loss: 1.0350\n",
      "Epoch [7/10], Step [3440/4063], Loss: 0.9576\n",
      "Epoch [7/10], Step [3450/4063], Loss: 0.8520\n",
      "Epoch [7/10], Step [3460/4063], Loss: 0.8639\n",
      "Epoch [7/10], Step [3470/4063], Loss: 0.8969\n",
      "Epoch [7/10], Step [3480/4063], Loss: 0.7603\n",
      "Epoch [7/10], Step [3490/4063], Loss: 0.8904\n",
      "Epoch [7/10], Step [3500/4063], Loss: 0.8499\n",
      "Epoch [7/10], Step [3510/4063], Loss: 1.1695\n",
      "Epoch [7/10], Step [3520/4063], Loss: 1.2109\n",
      "Epoch [7/10], Step [3530/4063], Loss: 1.0326\n",
      "Epoch [7/10], Step [3540/4063], Loss: 0.8536\n",
      "Epoch [7/10], Step [3550/4063], Loss: 0.8981\n",
      "Epoch [7/10], Step [3560/4063], Loss: 0.9774\n",
      "Epoch [7/10], Step [3570/4063], Loss: 0.8416\n",
      "Epoch [7/10], Step [3580/4063], Loss: 0.7139\n",
      "Epoch [7/10], Step [3590/4063], Loss: 0.6078\n",
      "Epoch [7/10], Step [3600/4063], Loss: 0.8597\n",
      "Epoch [7/10], Step [3610/4063], Loss: 0.9721\n",
      "Epoch [7/10], Step [3620/4063], Loss: 1.2172\n",
      "Epoch [7/10], Step [3630/4063], Loss: 0.7665\n",
      "Epoch [7/10], Step [3640/4063], Loss: 0.8427\n",
      "Epoch [7/10], Step [3650/4063], Loss: 1.2586\n",
      "Epoch [7/10], Step [3660/4063], Loss: 0.7267\n",
      "Epoch [7/10], Step [3670/4063], Loss: 1.0845\n",
      "Epoch [7/10], Step [3680/4063], Loss: 0.9562\n",
      "Epoch [7/10], Step [3690/4063], Loss: 1.1011\n",
      "Epoch [7/10], Step [3700/4063], Loss: 0.7333\n",
      "Epoch [7/10], Step [3710/4063], Loss: 0.9954\n",
      "Epoch [7/10], Step [3720/4063], Loss: 0.9473\n",
      "Epoch [7/10], Step [3730/4063], Loss: 1.1651\n",
      "Epoch [7/10], Step [3740/4063], Loss: 1.1045\n",
      "Epoch [7/10], Step [3750/4063], Loss: 0.8248\n",
      "Epoch [7/10], Step [3760/4063], Loss: 0.9337\n",
      "Epoch [7/10], Step [3770/4063], Loss: 1.1998\n",
      "Epoch [7/10], Step [3780/4063], Loss: 0.8884\n",
      "Epoch [7/10], Step [3790/4063], Loss: 0.8544\n",
      "Epoch [7/10], Step [3800/4063], Loss: 1.0984\n",
      "Epoch [7/10], Step [3810/4063], Loss: 0.7903\n",
      "Epoch [7/10], Step [3820/4063], Loss: 0.7488\n",
      "Epoch [7/10], Step [3830/4063], Loss: 0.9404\n",
      "Epoch [7/10], Step [3840/4063], Loss: 0.8471\n",
      "Epoch [7/10], Step [3850/4063], Loss: 0.9560\n",
      "Epoch [7/10], Step [3860/4063], Loss: 0.9863\n",
      "Epoch [7/10], Step [3870/4063], Loss: 1.3218\n",
      "Epoch [7/10], Step [3880/4063], Loss: 1.1914\n",
      "Epoch [7/10], Step [3890/4063], Loss: 1.0516\n",
      "Epoch [7/10], Step [3900/4063], Loss: 0.9591\n",
      "Epoch [7/10], Step [3910/4063], Loss: 0.8683\n",
      "Epoch [7/10], Step [3920/4063], Loss: 1.1565\n",
      "Epoch [7/10], Step [3930/4063], Loss: 1.0492\n",
      "Epoch [7/10], Step [3940/4063], Loss: 0.8579\n",
      "Epoch [7/10], Step [3950/4063], Loss: 1.2822\n",
      "Epoch [7/10], Step [3960/4063], Loss: 0.7860\n",
      "Epoch [7/10], Step [3970/4063], Loss: 1.0610\n",
      "Epoch [7/10], Step [3980/4063], Loss: 0.9395\n",
      "Epoch [7/10], Step [3990/4063], Loss: 0.7454\n",
      "Epoch [7/10], Step [4000/4063], Loss: 1.3194\n",
      "Epoch [7/10], Step [4010/4063], Loss: 0.9905\n",
      "Epoch [7/10], Step [4020/4063], Loss: 1.1309\n",
      "Epoch [7/10], Step [4030/4063], Loss: 1.0382\n",
      "Epoch [7/10], Step [4040/4063], Loss: 0.8968\n",
      "Epoch [7/10], Step [4050/4063], Loss: 0.6430\n",
      "Epoch [7/10], Step [4060/4063], Loss: 1.1099\n",
      "Epoch [8/10], Step [10/4063], Loss: 0.7367\n",
      "Epoch [8/10], Step [20/4063], Loss: 1.5467\n",
      "Epoch [8/10], Step [30/4063], Loss: 0.9449\n",
      "Epoch [8/10], Step [40/4063], Loss: 0.7312\n",
      "Epoch [8/10], Step [50/4063], Loss: 0.4794\n",
      "Epoch [8/10], Step [60/4063], Loss: 0.8513\n",
      "Epoch [8/10], Step [70/4063], Loss: 0.8556\n",
      "Epoch [8/10], Step [80/4063], Loss: 1.0952\n",
      "Epoch [8/10], Step [90/4063], Loss: 1.0823\n",
      "Epoch [8/10], Step [100/4063], Loss: 0.8248\n",
      "Epoch [8/10], Step [110/4063], Loss: 1.1961\n",
      "Epoch [8/10], Step [120/4063], Loss: 0.8211\n",
      "Epoch [8/10], Step [130/4063], Loss: 0.9429\n",
      "Epoch [8/10], Step [140/4063], Loss: 0.7337\n",
      "Epoch [8/10], Step [150/4063], Loss: 0.8386\n",
      "Epoch [8/10], Step [160/4063], Loss: 0.9723\n",
      "Epoch [8/10], Step [170/4063], Loss: 0.7687\n",
      "Epoch [8/10], Step [180/4063], Loss: 0.5928\n",
      "Epoch [8/10], Step [190/4063], Loss: 0.6994\n",
      "Epoch [8/10], Step [200/4063], Loss: 0.8598\n",
      "Epoch [8/10], Step [210/4063], Loss: 0.8005\n",
      "Epoch [8/10], Step [220/4063], Loss: 0.9310\n",
      "Epoch [8/10], Step [230/4063], Loss: 0.8527\n",
      "Epoch [8/10], Step [240/4063], Loss: 0.8450\n",
      "Epoch [8/10], Step [250/4063], Loss: 0.7977\n",
      "Epoch [8/10], Step [260/4063], Loss: 1.3086\n",
      "Epoch [8/10], Step [270/4063], Loss: 0.8015\n",
      "Epoch [8/10], Step [280/4063], Loss: 0.7407\n",
      "Epoch [8/10], Step [290/4063], Loss: 1.0741\n",
      "Epoch [8/10], Step [300/4063], Loss: 0.9458\n",
      "Epoch [8/10], Step [310/4063], Loss: 0.7249\n",
      "Epoch [8/10], Step [320/4063], Loss: 1.1786\n",
      "Epoch [8/10], Step [330/4063], Loss: 0.9042\n",
      "Epoch [8/10], Step [340/4063], Loss: 1.0164\n",
      "Epoch [8/10], Step [350/4063], Loss: 0.8669\n",
      "Epoch [8/10], Step [360/4063], Loss: 1.1503\n",
      "Epoch [8/10], Step [370/4063], Loss: 0.6825\n",
      "Epoch [8/10], Step [380/4063], Loss: 0.6723\n",
      "Epoch [8/10], Step [390/4063], Loss: 0.9875\n",
      "Epoch [8/10], Step [400/4063], Loss: 0.8635\n",
      "Epoch [8/10], Step [410/4063], Loss: 0.9112\n",
      "Epoch [8/10], Step [420/4063], Loss: 1.0635\n",
      "Epoch [8/10], Step [430/4063], Loss: 0.9922\n",
      "Epoch [8/10], Step [440/4063], Loss: 1.0869\n",
      "Epoch [8/10], Step [450/4063], Loss: 0.7815\n",
      "Epoch [8/10], Step [460/4063], Loss: 0.9434\n",
      "Epoch [8/10], Step [470/4063], Loss: 0.6282\n",
      "Epoch [8/10], Step [480/4063], Loss: 1.0838\n",
      "Epoch [8/10], Step [490/4063], Loss: 0.9623\n",
      "Epoch [8/10], Step [500/4063], Loss: 0.7308\n",
      "Epoch [8/10], Step [510/4063], Loss: 0.7290\n",
      "Epoch [8/10], Step [520/4063], Loss: 0.7229\n",
      "Epoch [8/10], Step [530/4063], Loss: 1.0685\n",
      "Epoch [8/10], Step [540/4063], Loss: 1.0570\n",
      "Epoch [8/10], Step [550/4063], Loss: 0.5553\n",
      "Epoch [8/10], Step [560/4063], Loss: 0.8205\n",
      "Epoch [8/10], Step [570/4063], Loss: 1.0295\n",
      "Epoch [8/10], Step [580/4063], Loss: 0.6571\n",
      "Epoch [8/10], Step [590/4063], Loss: 0.6933\n",
      "Epoch [8/10], Step [600/4063], Loss: 0.8402\n",
      "Epoch [8/10], Step [610/4063], Loss: 0.9974\n",
      "Epoch [8/10], Step [620/4063], Loss: 1.1908\n",
      "Epoch [8/10], Step [630/4063], Loss: 0.8267\n",
      "Epoch [8/10], Step [640/4063], Loss: 1.3749\n",
      "Epoch [8/10], Step [650/4063], Loss: 1.0048\n",
      "Epoch [8/10], Step [660/4063], Loss: 1.1247\n",
      "Epoch [8/10], Step [670/4063], Loss: 1.0067\n",
      "Epoch [8/10], Step [680/4063], Loss: 1.0904\n",
      "Epoch [8/10], Step [690/4063], Loss: 0.9503\n",
      "Epoch [8/10], Step [700/4063], Loss: 1.5271\n",
      "Epoch [8/10], Step [710/4063], Loss: 0.7942\n",
      "Epoch [8/10], Step [720/4063], Loss: 0.9536\n",
      "Epoch [8/10], Step [730/4063], Loss: 1.0332\n",
      "Epoch [8/10], Step [740/4063], Loss: 0.8033\n",
      "Epoch [8/10], Step [750/4063], Loss: 0.9899\n",
      "Epoch [8/10], Step [760/4063], Loss: 0.9776\n",
      "Epoch [8/10], Step [770/4063], Loss: 0.6019\n",
      "Epoch [8/10], Step [780/4063], Loss: 0.8088\n",
      "Epoch [8/10], Step [790/4063], Loss: 1.2807\n",
      "Epoch [8/10], Step [800/4063], Loss: 0.9762\n",
      "Epoch [8/10], Step [810/4063], Loss: 0.9033\n",
      "Epoch [8/10], Step [820/4063], Loss: 0.8282\n",
      "Epoch [8/10], Step [830/4063], Loss: 1.0527\n",
      "Epoch [8/10], Step [840/4063], Loss: 1.5131\n",
      "Epoch [8/10], Step [850/4063], Loss: 1.5796\n",
      "Epoch [8/10], Step [860/4063], Loss: 0.7017\n",
      "Epoch [8/10], Step [870/4063], Loss: 0.5023\n",
      "Epoch [8/10], Step [880/4063], Loss: 0.6049\n",
      "Epoch [8/10], Step [890/4063], Loss: 0.6534\n",
      "Epoch [8/10], Step [900/4063], Loss: 0.8438\n",
      "Epoch [8/10], Step [910/4063], Loss: 1.1354\n",
      "Epoch [8/10], Step [920/4063], Loss: 1.0763\n",
      "Epoch [8/10], Step [930/4063], Loss: 0.5429\n",
      "Epoch [8/10], Step [940/4063], Loss: 1.2053\n",
      "Epoch [8/10], Step [950/4063], Loss: 1.1701\n",
      "Epoch [8/10], Step [960/4063], Loss: 0.7730\n",
      "Epoch [8/10], Step [970/4063], Loss: 0.5711\n",
      "Epoch [8/10], Step [980/4063], Loss: 1.2003\n",
      "Epoch [8/10], Step [990/4063], Loss: 0.8864\n",
      "Epoch [8/10], Step [1000/4063], Loss: 0.9024\n",
      "Epoch [8/10], Step [1010/4063], Loss: 0.7496\n",
      "Epoch [8/10], Step [1020/4063], Loss: 0.6590\n",
      "Epoch [8/10], Step [1030/4063], Loss: 0.9561\n",
      "Epoch [8/10], Step [1040/4063], Loss: 0.7820\n",
      "Epoch [8/10], Step [1050/4063], Loss: 1.0919\n",
      "Epoch [8/10], Step [1060/4063], Loss: 1.2098\n",
      "Epoch [8/10], Step [1070/4063], Loss: 1.1275\n",
      "Epoch [8/10], Step [1080/4063], Loss: 0.9283\n",
      "Epoch [8/10], Step [1090/4063], Loss: 0.7486\n",
      "Epoch [8/10], Step [1100/4063], Loss: 0.6862\n",
      "Epoch [8/10], Step [1110/4063], Loss: 0.8449\n",
      "Epoch [8/10], Step [1120/4063], Loss: 0.7831\n",
      "Epoch [8/10], Step [1130/4063], Loss: 1.0179\n",
      "Epoch [8/10], Step [1140/4063], Loss: 0.9445\n",
      "Epoch [8/10], Step [1150/4063], Loss: 0.7919\n",
      "Epoch [8/10], Step [1160/4063], Loss: 0.9101\n",
      "Epoch [8/10], Step [1170/4063], Loss: 0.6739\n",
      "Epoch [8/10], Step [1180/4063], Loss: 0.8071\n",
      "Epoch [8/10], Step [1190/4063], Loss: 0.9654\n",
      "Epoch [8/10], Step [1200/4063], Loss: 0.7232\n",
      "Epoch [8/10], Step [1210/4063], Loss: 0.9574\n",
      "Epoch [8/10], Step [1220/4063], Loss: 0.5678\n",
      "Epoch [8/10], Step [1230/4063], Loss: 0.7992\n",
      "Epoch [8/10], Step [1240/4063], Loss: 0.7818\n",
      "Epoch [8/10], Step [1250/4063], Loss: 1.0551\n",
      "Epoch [8/10], Step [1260/4063], Loss: 0.8110\n",
      "Epoch [8/10], Step [1270/4063], Loss: 0.6415\n",
      "Epoch [8/10], Step [1280/4063], Loss: 0.8966\n",
      "Epoch [8/10], Step [1290/4063], Loss: 1.1483\n",
      "Epoch [8/10], Step [1300/4063], Loss: 0.7254\n",
      "Epoch [8/10], Step [1310/4063], Loss: 0.8579\n",
      "Epoch [8/10], Step [1320/4063], Loss: 1.3308\n",
      "Epoch [8/10], Step [1330/4063], Loss: 0.6381\n",
      "Epoch [8/10], Step [1340/4063], Loss: 0.5582\n",
      "Epoch [8/10], Step [1350/4063], Loss: 1.0665\n",
      "Epoch [8/10], Step [1360/4063], Loss: 0.8709\n",
      "Epoch [8/10], Step [1370/4063], Loss: 0.8347\n",
      "Epoch [8/10], Step [1380/4063], Loss: 1.0973\n",
      "Epoch [8/10], Step [1390/4063], Loss: 0.9675\n",
      "Epoch [8/10], Step [1400/4063], Loss: 1.2189\n",
      "Epoch [8/10], Step [1410/4063], Loss: 0.7345\n",
      "Epoch [8/10], Step [1420/4063], Loss: 0.9612\n",
      "Epoch [8/10], Step [1430/4063], Loss: 1.3715\n",
      "Epoch [8/10], Step [1440/4063], Loss: 1.1669\n",
      "Epoch [8/10], Step [1450/4063], Loss: 0.9097\n",
      "Epoch [8/10], Step [1460/4063], Loss: 1.3687\n",
      "Epoch [8/10], Step [1470/4063], Loss: 0.4775\n",
      "Epoch [8/10], Step [1480/4063], Loss: 0.8862\n",
      "Epoch [8/10], Step [1490/4063], Loss: 0.9714\n",
      "Epoch [8/10], Step [1500/4063], Loss: 1.1541\n",
      "Epoch [8/10], Step [1510/4063], Loss: 0.5892\n",
      "Epoch [8/10], Step [1520/4063], Loss: 0.9080\n",
      "Epoch [8/10], Step [1530/4063], Loss: 0.7846\n",
      "Epoch [8/10], Step [1540/4063], Loss: 1.2670\n",
      "Epoch [8/10], Step [1550/4063], Loss: 0.7443\n",
      "Epoch [8/10], Step [1560/4063], Loss: 0.6107\n",
      "Epoch [8/10], Step [1570/4063], Loss: 1.3568\n",
      "Epoch [8/10], Step [1580/4063], Loss: 0.9965\n",
      "Epoch [8/10], Step [1590/4063], Loss: 0.7318\n",
      "Epoch [8/10], Step [1600/4063], Loss: 1.1764\n",
      "Epoch [8/10], Step [1610/4063], Loss: 0.6714\n",
      "Epoch [8/10], Step [1620/4063], Loss: 0.8819\n",
      "Epoch [8/10], Step [1630/4063], Loss: 1.5081\n",
      "Epoch [8/10], Step [1640/4063], Loss: 0.6762\n",
      "Epoch [8/10], Step [1650/4063], Loss: 0.5173\n",
      "Epoch [8/10], Step [1660/4063], Loss: 0.8165\n",
      "Epoch [8/10], Step [1670/4063], Loss: 0.8304\n",
      "Epoch [8/10], Step [1680/4063], Loss: 0.7689\n",
      "Epoch [8/10], Step [1690/4063], Loss: 0.8462\n",
      "Epoch [8/10], Step [1700/4063], Loss: 0.7482\n",
      "Epoch [8/10], Step [1710/4063], Loss: 0.7563\n",
      "Epoch [8/10], Step [1720/4063], Loss: 0.5862\n",
      "Epoch [8/10], Step [1730/4063], Loss: 0.7283\n",
      "Epoch [8/10], Step [1740/4063], Loss: 1.0841\n",
      "Epoch [8/10], Step [1750/4063], Loss: 0.4293\n",
      "Epoch [8/10], Step [1760/4063], Loss: 0.6965\n",
      "Epoch [8/10], Step [1770/4063], Loss: 0.7798\n",
      "Epoch [8/10], Step [1780/4063], Loss: 0.8766\n",
      "Epoch [8/10], Step [1790/4063], Loss: 0.6471\n",
      "Epoch [8/10], Step [1800/4063], Loss: 0.6823\n",
      "Epoch [8/10], Step [1810/4063], Loss: 1.2704\n",
      "Epoch [8/10], Step [1820/4063], Loss: 0.8118\n",
      "Epoch [8/10], Step [1830/4063], Loss: 0.6880\n",
      "Epoch [8/10], Step [1840/4063], Loss: 0.7574\n",
      "Epoch [8/10], Step [1850/4063], Loss: 0.8731\n",
      "Epoch [8/10], Step [1860/4063], Loss: 0.8633\n",
      "Epoch [8/10], Step [1870/4063], Loss: 0.9145\n",
      "Epoch [8/10], Step [1880/4063], Loss: 0.7336\n",
      "Epoch [8/10], Step [1890/4063], Loss: 0.9730\n",
      "Epoch [8/10], Step [1900/4063], Loss: 0.8700\n",
      "Epoch [8/10], Step [1910/4063], Loss: 0.9934\n",
      "Epoch [8/10], Step [1920/4063], Loss: 0.7614\n",
      "Epoch [8/10], Step [1930/4063], Loss: 0.7182\n",
      "Epoch [8/10], Step [1940/4063], Loss: 1.3744\n",
      "Epoch [8/10], Step [1950/4063], Loss: 0.8880\n",
      "Epoch [8/10], Step [1960/4063], Loss: 1.1258\n",
      "Epoch [8/10], Step [1970/4063], Loss: 1.0293\n",
      "Epoch [8/10], Step [1980/4063], Loss: 0.5522\n",
      "Epoch [8/10], Step [1990/4063], Loss: 0.6754\n",
      "Epoch [8/10], Step [2000/4063], Loss: 0.6478\n",
      "Epoch [8/10], Step [2010/4063], Loss: 0.9115\n",
      "Epoch [8/10], Step [2020/4063], Loss: 0.6742\n",
      "Epoch [8/10], Step [2030/4063], Loss: 0.8731\n",
      "Epoch [8/10], Step [2040/4063], Loss: 0.7741\n",
      "Epoch [8/10], Step [2050/4063], Loss: 0.8112\n",
      "Epoch [8/10], Step [2060/4063], Loss: 0.9097\n",
      "Epoch [8/10], Step [2070/4063], Loss: 1.0265\n",
      "Epoch [8/10], Step [2080/4063], Loss: 0.9212\n",
      "Epoch [8/10], Step [2090/4063], Loss: 0.9397\n",
      "Epoch [8/10], Step [2100/4063], Loss: 0.9593\n",
      "Epoch [8/10], Step [2110/4063], Loss: 1.0615\n",
      "Epoch [8/10], Step [2120/4063], Loss: 0.7913\n",
      "Epoch [8/10], Step [2130/4063], Loss: 1.0039\n",
      "Epoch [8/10], Step [2140/4063], Loss: 0.8254\n",
      "Epoch [8/10], Step [2150/4063], Loss: 1.1129\n",
      "Epoch [8/10], Step [2160/4063], Loss: 0.9434\n",
      "Epoch [8/10], Step [2170/4063], Loss: 1.0905\n",
      "Epoch [8/10], Step [2180/4063], Loss: 0.7970\n",
      "Epoch [8/10], Step [2190/4063], Loss: 0.8567\n",
      "Epoch [8/10], Step [2200/4063], Loss: 0.5955\n",
      "Epoch [8/10], Step [2210/4063], Loss: 0.9670\n",
      "Epoch [8/10], Step [2220/4063], Loss: 1.0745\n",
      "Epoch [8/10], Step [2230/4063], Loss: 0.8931\n",
      "Epoch [8/10], Step [2240/4063], Loss: 0.8497\n",
      "Epoch [8/10], Step [2250/4063], Loss: 0.9426\n",
      "Epoch [8/10], Step [2260/4063], Loss: 1.2639\n",
      "Epoch [8/10], Step [2270/4063], Loss: 0.6726\n",
      "Epoch [8/10], Step [2280/4063], Loss: 0.8393\n",
      "Epoch [8/10], Step [2290/4063], Loss: 0.6969\n",
      "Epoch [8/10], Step [2300/4063], Loss: 1.2782\n",
      "Epoch [8/10], Step [2310/4063], Loss: 0.7417\n",
      "Epoch [8/10], Step [2320/4063], Loss: 0.9061\n",
      "Epoch [8/10], Step [2330/4063], Loss: 0.9119\n",
      "Epoch [8/10], Step [2340/4063], Loss: 1.0615\n",
      "Epoch [8/10], Step [2350/4063], Loss: 0.8596\n",
      "Epoch [8/10], Step [2360/4063], Loss: 0.4521\n",
      "Epoch [8/10], Step [2370/4063], Loss: 1.0255\n",
      "Epoch [8/10], Step [2380/4063], Loss: 1.3402\n",
      "Epoch [8/10], Step [2390/4063], Loss: 1.0799\n",
      "Epoch [8/10], Step [2400/4063], Loss: 0.7643\n",
      "Epoch [8/10], Step [2410/4063], Loss: 0.7272\n",
      "Epoch [8/10], Step [2420/4063], Loss: 1.1619\n",
      "Epoch [8/10], Step [2430/4063], Loss: 0.9819\n",
      "Epoch [8/10], Step [2440/4063], Loss: 1.0213\n",
      "Epoch [8/10], Step [2450/4063], Loss: 0.9398\n",
      "Epoch [8/10], Step [2460/4063], Loss: 0.7562\n",
      "Epoch [8/10], Step [2470/4063], Loss: 0.8413\n",
      "Epoch [8/10], Step [2480/4063], Loss: 1.0708\n",
      "Epoch [8/10], Step [2490/4063], Loss: 1.4298\n",
      "Epoch [8/10], Step [2500/4063], Loss: 1.1973\n",
      "Epoch [8/10], Step [2510/4063], Loss: 0.7584\n",
      "Epoch [8/10], Step [2520/4063], Loss: 0.9029\n",
      "Epoch [8/10], Step [2530/4063], Loss: 1.1442\n",
      "Epoch [8/10], Step [2540/4063], Loss: 0.9771\n",
      "Epoch [8/10], Step [2550/4063], Loss: 0.8468\n",
      "Epoch [8/10], Step [2560/4063], Loss: 0.9975\n",
      "Epoch [8/10], Step [2570/4063], Loss: 0.7770\n",
      "Epoch [8/10], Step [2580/4063], Loss: 0.8094\n",
      "Epoch [8/10], Step [2590/4063], Loss: 1.1687\n",
      "Epoch [8/10], Step [2600/4063], Loss: 1.0071\n",
      "Epoch [8/10], Step [2610/4063], Loss: 0.9540\n",
      "Epoch [8/10], Step [2620/4063], Loss: 0.8839\n",
      "Epoch [8/10], Step [2630/4063], Loss: 1.0461\n",
      "Epoch [8/10], Step [2640/4063], Loss: 0.9975\n",
      "Epoch [8/10], Step [2650/4063], Loss: 0.9962\n",
      "Epoch [8/10], Step [2660/4063], Loss: 0.9704\n",
      "Epoch [8/10], Step [2670/4063], Loss: 0.5446\n",
      "Epoch [8/10], Step [2680/4063], Loss: 0.8909\n",
      "Epoch [8/10], Step [2690/4063], Loss: 0.6888\n",
      "Epoch [8/10], Step [2700/4063], Loss: 1.0272\n",
      "Epoch [8/10], Step [2710/4063], Loss: 0.8394\n",
      "Epoch [8/10], Step [2720/4063], Loss: 0.5026\n",
      "Epoch [8/10], Step [2730/4063], Loss: 0.8440\n",
      "Epoch [8/10], Step [2740/4063], Loss: 1.0586\n",
      "Epoch [8/10], Step [2750/4063], Loss: 0.8850\n",
      "Epoch [8/10], Step [2760/4063], Loss: 0.9017\n",
      "Epoch [8/10], Step [2770/4063], Loss: 0.9229\n",
      "Epoch [8/10], Step [2780/4063], Loss: 0.7662\n",
      "Epoch [8/10], Step [2790/4063], Loss: 0.8541\n",
      "Epoch [8/10], Step [2800/4063], Loss: 0.8563\n",
      "Epoch [8/10], Step [2810/4063], Loss: 0.8985\n",
      "Epoch [8/10], Step [2820/4063], Loss: 1.2793\n",
      "Epoch [8/10], Step [2830/4063], Loss: 0.8360\n",
      "Epoch [8/10], Step [2840/4063], Loss: 0.9183\n",
      "Epoch [8/10], Step [2850/4063], Loss: 0.7677\n",
      "Epoch [8/10], Step [2860/4063], Loss: 0.8870\n",
      "Epoch [8/10], Step [2870/4063], Loss: 0.7257\n",
      "Epoch [8/10], Step [2880/4063], Loss: 0.8967\n",
      "Epoch [8/10], Step [2890/4063], Loss: 0.8325\n",
      "Epoch [8/10], Step [2900/4063], Loss: 1.0814\n",
      "Epoch [8/10], Step [2910/4063], Loss: 0.9660\n",
      "Epoch [8/10], Step [2920/4063], Loss: 1.3246\n",
      "Epoch [8/10], Step [2930/4063], Loss: 1.4994\n",
      "Epoch [8/10], Step [2940/4063], Loss: 0.8950\n",
      "Epoch [8/10], Step [2950/4063], Loss: 0.8128\n",
      "Epoch [8/10], Step [2960/4063], Loss: 0.9309\n",
      "Epoch [8/10], Step [2970/4063], Loss: 1.1045\n",
      "Epoch [8/10], Step [2980/4063], Loss: 0.9608\n",
      "Epoch [8/10], Step [2990/4063], Loss: 1.2097\n",
      "Epoch [8/10], Step [3000/4063], Loss: 0.8076\n",
      "Epoch [8/10], Step [3010/4063], Loss: 1.1370\n",
      "Epoch [8/10], Step [3020/4063], Loss: 0.4636\n",
      "Epoch [8/10], Step [3030/4063], Loss: 1.3497\n",
      "Epoch [8/10], Step [3040/4063], Loss: 0.8388\n",
      "Epoch [8/10], Step [3050/4063], Loss: 0.6713\n",
      "Epoch [8/10], Step [3060/4063], Loss: 0.9106\n",
      "Epoch [8/10], Step [3070/4063], Loss: 0.9211\n",
      "Epoch [8/10], Step [3080/4063], Loss: 0.9019\n",
      "Epoch [8/10], Step [3090/4063], Loss: 0.9119\n",
      "Epoch [8/10], Step [3100/4063], Loss: 0.8925\n",
      "Epoch [8/10], Step [3110/4063], Loss: 0.7616\n",
      "Epoch [8/10], Step [3120/4063], Loss: 0.6302\n",
      "Epoch [8/10], Step [3130/4063], Loss: 0.9130\n",
      "Epoch [8/10], Step [3140/4063], Loss: 1.1220\n",
      "Epoch [8/10], Step [3150/4063], Loss: 1.1225\n",
      "Epoch [8/10], Step [3160/4063], Loss: 0.7413\n",
      "Epoch [8/10], Step [3170/4063], Loss: 1.1565\n",
      "Epoch [8/10], Step [3180/4063], Loss: 1.2073\n",
      "Epoch [8/10], Step [3190/4063], Loss: 0.9230\n",
      "Epoch [8/10], Step [3200/4063], Loss: 0.6612\n",
      "Epoch [8/10], Step [3210/4063], Loss: 0.7034\n",
      "Epoch [8/10], Step [3220/4063], Loss: 1.0361\n",
      "Epoch [8/10], Step [3230/4063], Loss: 1.1758\n",
      "Epoch [8/10], Step [3240/4063], Loss: 0.9092\n",
      "Epoch [8/10], Step [3250/4063], Loss: 0.7923\n",
      "Epoch [8/10], Step [3260/4063], Loss: 0.9197\n",
      "Epoch [8/10], Step [3270/4063], Loss: 0.6081\n",
      "Epoch [8/10], Step [3280/4063], Loss: 0.9217\n",
      "Epoch [8/10], Step [3290/4063], Loss: 0.7483\n",
      "Epoch [8/10], Step [3300/4063], Loss: 1.0093\n",
      "Epoch [8/10], Step [3310/4063], Loss: 0.8724\n",
      "Epoch [8/10], Step [3320/4063], Loss: 0.8156\n",
      "Epoch [8/10], Step [3330/4063], Loss: 0.9579\n",
      "Epoch [8/10], Step [3340/4063], Loss: 0.8219\n",
      "Epoch [8/10], Step [3350/4063], Loss: 0.9254\n",
      "Epoch [8/10], Step [3360/4063], Loss: 1.1281\n",
      "Epoch [8/10], Step [3370/4063], Loss: 0.9236\n",
      "Epoch [8/10], Step [3380/4063], Loss: 0.6360\n",
      "Epoch [8/10], Step [3390/4063], Loss: 1.1000\n",
      "Epoch [8/10], Step [3400/4063], Loss: 1.1074\n",
      "Epoch [8/10], Step [3410/4063], Loss: 0.7757\n",
      "Epoch [8/10], Step [3420/4063], Loss: 0.9194\n",
      "Epoch [8/10], Step [3430/4063], Loss: 1.1465\n",
      "Epoch [8/10], Step [3440/4063], Loss: 0.7342\n",
      "Epoch [8/10], Step [3450/4063], Loss: 1.0837\n",
      "Epoch [8/10], Step [3460/4063], Loss: 0.6374\n",
      "Epoch [8/10], Step [3470/4063], Loss: 1.0290\n",
      "Epoch [8/10], Step [3480/4063], Loss: 0.9099\n",
      "Epoch [8/10], Step [3490/4063], Loss: 0.7924\n",
      "Epoch [8/10], Step [3500/4063], Loss: 0.6951\n",
      "Epoch [8/10], Step [3510/4063], Loss: 0.9569\n",
      "Epoch [8/10], Step [3520/4063], Loss: 0.6578\n",
      "Epoch [8/10], Step [3530/4063], Loss: 0.8849\n",
      "Epoch [8/10], Step [3540/4063], Loss: 0.5818\n",
      "Epoch [8/10], Step [3550/4063], Loss: 0.7862\n",
      "Epoch [8/10], Step [3560/4063], Loss: 1.3374\n",
      "Epoch [8/10], Step [3570/4063], Loss: 1.0851\n",
      "Epoch [8/10], Step [3580/4063], Loss: 1.1754\n",
      "Epoch [8/10], Step [3590/4063], Loss: 0.7016\n",
      "Epoch [8/10], Step [3600/4063], Loss: 1.1557\n",
      "Epoch [8/10], Step [3610/4063], Loss: 0.8760\n",
      "Epoch [8/10], Step [3620/4063], Loss: 1.1473\n",
      "Epoch [8/10], Step [3630/4063], Loss: 0.9204\n",
      "Epoch [8/10], Step [3640/4063], Loss: 0.6852\n",
      "Epoch [8/10], Step [3650/4063], Loss: 0.6956\n",
      "Epoch [8/10], Step [3660/4063], Loss: 1.2588\n",
      "Epoch [8/10], Step [3670/4063], Loss: 0.9476\n",
      "Epoch [8/10], Step [3680/4063], Loss: 0.7367\n",
      "Epoch [8/10], Step [3690/4063], Loss: 1.2798\n",
      "Epoch [8/10], Step [3700/4063], Loss: 0.7537\n",
      "Epoch [8/10], Step [3710/4063], Loss: 0.7772\n",
      "Epoch [8/10], Step [3720/4063], Loss: 0.8923\n",
      "Epoch [8/10], Step [3730/4063], Loss: 1.1383\n",
      "Epoch [8/10], Step [3740/4063], Loss: 0.9366\n",
      "Epoch [8/10], Step [3750/4063], Loss: 1.0711\n",
      "Epoch [8/10], Step [3760/4063], Loss: 1.0121\n",
      "Epoch [8/10], Step [3770/4063], Loss: 0.8307\n",
      "Epoch [8/10], Step [3780/4063], Loss: 0.6454\n",
      "Epoch [8/10], Step [3790/4063], Loss: 0.6686\n",
      "Epoch [8/10], Step [3800/4063], Loss: 1.2259\n",
      "Epoch [8/10], Step [3810/4063], Loss: 0.9669\n",
      "Epoch [8/10], Step [3820/4063], Loss: 0.5431\n",
      "Epoch [8/10], Step [3830/4063], Loss: 0.5348\n",
      "Epoch [8/10], Step [3840/4063], Loss: 1.0875\n",
      "Epoch [8/10], Step [3850/4063], Loss: 0.9369\n",
      "Epoch [8/10], Step [3860/4063], Loss: 0.8569\n",
      "Epoch [8/10], Step [3870/4063], Loss: 0.9348\n",
      "Epoch [8/10], Step [3880/4063], Loss: 0.6022\n",
      "Epoch [8/10], Step [3890/4063], Loss: 1.1387\n",
      "Epoch [8/10], Step [3900/4063], Loss: 0.6059\n",
      "Epoch [8/10], Step [3910/4063], Loss: 0.6856\n",
      "Epoch [8/10], Step [3920/4063], Loss: 1.3267\n",
      "Epoch [8/10], Step [3930/4063], Loss: 0.9618\n",
      "Epoch [8/10], Step [3940/4063], Loss: 0.8882\n",
      "Epoch [8/10], Step [3950/4063], Loss: 1.0296\n",
      "Epoch [8/10], Step [3960/4063], Loss: 1.0187\n",
      "Epoch [8/10], Step [3970/4063], Loss: 0.8372\n",
      "Epoch [8/10], Step [3980/4063], Loss: 1.8374\n",
      "Epoch [8/10], Step [3990/4063], Loss: 0.9159\n",
      "Epoch [8/10], Step [4000/4063], Loss: 0.9956\n",
      "Epoch [8/10], Step [4010/4063], Loss: 0.9476\n",
      "Epoch [8/10], Step [4020/4063], Loss: 0.6963\n",
      "Epoch [8/10], Step [4030/4063], Loss: 1.2771\n",
      "Epoch [8/10], Step [4040/4063], Loss: 1.1218\n",
      "Epoch [8/10], Step [4050/4063], Loss: 0.9191\n",
      "Epoch [8/10], Step [4060/4063], Loss: 0.5253\n",
      "Epoch [9/10], Step [10/4063], Loss: 0.8464\n",
      "Epoch [9/10], Step [20/4063], Loss: 0.9398\n",
      "Epoch [9/10], Step [30/4063], Loss: 0.8418\n",
      "Epoch [9/10], Step [40/4063], Loss: 0.8865\n",
      "Epoch [9/10], Step [50/4063], Loss: 0.8140\n",
      "Epoch [9/10], Step [60/4063], Loss: 0.5193\n",
      "Epoch [9/10], Step [70/4063], Loss: 0.9943\n",
      "Epoch [9/10], Step [80/4063], Loss: 1.0545\n",
      "Epoch [9/10], Step [90/4063], Loss: 1.1146\n",
      "Epoch [9/10], Step [100/4063], Loss: 0.5659\n",
      "Epoch [9/10], Step [110/4063], Loss: 0.8029\n",
      "Epoch [9/10], Step [120/4063], Loss: 0.8061\n",
      "Epoch [9/10], Step [130/4063], Loss: 0.6122\n",
      "Epoch [9/10], Step [140/4063], Loss: 0.6779\n",
      "Epoch [9/10], Step [150/4063], Loss: 0.8597\n",
      "Epoch [9/10], Step [160/4063], Loss: 0.9352\n",
      "Epoch [9/10], Step [170/4063], Loss: 0.7585\n",
      "Epoch [9/10], Step [180/4063], Loss: 0.8964\n",
      "Epoch [9/10], Step [190/4063], Loss: 0.7578\n",
      "Epoch [9/10], Step [200/4063], Loss: 0.9696\n",
      "Epoch [9/10], Step [210/4063], Loss: 0.7750\n",
      "Epoch [9/10], Step [220/4063], Loss: 1.0947\n",
      "Epoch [9/10], Step [230/4063], Loss: 0.8854\n",
      "Epoch [9/10], Step [240/4063], Loss: 1.1758\n",
      "Epoch [9/10], Step [250/4063], Loss: 0.6765\n",
      "Epoch [9/10], Step [260/4063], Loss: 1.0170\n",
      "Epoch [9/10], Step [270/4063], Loss: 1.1739\n",
      "Epoch [9/10], Step [280/4063], Loss: 0.9656\n",
      "Epoch [9/10], Step [290/4063], Loss: 0.7229\n",
      "Epoch [9/10], Step [300/4063], Loss: 1.2799\n",
      "Epoch [9/10], Step [310/4063], Loss: 0.9430\n",
      "Epoch [9/10], Step [320/4063], Loss: 1.2155\n",
      "Epoch [9/10], Step [330/4063], Loss: 0.8434\n",
      "Epoch [9/10], Step [340/4063], Loss: 0.7824\n",
      "Epoch [9/10], Step [350/4063], Loss: 0.8628\n",
      "Epoch [9/10], Step [360/4063], Loss: 0.7344\n",
      "Epoch [9/10], Step [370/4063], Loss: 0.9616\n",
      "Epoch [9/10], Step [380/4063], Loss: 0.8966\n",
      "Epoch [9/10], Step [390/4063], Loss: 1.0148\n",
      "Epoch [9/10], Step [400/4063], Loss: 0.8702\n",
      "Epoch [9/10], Step [410/4063], Loss: 1.2183\n",
      "Epoch [9/10], Step [420/4063], Loss: 0.6392\n",
      "Epoch [9/10], Step [430/4063], Loss: 1.1445\n",
      "Epoch [9/10], Step [440/4063], Loss: 1.1519\n",
      "Epoch [9/10], Step [450/4063], Loss: 1.2136\n",
      "Epoch [9/10], Step [460/4063], Loss: 0.5424\n",
      "Epoch [9/10], Step [470/4063], Loss: 0.7542\n",
      "Epoch [9/10], Step [480/4063], Loss: 0.9867\n",
      "Epoch [9/10], Step [490/4063], Loss: 0.9039\n",
      "Epoch [9/10], Step [500/4063], Loss: 1.0187\n",
      "Epoch [9/10], Step [510/4063], Loss: 0.8668\n",
      "Epoch [9/10], Step [520/4063], Loss: 0.9566\n",
      "Epoch [9/10], Step [530/4063], Loss: 1.0407\n",
      "Epoch [9/10], Step [540/4063], Loss: 0.8166\n",
      "Epoch [9/10], Step [550/4063], Loss: 0.7751\n",
      "Epoch [9/10], Step [560/4063], Loss: 0.7219\n",
      "Epoch [9/10], Step [570/4063], Loss: 0.7901\n",
      "Epoch [9/10], Step [580/4063], Loss: 1.0873\n",
      "Epoch [9/10], Step [590/4063], Loss: 0.9982\n",
      "Epoch [9/10], Step [600/4063], Loss: 1.2578\n",
      "Epoch [9/10], Step [610/4063], Loss: 0.9894\n",
      "Epoch [9/10], Step [620/4063], Loss: 0.8281\n",
      "Epoch [9/10], Step [630/4063], Loss: 1.0879\n",
      "Epoch [9/10], Step [640/4063], Loss: 0.7090\n",
      "Epoch [9/10], Step [650/4063], Loss: 0.9347\n",
      "Epoch [9/10], Step [660/4063], Loss: 1.2436\n",
      "Epoch [9/10], Step [670/4063], Loss: 1.0937\n",
      "Epoch [9/10], Step [680/4063], Loss: 0.4762\n",
      "Epoch [9/10], Step [690/4063], Loss: 0.7177\n",
      "Epoch [9/10], Step [700/4063], Loss: 0.5363\n",
      "Epoch [9/10], Step [710/4063], Loss: 0.7736\n",
      "Epoch [9/10], Step [720/4063], Loss: 0.6090\n",
      "Epoch [9/10], Step [730/4063], Loss: 0.6298\n",
      "Epoch [9/10], Step [740/4063], Loss: 0.4732\n",
      "Epoch [9/10], Step [750/4063], Loss: 0.9640\n",
      "Epoch [9/10], Step [760/4063], Loss: 1.0666\n",
      "Epoch [9/10], Step [770/4063], Loss: 0.8441\n",
      "Epoch [9/10], Step [780/4063], Loss: 0.5683\n",
      "Epoch [9/10], Step [790/4063], Loss: 0.7677\n",
      "Epoch [9/10], Step [800/4063], Loss: 0.6041\n",
      "Epoch [9/10], Step [810/4063], Loss: 0.8064\n",
      "Epoch [9/10], Step [820/4063], Loss: 0.7069\n",
      "Epoch [9/10], Step [830/4063], Loss: 1.2803\n",
      "Epoch [9/10], Step [840/4063], Loss: 1.1765\n",
      "Epoch [9/10], Step [850/4063], Loss: 0.2232\n",
      "Epoch [9/10], Step [860/4063], Loss: 1.0629\n",
      "Epoch [9/10], Step [870/4063], Loss: 1.1930\n",
      "Epoch [9/10], Step [880/4063], Loss: 0.6960\n",
      "Epoch [9/10], Step [890/4063], Loss: 0.3954\n",
      "Epoch [9/10], Step [900/4063], Loss: 0.6327\n",
      "Epoch [9/10], Step [910/4063], Loss: 0.8144\n",
      "Epoch [9/10], Step [920/4063], Loss: 0.7444\n",
      "Epoch [9/10], Step [930/4063], Loss: 0.8284\n",
      "Epoch [9/10], Step [940/4063], Loss: 0.8573\n",
      "Epoch [9/10], Step [950/4063], Loss: 0.7488\n",
      "Epoch [9/10], Step [960/4063], Loss: 1.0936\n",
      "Epoch [9/10], Step [970/4063], Loss: 1.0918\n",
      "Epoch [9/10], Step [980/4063], Loss: 0.9439\n",
      "Epoch [9/10], Step [990/4063], Loss: 0.9441\n",
      "Epoch [9/10], Step [1000/4063], Loss: 0.8243\n",
      "Epoch [9/10], Step [1010/4063], Loss: 0.5316\n",
      "Epoch [9/10], Step [1020/4063], Loss: 0.6695\n",
      "Epoch [9/10], Step [1030/4063], Loss: 0.7985\n",
      "Epoch [9/10], Step [1040/4063], Loss: 1.0664\n",
      "Epoch [9/10], Step [1050/4063], Loss: 1.0568\n",
      "Epoch [9/10], Step [1060/4063], Loss: 0.7507\n",
      "Epoch [9/10], Step [1070/4063], Loss: 0.8645\n",
      "Epoch [9/10], Step [1080/4063], Loss: 0.5125\n",
      "Epoch [9/10], Step [1090/4063], Loss: 0.8296\n",
      "Epoch [9/10], Step [1100/4063], Loss: 0.7015\n",
      "Epoch [9/10], Step [1110/4063], Loss: 1.2827\n",
      "Epoch [9/10], Step [1120/4063], Loss: 0.7512\n",
      "Epoch [9/10], Step [1130/4063], Loss: 0.9953\n",
      "Epoch [9/10], Step [1140/4063], Loss: 0.8738\n",
      "Epoch [9/10], Step [1150/4063], Loss: 1.1625\n",
      "Epoch [9/10], Step [1160/4063], Loss: 0.4901\n",
      "Epoch [9/10], Step [1170/4063], Loss: 0.7577\n",
      "Epoch [9/10], Step [1180/4063], Loss: 0.9194\n",
      "Epoch [9/10], Step [1190/4063], Loss: 1.0668\n",
      "Epoch [9/10], Step [1200/4063], Loss: 0.9662\n",
      "Epoch [9/10], Step [1210/4063], Loss: 0.9417\n",
      "Epoch [9/10], Step [1220/4063], Loss: 0.7450\n",
      "Epoch [9/10], Step [1230/4063], Loss: 0.6447\n",
      "Epoch [9/10], Step [1240/4063], Loss: 1.0166\n",
      "Epoch [9/10], Step [1250/4063], Loss: 0.5488\n",
      "Epoch [9/10], Step [1260/4063], Loss: 0.8802\n",
      "Epoch [9/10], Step [1270/4063], Loss: 0.8460\n",
      "Epoch [9/10], Step [1280/4063], Loss: 0.7989\n",
      "Epoch [9/10], Step [1290/4063], Loss: 1.0449\n",
      "Epoch [9/10], Step [1300/4063], Loss: 1.1019\n",
      "Epoch [9/10], Step [1310/4063], Loss: 1.0192\n",
      "Epoch [9/10], Step [1320/4063], Loss: 1.1160\n",
      "Epoch [9/10], Step [1330/4063], Loss: 0.8206\n",
      "Epoch [9/10], Step [1340/4063], Loss: 0.9624\n",
      "Epoch [9/10], Step [1350/4063], Loss: 1.2766\n",
      "Epoch [9/10], Step [1360/4063], Loss: 1.0503\n",
      "Epoch [9/10], Step [1370/4063], Loss: 1.2176\n",
      "Epoch [9/10], Step [1380/4063], Loss: 0.7991\n",
      "Epoch [9/10], Step [1390/4063], Loss: 0.8596\n",
      "Epoch [9/10], Step [1400/4063], Loss: 0.8174\n",
      "Epoch [9/10], Step [1410/4063], Loss: 0.9939\n",
      "Epoch [9/10], Step [1420/4063], Loss: 0.9108\n",
      "Epoch [9/10], Step [1430/4063], Loss: 1.2007\n",
      "Epoch [9/10], Step [1440/4063], Loss: 1.2756\n",
      "Epoch [9/10], Step [1450/4063], Loss: 1.0186\n",
      "Epoch [9/10], Step [1460/4063], Loss: 1.1945\n",
      "Epoch [9/10], Step [1470/4063], Loss: 1.1861\n",
      "Epoch [9/10], Step [1480/4063], Loss: 0.8718\n",
      "Epoch [9/10], Step [1490/4063], Loss: 0.8650\n",
      "Epoch [9/10], Step [1500/4063], Loss: 1.0145\n",
      "Epoch [9/10], Step [1510/4063], Loss: 0.8091\n",
      "Epoch [9/10], Step [1520/4063], Loss: 0.9268\n",
      "Epoch [9/10], Step [1530/4063], Loss: 0.6946\n",
      "Epoch [9/10], Step [1540/4063], Loss: 0.6175\n",
      "Epoch [9/10], Step [1550/4063], Loss: 0.9556\n",
      "Epoch [9/10], Step [1560/4063], Loss: 0.7691\n",
      "Epoch [9/10], Step [1570/4063], Loss: 0.9190\n",
      "Epoch [9/10], Step [1580/4063], Loss: 1.2993\n",
      "Epoch [9/10], Step [1590/4063], Loss: 0.9432\n",
      "Epoch [9/10], Step [1600/4063], Loss: 0.6016\n",
      "Epoch [9/10], Step [1610/4063], Loss: 0.7041\n",
      "Epoch [9/10], Step [1620/4063], Loss: 1.0532\n",
      "Epoch [9/10], Step [1630/4063], Loss: 0.7936\n",
      "Epoch [9/10], Step [1640/4063], Loss: 0.8692\n",
      "Epoch [9/10], Step [1650/4063], Loss: 0.8470\n",
      "Epoch [9/10], Step [1660/4063], Loss: 1.0059\n",
      "Epoch [9/10], Step [1670/4063], Loss: 0.7321\n",
      "Epoch [9/10], Step [1680/4063], Loss: 1.0192\n",
      "Epoch [9/10], Step [1690/4063], Loss: 0.9076\n",
      "Epoch [9/10], Step [1700/4063], Loss: 0.8948\n",
      "Epoch [9/10], Step [1710/4063], Loss: 0.9590\n",
      "Epoch [9/10], Step [1720/4063], Loss: 0.7348\n",
      "Epoch [9/10], Step [1730/4063], Loss: 1.1293\n",
      "Epoch [9/10], Step [1740/4063], Loss: 0.8856\n",
      "Epoch [9/10], Step [1750/4063], Loss: 0.9273\n",
      "Epoch [9/10], Step [1760/4063], Loss: 0.7642\n",
      "Epoch [9/10], Step [1770/4063], Loss: 1.0062\n",
      "Epoch [9/10], Step [1780/4063], Loss: 0.5710\n",
      "Epoch [9/10], Step [1790/4063], Loss: 0.8548\n",
      "Epoch [9/10], Step [1800/4063], Loss: 1.0338\n",
      "Epoch [9/10], Step [1810/4063], Loss: 0.9509\n",
      "Epoch [9/10], Step [1820/4063], Loss: 0.3571\n",
      "Epoch [9/10], Step [1830/4063], Loss: 1.2271\n",
      "Epoch [9/10], Step [1840/4063], Loss: 0.8237\n",
      "Epoch [9/10], Step [1850/4063], Loss: 0.8178\n",
      "Epoch [9/10], Step [1860/4063], Loss: 1.1566\n",
      "Epoch [9/10], Step [1870/4063], Loss: 1.0003\n",
      "Epoch [9/10], Step [1880/4063], Loss: 0.7760\n",
      "Epoch [9/10], Step [1890/4063], Loss: 0.7926\n",
      "Epoch [9/10], Step [1900/4063], Loss: 0.4999\n",
      "Epoch [9/10], Step [1910/4063], Loss: 0.7206\n",
      "Epoch [9/10], Step [1920/4063], Loss: 0.9830\n",
      "Epoch [9/10], Step [1930/4063], Loss: 0.7990\n",
      "Epoch [9/10], Step [1940/4063], Loss: 1.1561\n",
      "Epoch [9/10], Step [1950/4063], Loss: 0.9432\n",
      "Epoch [9/10], Step [1960/4063], Loss: 0.6410\n",
      "Epoch [9/10], Step [1970/4063], Loss: 0.8710\n",
      "Epoch [9/10], Step [1980/4063], Loss: 1.1918\n",
      "Epoch [9/10], Step [1990/4063], Loss: 0.6719\n",
      "Epoch [9/10], Step [2000/4063], Loss: 0.8874\n",
      "Epoch [9/10], Step [2010/4063], Loss: 0.9620\n",
      "Epoch [9/10], Step [2020/4063], Loss: 1.3109\n",
      "Epoch [9/10], Step [2030/4063], Loss: 0.7515\n",
      "Epoch [9/10], Step [2040/4063], Loss: 1.0441\n",
      "Epoch [9/10], Step [2050/4063], Loss: 0.8701\n",
      "Epoch [9/10], Step [2060/4063], Loss: 0.6623\n",
      "Epoch [9/10], Step [2070/4063], Loss: 0.8231\n",
      "Epoch [9/10], Step [2080/4063], Loss: 1.3205\n",
      "Epoch [9/10], Step [2090/4063], Loss: 0.8460\n",
      "Epoch [9/10], Step [2100/4063], Loss: 0.8172\n",
      "Epoch [9/10], Step [2110/4063], Loss: 0.7472\n",
      "Epoch [9/10], Step [2120/4063], Loss: 0.9904\n",
      "Epoch [9/10], Step [2130/4063], Loss: 1.4547\n",
      "Epoch [9/10], Step [2140/4063], Loss: 0.8755\n",
      "Epoch [9/10], Step [2150/4063], Loss: 0.6159\n",
      "Epoch [9/10], Step [2160/4063], Loss: 0.9667\n",
      "Epoch [9/10], Step [2170/4063], Loss: 0.8517\n",
      "Epoch [9/10], Step [2180/4063], Loss: 0.5695\n",
      "Epoch [9/10], Step [2190/4063], Loss: 0.9914\n",
      "Epoch [9/10], Step [2200/4063], Loss: 1.2069\n",
      "Epoch [9/10], Step [2210/4063], Loss: 0.6462\n",
      "Epoch [9/10], Step [2220/4063], Loss: 1.1926\n",
      "Epoch [9/10], Step [2230/4063], Loss: 1.4351\n",
      "Epoch [9/10], Step [2240/4063], Loss: 0.8227\n",
      "Epoch [9/10], Step [2250/4063], Loss: 0.7273\n",
      "Epoch [9/10], Step [2260/4063], Loss: 0.9466\n",
      "Epoch [9/10], Step [2270/4063], Loss: 0.7882\n",
      "Epoch [9/10], Step [2280/4063], Loss: 0.7913\n",
      "Epoch [9/10], Step [2290/4063], Loss: 0.7792\n",
      "Epoch [9/10], Step [2300/4063], Loss: 1.0051\n",
      "Epoch [9/10], Step [2310/4063], Loss: 1.0877\n",
      "Epoch [9/10], Step [2320/4063], Loss: 0.8265\n",
      "Epoch [9/10], Step [2330/4063], Loss: 0.9393\n",
      "Epoch [9/10], Step [2340/4063], Loss: 0.9711\n",
      "Epoch [9/10], Step [2350/4063], Loss: 0.6596\n",
      "Epoch [9/10], Step [2360/4063], Loss: 0.7403\n",
      "Epoch [9/10], Step [2370/4063], Loss: 1.0587\n",
      "Epoch [9/10], Step [2380/4063], Loss: 1.1710\n",
      "Epoch [9/10], Step [2390/4063], Loss: 0.7277\n",
      "Epoch [9/10], Step [2400/4063], Loss: 1.4520\n",
      "Epoch [9/10], Step [2410/4063], Loss: 1.0761\n",
      "Epoch [9/10], Step [2420/4063], Loss: 0.6312\n",
      "Epoch [9/10], Step [2430/4063], Loss: 0.9878\n",
      "Epoch [9/10], Step [2440/4063], Loss: 1.3764\n",
      "Epoch [9/10], Step [2450/4063], Loss: 1.0628\n",
      "Epoch [9/10], Step [2460/4063], Loss: 0.5806\n",
      "Epoch [9/10], Step [2470/4063], Loss: 1.0304\n",
      "Epoch [9/10], Step [2480/4063], Loss: 1.1541\n",
      "Epoch [9/10], Step [2490/4063], Loss: 1.2969\n",
      "Epoch [9/10], Step [2500/4063], Loss: 0.8392\n",
      "Epoch [9/10], Step [2510/4063], Loss: 1.1507\n",
      "Epoch [9/10], Step [2520/4063], Loss: 1.0333\n",
      "Epoch [9/10], Step [2530/4063], Loss: 0.6147\n",
      "Epoch [9/10], Step [2540/4063], Loss: 1.0444\n",
      "Epoch [9/10], Step [2550/4063], Loss: 0.9088\n",
      "Epoch [9/10], Step [2560/4063], Loss: 1.1534\n",
      "Epoch [9/10], Step [2570/4063], Loss: 0.6024\n",
      "Epoch [9/10], Step [2580/4063], Loss: 1.0044\n",
      "Epoch [9/10], Step [2590/4063], Loss: 0.7797\n",
      "Epoch [9/10], Step [2600/4063], Loss: 1.1860\n",
      "Epoch [9/10], Step [2610/4063], Loss: 0.7961\n",
      "Epoch [9/10], Step [2620/4063], Loss: 0.6564\n",
      "Epoch [9/10], Step [2630/4063], Loss: 1.0885\n",
      "Epoch [9/10], Step [2640/4063], Loss: 0.6964\n",
      "Epoch [9/10], Step [2650/4063], Loss: 0.7698\n",
      "Epoch [9/10], Step [2660/4063], Loss: 0.9302\n",
      "Epoch [9/10], Step [2670/4063], Loss: 0.6453\n",
      "Epoch [9/10], Step [2680/4063], Loss: 1.1929\n",
      "Epoch [9/10], Step [2690/4063], Loss: 0.8160\n",
      "Epoch [9/10], Step [2700/4063], Loss: 0.8978\n",
      "Epoch [9/10], Step [2710/4063], Loss: 0.7106\n",
      "Epoch [9/10], Step [2720/4063], Loss: 1.2462\n",
      "Epoch [9/10], Step [2730/4063], Loss: 1.0799\n",
      "Epoch [9/10], Step [2740/4063], Loss: 0.7396\n",
      "Epoch [9/10], Step [2750/4063], Loss: 1.1790\n",
      "Epoch [9/10], Step [2760/4063], Loss: 1.0517\n",
      "Epoch [9/10], Step [2770/4063], Loss: 1.2127\n",
      "Epoch [9/10], Step [2780/4063], Loss: 0.7463\n",
      "Epoch [9/10], Step [2790/4063], Loss: 0.8161\n",
      "Epoch [9/10], Step [2800/4063], Loss: 1.1254\n",
      "Epoch [9/10], Step [2810/4063], Loss: 0.9977\n",
      "Epoch [9/10], Step [2820/4063], Loss: 1.1907\n",
      "Epoch [9/10], Step [2830/4063], Loss: 1.0401\n",
      "Epoch [9/10], Step [2840/4063], Loss: 0.8558\n",
      "Epoch [9/10], Step [2850/4063], Loss: 1.0791\n",
      "Epoch [9/10], Step [2860/4063], Loss: 0.8757\n",
      "Epoch [9/10], Step [2870/4063], Loss: 0.8276\n",
      "Epoch [9/10], Step [2880/4063], Loss: 0.7187\n",
      "Epoch [9/10], Step [2890/4063], Loss: 0.9263\n",
      "Epoch [9/10], Step [2900/4063], Loss: 0.6977\n",
      "Epoch [9/10], Step [2910/4063], Loss: 0.9491\n",
      "Epoch [9/10], Step [2920/4063], Loss: 0.9365\n",
      "Epoch [9/10], Step [2930/4063], Loss: 0.6065\n",
      "Epoch [9/10], Step [2940/4063], Loss: 0.8338\n",
      "Epoch [9/10], Step [2950/4063], Loss: 0.9031\n",
      "Epoch [9/10], Step [2960/4063], Loss: 1.3752\n",
      "Epoch [9/10], Step [2970/4063], Loss: 0.4420\n",
      "Epoch [9/10], Step [2980/4063], Loss: 0.8485\n",
      "Epoch [9/10], Step [2990/4063], Loss: 0.9430\n",
      "Epoch [9/10], Step [3000/4063], Loss: 0.6455\n",
      "Epoch [9/10], Step [3010/4063], Loss: 0.9956\n",
      "Epoch [9/10], Step [3020/4063], Loss: 1.0714\n",
      "Epoch [9/10], Step [3030/4063], Loss: 0.7565\n",
      "Epoch [9/10], Step [3040/4063], Loss: 0.8479\n",
      "Epoch [9/10], Step [3050/4063], Loss: 0.9473\n",
      "Epoch [9/10], Step [3060/4063], Loss: 0.9232\n",
      "Epoch [9/10], Step [3070/4063], Loss: 1.0763\n",
      "Epoch [9/10], Step [3080/4063], Loss: 1.2838\n",
      "Epoch [9/10], Step [3090/4063], Loss: 0.6030\n",
      "Epoch [9/10], Step [3100/4063], Loss: 0.7732\n",
      "Epoch [9/10], Step [3110/4063], Loss: 1.2254\n",
      "Epoch [9/10], Step [3120/4063], Loss: 0.8578\n",
      "Epoch [9/10], Step [3130/4063], Loss: 0.8301\n",
      "Epoch [9/10], Step [3140/4063], Loss: 1.0698\n",
      "Epoch [9/10], Step [3150/4063], Loss: 0.8911\n",
      "Epoch [9/10], Step [3160/4063], Loss: 0.6621\n",
      "Epoch [9/10], Step [3170/4063], Loss: 0.6554\n",
      "Epoch [9/10], Step [3180/4063], Loss: 0.7395\n",
      "Epoch [9/10], Step [3190/4063], Loss: 0.6581\n",
      "Epoch [9/10], Step [3200/4063], Loss: 0.5768\n",
      "Epoch [9/10], Step [3210/4063], Loss: 0.6642\n",
      "Epoch [9/10], Step [3220/4063], Loss: 0.7646\n",
      "Epoch [9/10], Step [3230/4063], Loss: 0.8979\n",
      "Epoch [9/10], Step [3240/4063], Loss: 0.7707\n",
      "Epoch [9/10], Step [3250/4063], Loss: 0.8635\n",
      "Epoch [9/10], Step [3260/4063], Loss: 0.8636\n",
      "Epoch [9/10], Step [3270/4063], Loss: 1.1844\n",
      "Epoch [9/10], Step [3280/4063], Loss: 1.4776\n",
      "Epoch [9/10], Step [3290/4063], Loss: 1.0977\n",
      "Epoch [9/10], Step [3300/4063], Loss: 0.9782\n",
      "Epoch [9/10], Step [3310/4063], Loss: 0.8639\n",
      "Epoch [9/10], Step [3320/4063], Loss: 1.0095\n",
      "Epoch [9/10], Step [3330/4063], Loss: 0.7120\n",
      "Epoch [9/10], Step [3340/4063], Loss: 0.7031\n",
      "Epoch [9/10], Step [3350/4063], Loss: 0.6013\n",
      "Epoch [9/10], Step [3360/4063], Loss: 0.8451\n",
      "Epoch [9/10], Step [3370/4063], Loss: 0.6217\n",
      "Epoch [9/10], Step [3380/4063], Loss: 1.1303\n",
      "Epoch [9/10], Step [3390/4063], Loss: 0.8894\n",
      "Epoch [9/10], Step [3400/4063], Loss: 1.0271\n",
      "Epoch [9/10], Step [3410/4063], Loss: 1.5169\n",
      "Epoch [9/10], Step [3420/4063], Loss: 0.8265\n",
      "Epoch [9/10], Step [3430/4063], Loss: 1.1937\n",
      "Epoch [9/10], Step [3440/4063], Loss: 1.1741\n",
      "Epoch [9/10], Step [3450/4063], Loss: 1.2279\n",
      "Epoch [9/10], Step [3460/4063], Loss: 0.7557\n",
      "Epoch [9/10], Step [3470/4063], Loss: 0.5430\n",
      "Epoch [9/10], Step [3480/4063], Loss: 0.9488\n",
      "Epoch [9/10], Step [3490/4063], Loss: 0.8246\n",
      "Epoch [9/10], Step [3500/4063], Loss: 0.8121\n",
      "Epoch [9/10], Step [3510/4063], Loss: 0.9910\n",
      "Epoch [9/10], Step [3520/4063], Loss: 0.8175\n",
      "Epoch [9/10], Step [3530/4063], Loss: 0.8683\n",
      "Epoch [9/10], Step [3540/4063], Loss: 1.0050\n",
      "Epoch [9/10], Step [3550/4063], Loss: 0.6136\n",
      "Epoch [9/10], Step [3560/4063], Loss: 0.8284\n",
      "Epoch [9/10], Step [3570/4063], Loss: 0.6121\n",
      "Epoch [9/10], Step [3580/4063], Loss: 0.6801\n",
      "Epoch [9/10], Step [3590/4063], Loss: 0.9434\n",
      "Epoch [9/10], Step [3600/4063], Loss: 1.1085\n",
      "Epoch [9/10], Step [3610/4063], Loss: 0.8956\n",
      "Epoch [9/10], Step [3620/4063], Loss: 0.9427\n",
      "Epoch [9/10], Step [3630/4063], Loss: 0.8192\n",
      "Epoch [9/10], Step [3640/4063], Loss: 0.8940\n",
      "Epoch [9/10], Step [3650/4063], Loss: 0.8813\n",
      "Epoch [9/10], Step [3660/4063], Loss: 0.6527\n",
      "Epoch [9/10], Step [3670/4063], Loss: 0.8397\n",
      "Epoch [9/10], Step [3680/4063], Loss: 1.2299\n",
      "Epoch [9/10], Step [3690/4063], Loss: 0.9261\n",
      "Epoch [9/10], Step [3700/4063], Loss: 1.0929\n",
      "Epoch [9/10], Step [3710/4063], Loss: 1.0612\n",
      "Epoch [9/10], Step [3720/4063], Loss: 1.2560\n",
      "Epoch [9/10], Step [3730/4063], Loss: 1.1563\n",
      "Epoch [9/10], Step [3740/4063], Loss: 0.9271\n",
      "Epoch [9/10], Step [3750/4063], Loss: 0.8156\n",
      "Epoch [9/10], Step [3760/4063], Loss: 1.3032\n",
      "Epoch [9/10], Step [3770/4063], Loss: 0.7173\n",
      "Epoch [9/10], Step [3780/4063], Loss: 1.1450\n",
      "Epoch [9/10], Step [3790/4063], Loss: 0.7455\n",
      "Epoch [9/10], Step [3800/4063], Loss: 0.6917\n",
      "Epoch [9/10], Step [3810/4063], Loss: 0.7815\n",
      "Epoch [9/10], Step [3820/4063], Loss: 0.6101\n",
      "Epoch [9/10], Step [3830/4063], Loss: 0.7417\n",
      "Epoch [9/10], Step [3840/4063], Loss: 0.9387\n",
      "Epoch [9/10], Step [3850/4063], Loss: 0.7004\n",
      "Epoch [9/10], Step [3860/4063], Loss: 0.8980\n",
      "Epoch [9/10], Step [3870/4063], Loss: 0.7716\n",
      "Epoch [9/10], Step [3880/4063], Loss: 0.8873\n",
      "Epoch [9/10], Step [3890/4063], Loss: 0.9805\n",
      "Epoch [9/10], Step [3900/4063], Loss: 0.5095\n",
      "Epoch [9/10], Step [3910/4063], Loss: 0.8992\n",
      "Epoch [9/10], Step [3920/4063], Loss: 0.6466\n",
      "Epoch [9/10], Step [3930/4063], Loss: 0.7795\n",
      "Epoch [9/10], Step [3940/4063], Loss: 0.6203\n",
      "Epoch [9/10], Step [3950/4063], Loss: 1.0257\n",
      "Epoch [9/10], Step [3960/4063], Loss: 0.9391\n",
      "Epoch [9/10], Step [3970/4063], Loss: 0.4756\n",
      "Epoch [9/10], Step [3980/4063], Loss: 1.4708\n",
      "Epoch [9/10], Step [3990/4063], Loss: 0.5800\n",
      "Epoch [9/10], Step [4000/4063], Loss: 0.9136\n",
      "Epoch [9/10], Step [4010/4063], Loss: 0.7552\n",
      "Epoch [9/10], Step [4020/4063], Loss: 0.6050\n",
      "Epoch [9/10], Step [4030/4063], Loss: 1.1159\n",
      "Epoch [9/10], Step [4040/4063], Loss: 0.7330\n",
      "Epoch [9/10], Step [4050/4063], Loss: 0.8306\n",
      "Epoch [9/10], Step [4060/4063], Loss: 0.7376\n",
      "Epoch [10/10], Step [10/4063], Loss: 1.0874\n",
      "Epoch [10/10], Step [20/4063], Loss: 0.8118\n",
      "Epoch [10/10], Step [30/4063], Loss: 0.7598\n",
      "Epoch [10/10], Step [40/4063], Loss: 1.0133\n",
      "Epoch [10/10], Step [50/4063], Loss: 0.6243\n",
      "Epoch [10/10], Step [60/4063], Loss: 1.0558\n",
      "Epoch [10/10], Step [70/4063], Loss: 0.8325\n",
      "Epoch [10/10], Step [80/4063], Loss: 0.5526\n",
      "Epoch [10/10], Step [90/4063], Loss: 0.3423\n",
      "Epoch [10/10], Step [100/4063], Loss: 1.1335\n",
      "Epoch [10/10], Step [110/4063], Loss: 0.8648\n",
      "Epoch [10/10], Step [120/4063], Loss: 0.6668\n",
      "Epoch [10/10], Step [130/4063], Loss: 1.0320\n",
      "Epoch [10/10], Step [140/4063], Loss: 0.6137\n",
      "Epoch [10/10], Step [150/4063], Loss: 0.5177\n",
      "Epoch [10/10], Step [160/4063], Loss: 0.6427\n",
      "Epoch [10/10], Step [170/4063], Loss: 1.1510\n",
      "Epoch [10/10], Step [180/4063], Loss: 1.1974\n",
      "Epoch [10/10], Step [190/4063], Loss: 1.1258\n",
      "Epoch [10/10], Step [200/4063], Loss: 1.1401\n",
      "Epoch [10/10], Step [210/4063], Loss: 0.6323\n",
      "Epoch [10/10], Step [220/4063], Loss: 1.1431\n",
      "Epoch [10/10], Step [230/4063], Loss: 0.8937\n",
      "Epoch [10/10], Step [240/4063], Loss: 0.7550\n",
      "Epoch [10/10], Step [250/4063], Loss: 0.8908\n",
      "Epoch [10/10], Step [260/4063], Loss: 1.1708\n",
      "Epoch [10/10], Step [270/4063], Loss: 0.8244\n",
      "Epoch [10/10], Step [280/4063], Loss: 1.1065\n",
      "Epoch [10/10], Step [290/4063], Loss: 0.6856\n",
      "Epoch [10/10], Step [300/4063], Loss: 1.0358\n",
      "Epoch [10/10], Step [310/4063], Loss: 0.7338\n",
      "Epoch [10/10], Step [320/4063], Loss: 0.6324\n",
      "Epoch [10/10], Step [330/4063], Loss: 0.9231\n",
      "Epoch [10/10], Step [340/4063], Loss: 0.8036\n",
      "Epoch [10/10], Step [350/4063], Loss: 1.0699\n",
      "Epoch [10/10], Step [360/4063], Loss: 0.6017\n",
      "Epoch [10/10], Step [370/4063], Loss: 0.9819\n",
      "Epoch [10/10], Step [380/4063], Loss: 0.5314\n",
      "Epoch [10/10], Step [390/4063], Loss: 0.8965\n",
      "Epoch [10/10], Step [400/4063], Loss: 1.1394\n",
      "Epoch [10/10], Step [410/4063], Loss: 0.9679\n",
      "Epoch [10/10], Step [420/4063], Loss: 0.7126\n",
      "Epoch [10/10], Step [430/4063], Loss: 0.9151\n",
      "Epoch [10/10], Step [440/4063], Loss: 0.9096\n",
      "Epoch [10/10], Step [450/4063], Loss: 0.8698\n",
      "Epoch [10/10], Step [460/4063], Loss: 0.9384\n",
      "Epoch [10/10], Step [470/4063], Loss: 0.5467\n",
      "Epoch [10/10], Step [480/4063], Loss: 1.1467\n",
      "Epoch [10/10], Step [490/4063], Loss: 1.3292\n",
      "Epoch [10/10], Step [500/4063], Loss: 1.5419\n",
      "Epoch [10/10], Step [510/4063], Loss: 0.8823\n",
      "Epoch [10/10], Step [520/4063], Loss: 1.1890\n",
      "Epoch [10/10], Step [530/4063], Loss: 1.0484\n",
      "Epoch [10/10], Step [540/4063], Loss: 0.5696\n",
      "Epoch [10/10], Step [550/4063], Loss: 1.2304\n",
      "Epoch [10/10], Step [560/4063], Loss: 1.2252\n",
      "Epoch [10/10], Step [570/4063], Loss: 0.9384\n",
      "Epoch [10/10], Step [580/4063], Loss: 1.2017\n",
      "Epoch [10/10], Step [590/4063], Loss: 0.7273\n",
      "Epoch [10/10], Step [600/4063], Loss: 0.8424\n",
      "Epoch [10/10], Step [610/4063], Loss: 1.3489\n",
      "Epoch [10/10], Step [620/4063], Loss: 0.8289\n",
      "Epoch [10/10], Step [630/4063], Loss: 0.9686\n",
      "Epoch [10/10], Step [640/4063], Loss: 0.7580\n",
      "Epoch [10/10], Step [650/4063], Loss: 0.8077\n",
      "Epoch [10/10], Step [660/4063], Loss: 0.7431\n",
      "Epoch [10/10], Step [670/4063], Loss: 1.0142\n",
      "Epoch [10/10], Step [680/4063], Loss: 0.8723\n",
      "Epoch [10/10], Step [690/4063], Loss: 0.6676\n",
      "Epoch [10/10], Step [700/4063], Loss: 0.7407\n",
      "Epoch [10/10], Step [710/4063], Loss: 0.7627\n",
      "Epoch [10/10], Step [720/4063], Loss: 0.5015\n",
      "Epoch [10/10], Step [730/4063], Loss: 0.7738\n",
      "Epoch [10/10], Step [740/4063], Loss: 0.8387\n",
      "Epoch [10/10], Step [750/4063], Loss: 0.7611\n",
      "Epoch [10/10], Step [760/4063], Loss: 0.8903\n",
      "Epoch [10/10], Step [770/4063], Loss: 0.8043\n",
      "Epoch [10/10], Step [780/4063], Loss: 1.0255\n",
      "Epoch [10/10], Step [790/4063], Loss: 0.8264\n",
      "Epoch [10/10], Step [800/4063], Loss: 0.8140\n",
      "Epoch [10/10], Step [810/4063], Loss: 0.7308\n",
      "Epoch [10/10], Step [820/4063], Loss: 1.2576\n",
      "Epoch [10/10], Step [830/4063], Loss: 1.0455\n",
      "Epoch [10/10], Step [840/4063], Loss: 0.8835\n",
      "Epoch [10/10], Step [850/4063], Loss: 0.8809\n",
      "Epoch [10/10], Step [860/4063], Loss: 0.6886\n",
      "Epoch [10/10], Step [870/4063], Loss: 0.8655\n",
      "Epoch [10/10], Step [880/4063], Loss: 0.7460\n",
      "Epoch [10/10], Step [890/4063], Loss: 1.1653\n",
      "Epoch [10/10], Step [900/4063], Loss: 1.1168\n",
      "Epoch [10/10], Step [910/4063], Loss: 0.5376\n",
      "Epoch [10/10], Step [920/4063], Loss: 0.8780\n",
      "Epoch [10/10], Step [930/4063], Loss: 0.4988\n",
      "Epoch [10/10], Step [940/4063], Loss: 0.8798\n",
      "Epoch [10/10], Step [950/4063], Loss: 0.8357\n",
      "Epoch [10/10], Step [960/4063], Loss: 1.0105\n",
      "Epoch [10/10], Step [970/4063], Loss: 0.8515\n",
      "Epoch [10/10], Step [980/4063], Loss: 0.8879\n",
      "Epoch [10/10], Step [990/4063], Loss: 0.7415\n",
      "Epoch [10/10], Step [1000/4063], Loss: 0.9313\n",
      "Epoch [10/10], Step [1010/4063], Loss: 0.7762\n",
      "Epoch [10/10], Step [1020/4063], Loss: 0.5935\n",
      "Epoch [10/10], Step [1030/4063], Loss: 0.9240\n",
      "Epoch [10/10], Step [1040/4063], Loss: 0.6773\n",
      "Epoch [10/10], Step [1050/4063], Loss: 1.4574\n",
      "Epoch [10/10], Step [1060/4063], Loss: 1.3630\n",
      "Epoch [10/10], Step [1070/4063], Loss: 0.9937\n",
      "Epoch [10/10], Step [1080/4063], Loss: 1.1521\n",
      "Epoch [10/10], Step [1090/4063], Loss: 1.3375\n",
      "Epoch [10/10], Step [1100/4063], Loss: 0.8194\n",
      "Epoch [10/10], Step [1110/4063], Loss: 0.8395\n",
      "Epoch [10/10], Step [1120/4063], Loss: 1.3745\n",
      "Epoch [10/10], Step [1130/4063], Loss: 1.0736\n",
      "Epoch [10/10], Step [1140/4063], Loss: 0.9036\n",
      "Epoch [10/10], Step [1150/4063], Loss: 0.9614\n",
      "Epoch [10/10], Step [1160/4063], Loss: 0.9615\n",
      "Epoch [10/10], Step [1170/4063], Loss: 0.8257\n",
      "Epoch [10/10], Step [1180/4063], Loss: 0.8394\n",
      "Epoch [10/10], Step [1190/4063], Loss: 0.8534\n",
      "Epoch [10/10], Step [1200/4063], Loss: 1.0998\n",
      "Epoch [10/10], Step [1210/4063], Loss: 0.9378\n",
      "Epoch [10/10], Step [1220/4063], Loss: 0.7936\n",
      "Epoch [10/10], Step [1230/4063], Loss: 1.5440\n",
      "Epoch [10/10], Step [1240/4063], Loss: 0.8407\n",
      "Epoch [10/10], Step [1250/4063], Loss: 0.7977\n",
      "Epoch [10/10], Step [1260/4063], Loss: 0.9719\n",
      "Epoch [10/10], Step [1270/4063], Loss: 0.7938\n",
      "Epoch [10/10], Step [1280/4063], Loss: 0.6635\n",
      "Epoch [10/10], Step [1290/4063], Loss: 1.1274\n",
      "Epoch [10/10], Step [1300/4063], Loss: 1.1470\n",
      "Epoch [10/10], Step [1310/4063], Loss: 0.9270\n",
      "Epoch [10/10], Step [1320/4063], Loss: 0.9407\n",
      "Epoch [10/10], Step [1330/4063], Loss: 0.9032\n",
      "Epoch [10/10], Step [1340/4063], Loss: 0.7378\n",
      "Epoch [10/10], Step [1350/4063], Loss: 1.0925\n",
      "Epoch [10/10], Step [1360/4063], Loss: 0.8015\n",
      "Epoch [10/10], Step [1370/4063], Loss: 0.9847\n",
      "Epoch [10/10], Step [1380/4063], Loss: 1.0589\n",
      "Epoch [10/10], Step [1390/4063], Loss: 0.9179\n",
      "Epoch [10/10], Step [1400/4063], Loss: 0.6794\n",
      "Epoch [10/10], Step [1410/4063], Loss: 0.7717\n",
      "Epoch [10/10], Step [1420/4063], Loss: 0.6339\n",
      "Epoch [10/10], Step [1430/4063], Loss: 0.7546\n",
      "Epoch [10/10], Step [1440/4063], Loss: 1.3538\n",
      "Epoch [10/10], Step [1450/4063], Loss: 1.0492\n",
      "Epoch [10/10], Step [1460/4063], Loss: 0.6184\n",
      "Epoch [10/10], Step [1470/4063], Loss: 1.0815\n",
      "Epoch [10/10], Step [1480/4063], Loss: 1.1661\n",
      "Epoch [10/10], Step [1490/4063], Loss: 0.9670\n",
      "Epoch [10/10], Step [1500/4063], Loss: 1.0986\n",
      "Epoch [10/10], Step [1510/4063], Loss: 0.8697\n",
      "Epoch [10/10], Step [1520/4063], Loss: 0.7643\n",
      "Epoch [10/10], Step [1530/4063], Loss: 1.2439\n",
      "Epoch [10/10], Step [1540/4063], Loss: 1.3252\n",
      "Epoch [10/10], Step [1550/4063], Loss: 0.8278\n",
      "Epoch [10/10], Step [1560/4063], Loss: 1.3036\n",
      "Epoch [10/10], Step [1570/4063], Loss: 0.5813\n",
      "Epoch [10/10], Step [1580/4063], Loss: 1.1982\n",
      "Epoch [10/10], Step [1590/4063], Loss: 0.6824\n",
      "Epoch [10/10], Step [1600/4063], Loss: 0.9371\n",
      "Epoch [10/10], Step [1610/4063], Loss: 0.8085\n",
      "Epoch [10/10], Step [1620/4063], Loss: 0.9295\n",
      "Epoch [10/10], Step [1630/4063], Loss: 1.3129\n",
      "Epoch [10/10], Step [1640/4063], Loss: 0.8226\n",
      "Epoch [10/10], Step [1650/4063], Loss: 0.8123\n",
      "Epoch [10/10], Step [1660/4063], Loss: 1.3445\n",
      "Epoch [10/10], Step [1670/4063], Loss: 0.6450\n",
      "Epoch [10/10], Step [1680/4063], Loss: 0.7436\n",
      "Epoch [10/10], Step [1690/4063], Loss: 0.7340\n",
      "Epoch [10/10], Step [1700/4063], Loss: 1.2437\n",
      "Epoch [10/10], Step [1710/4063], Loss: 1.0667\n",
      "Epoch [10/10], Step [1720/4063], Loss: 0.9316\n",
      "Epoch [10/10], Step [1730/4063], Loss: 0.9838\n",
      "Epoch [10/10], Step [1740/4063], Loss: 0.5961\n",
      "Epoch [10/10], Step [1750/4063], Loss: 0.6545\n",
      "Epoch [10/10], Step [1760/4063], Loss: 1.2167\n",
      "Epoch [10/10], Step [1770/4063], Loss: 0.9867\n",
      "Epoch [10/10], Step [1780/4063], Loss: 1.3923\n",
      "Epoch [10/10], Step [1790/4063], Loss: 1.4171\n",
      "Epoch [10/10], Step [1800/4063], Loss: 1.2880\n",
      "Epoch [10/10], Step [1810/4063], Loss: 0.7197\n",
      "Epoch [10/10], Step [1820/4063], Loss: 0.8252\n",
      "Epoch [10/10], Step [1830/4063], Loss: 0.4754\n",
      "Epoch [10/10], Step [1840/4063], Loss: 1.0948\n",
      "Epoch [10/10], Step [1850/4063], Loss: 0.7446\n",
      "Epoch [10/10], Step [1860/4063], Loss: 0.5871\n",
      "Epoch [10/10], Step [1870/4063], Loss: 0.7827\n",
      "Epoch [10/10], Step [1880/4063], Loss: 0.7882\n",
      "Epoch [10/10], Step [1890/4063], Loss: 1.1230\n",
      "Epoch [10/10], Step [1900/4063], Loss: 0.9143\n",
      "Epoch [10/10], Step [1910/4063], Loss: 0.7410\n",
      "Epoch [10/10], Step [1920/4063], Loss: 1.1353\n",
      "Epoch [10/10], Step [1930/4063], Loss: 0.8230\n",
      "Epoch [10/10], Step [1940/4063], Loss: 0.9473\n",
      "Epoch [10/10], Step [1950/4063], Loss: 0.6396\n",
      "Epoch [10/10], Step [1960/4063], Loss: 1.1539\n",
      "Epoch [10/10], Step [1970/4063], Loss: 1.0077\n",
      "Epoch [10/10], Step [1980/4063], Loss: 0.7262\n",
      "Epoch [10/10], Step [1990/4063], Loss: 0.9465\n",
      "Epoch [10/10], Step [2000/4063], Loss: 0.9224\n",
      "Epoch [10/10], Step [2010/4063], Loss: 0.8240\n",
      "Epoch [10/10], Step [2020/4063], Loss: 0.7681\n",
      "Epoch [10/10], Step [2030/4063], Loss: 0.7760\n",
      "Epoch [10/10], Step [2040/4063], Loss: 0.8612\n",
      "Epoch [10/10], Step [2050/4063], Loss: 1.1941\n",
      "Epoch [10/10], Step [2060/4063], Loss: 0.4298\n",
      "Epoch [10/10], Step [2070/4063], Loss: 0.7118\n",
      "Epoch [10/10], Step [2080/4063], Loss: 0.9703\n",
      "Epoch [10/10], Step [2090/4063], Loss: 0.8018\n",
      "Epoch [10/10], Step [2100/4063], Loss: 0.9106\n",
      "Epoch [10/10], Step [2110/4063], Loss: 0.8028\n",
      "Epoch [10/10], Step [2120/4063], Loss: 0.9101\n",
      "Epoch [10/10], Step [2130/4063], Loss: 0.9409\n",
      "Epoch [10/10], Step [2140/4063], Loss: 0.9774\n",
      "Epoch [10/10], Step [2150/4063], Loss: 1.2833\n",
      "Epoch [10/10], Step [2160/4063], Loss: 0.9497\n",
      "Epoch [10/10], Step [2170/4063], Loss: 0.8281\n",
      "Epoch [10/10], Step [2180/4063], Loss: 1.0561\n",
      "Epoch [10/10], Step [2190/4063], Loss: 0.8185\n",
      "Epoch [10/10], Step [2200/4063], Loss: 1.0794\n",
      "Epoch [10/10], Step [2210/4063], Loss: 1.0918\n",
      "Epoch [10/10], Step [2220/4063], Loss: 0.7134\n",
      "Epoch [10/10], Step [2230/4063], Loss: 1.0170\n",
      "Epoch [10/10], Step [2240/4063], Loss: 0.6760\n",
      "Epoch [10/10], Step [2250/4063], Loss: 0.5224\n",
      "Epoch [10/10], Step [2260/4063], Loss: 0.8960\n",
      "Epoch [10/10], Step [2270/4063], Loss: 1.2181\n",
      "Epoch [10/10], Step [2280/4063], Loss: 0.9597\n",
      "Epoch [10/10], Step [2290/4063], Loss: 0.7546\n",
      "Epoch [10/10], Step [2300/4063], Loss: 0.8087\n",
      "Epoch [10/10], Step [2310/4063], Loss: 0.7120\n",
      "Epoch [10/10], Step [2320/4063], Loss: 0.5550\n",
      "Epoch [10/10], Step [2330/4063], Loss: 0.7612\n",
      "Epoch [10/10], Step [2340/4063], Loss: 1.1440\n",
      "Epoch [10/10], Step [2350/4063], Loss: 1.1224\n",
      "Epoch [10/10], Step [2360/4063], Loss: 0.5848\n",
      "Epoch [10/10], Step [2370/4063], Loss: 0.9348\n",
      "Epoch [10/10], Step [2380/4063], Loss: 1.1270\n",
      "Epoch [10/10], Step [2390/4063], Loss: 0.8562\n",
      "Epoch [10/10], Step [2400/4063], Loss: 1.3672\n",
      "Epoch [10/10], Step [2410/4063], Loss: 0.8409\n",
      "Epoch [10/10], Step [2420/4063], Loss: 0.8443\n",
      "Epoch [10/10], Step [2430/4063], Loss: 0.7630\n",
      "Epoch [10/10], Step [2440/4063], Loss: 0.4203\n",
      "Epoch [10/10], Step [2450/4063], Loss: 1.1605\n",
      "Epoch [10/10], Step [2460/4063], Loss: 0.6461\n",
      "Epoch [10/10], Step [2470/4063], Loss: 0.8300\n",
      "Epoch [10/10], Step [2480/4063], Loss: 0.9130\n",
      "Epoch [10/10], Step [2490/4063], Loss: 0.7772\n",
      "Epoch [10/10], Step [2500/4063], Loss: 1.0980\n",
      "Epoch [10/10], Step [2510/4063], Loss: 0.8268\n",
      "Epoch [10/10], Step [2520/4063], Loss: 0.8793\n",
      "Epoch [10/10], Step [2530/4063], Loss: 0.7167\n",
      "Epoch [10/10], Step [2540/4063], Loss: 0.7654\n",
      "Epoch [10/10], Step [2550/4063], Loss: 0.9129\n",
      "Epoch [10/10], Step [2560/4063], Loss: 1.0391\n",
      "Epoch [10/10], Step [2570/4063], Loss: 0.8294\n",
      "Epoch [10/10], Step [2580/4063], Loss: 0.8660\n",
      "Epoch [10/10], Step [2590/4063], Loss: 1.1854\n",
      "Epoch [10/10], Step [2600/4063], Loss: 1.1863\n",
      "Epoch [10/10], Step [2610/4063], Loss: 0.9707\n",
      "Epoch [10/10], Step [2620/4063], Loss: 0.6855\n",
      "Epoch [10/10], Step [2630/4063], Loss: 1.0039\n",
      "Epoch [10/10], Step [2640/4063], Loss: 0.6407\n",
      "Epoch [10/10], Step [2650/4063], Loss: 0.6594\n",
      "Epoch [10/10], Step [2660/4063], Loss: 0.9378\n",
      "Epoch [10/10], Step [2670/4063], Loss: 0.8023\n",
      "Epoch [10/10], Step [2680/4063], Loss: 0.9693\n",
      "Epoch [10/10], Step [2690/4063], Loss: 0.7992\n",
      "Epoch [10/10], Step [2700/4063], Loss: 0.9070\n",
      "Epoch [10/10], Step [2710/4063], Loss: 0.5961\n",
      "Epoch [10/10], Step [2720/4063], Loss: 1.0375\n",
      "Epoch [10/10], Step [2730/4063], Loss: 0.7920\n",
      "Epoch [10/10], Step [2740/4063], Loss: 0.9287\n",
      "Epoch [10/10], Step [2750/4063], Loss: 0.4976\n",
      "Epoch [10/10], Step [2760/4063], Loss: 0.7067\n",
      "Epoch [10/10], Step [2770/4063], Loss: 0.9482\n",
      "Epoch [10/10], Step [2780/4063], Loss: 0.4030\n",
      "Epoch [10/10], Step [2790/4063], Loss: 1.0230\n",
      "Epoch [10/10], Step [2800/4063], Loss: 0.7772\n",
      "Epoch [10/10], Step [2810/4063], Loss: 0.9842\n",
      "Epoch [10/10], Step [2820/4063], Loss: 1.0644\n",
      "Epoch [10/10], Step [2830/4063], Loss: 0.4482\n",
      "Epoch [10/10], Step [2840/4063], Loss: 0.9536\n",
      "Epoch [10/10], Step [2850/4063], Loss: 1.3417\n",
      "Epoch [10/10], Step [2860/4063], Loss: 0.9625\n",
      "Epoch [10/10], Step [2870/4063], Loss: 0.9751\n",
      "Epoch [10/10], Step [2880/4063], Loss: 1.0937\n",
      "Epoch [10/10], Step [2890/4063], Loss: 0.9183\n",
      "Epoch [10/10], Step [2900/4063], Loss: 0.8392\n",
      "Epoch [10/10], Step [2910/4063], Loss: 0.9841\n",
      "Epoch [10/10], Step [2920/4063], Loss: 1.2441\n",
      "Epoch [10/10], Step [2930/4063], Loss: 0.8670\n",
      "Epoch [10/10], Step [2940/4063], Loss: 1.2475\n",
      "Epoch [10/10], Step [2950/4063], Loss: 0.7678\n",
      "Epoch [10/10], Step [2960/4063], Loss: 0.6743\n",
      "Epoch [10/10], Step [2970/4063], Loss: 0.9836\n",
      "Epoch [10/10], Step [2980/4063], Loss: 0.9639\n",
      "Epoch [10/10], Step [2990/4063], Loss: 0.7658\n",
      "Epoch [10/10], Step [3000/4063], Loss: 1.1434\n",
      "Epoch [10/10], Step [3010/4063], Loss: 1.0370\n",
      "Epoch [10/10], Step [3020/4063], Loss: 1.0027\n",
      "Epoch [10/10], Step [3030/4063], Loss: 0.9973\n",
      "Epoch [10/10], Step [3040/4063], Loss: 1.2335\n",
      "Epoch [10/10], Step [3050/4063], Loss: 1.1404\n",
      "Epoch [10/10], Step [3060/4063], Loss: 1.1276\n",
      "Epoch [10/10], Step [3070/4063], Loss: 0.8012\n",
      "Epoch [10/10], Step [3080/4063], Loss: 0.5606\n",
      "Epoch [10/10], Step [3090/4063], Loss: 1.0597\n",
      "Epoch [10/10], Step [3100/4063], Loss: 0.8071\n",
      "Epoch [10/10], Step [3110/4063], Loss: 1.0273\n",
      "Epoch [10/10], Step [3120/4063], Loss: 0.6617\n",
      "Epoch [10/10], Step [3130/4063], Loss: 0.8959\n",
      "Epoch [10/10], Step [3140/4063], Loss: 0.9893\n",
      "Epoch [10/10], Step [3150/4063], Loss: 0.7345\n",
      "Epoch [10/10], Step [3160/4063], Loss: 0.8992\n",
      "Epoch [10/10], Step [3170/4063], Loss: 1.0066\n",
      "Epoch [10/10], Step [3180/4063], Loss: 0.8889\n",
      "Epoch [10/10], Step [3190/4063], Loss: 0.9710\n",
      "Epoch [10/10], Step [3200/4063], Loss: 0.8095\n",
      "Epoch [10/10], Step [3210/4063], Loss: 1.0721\n",
      "Epoch [10/10], Step [3220/4063], Loss: 0.8907\n",
      "Epoch [10/10], Step [3230/4063], Loss: 0.6568\n",
      "Epoch [10/10], Step [3240/4063], Loss: 0.7336\n",
      "Epoch [10/10], Step [3250/4063], Loss: 1.1129\n",
      "Epoch [10/10], Step [3260/4063], Loss: 0.5863\n",
      "Epoch [10/10], Step [3270/4063], Loss: 0.6855\n",
      "Epoch [10/10], Step [3280/4063], Loss: 1.0883\n",
      "Epoch [10/10], Step [3290/4063], Loss: 1.0822\n",
      "Epoch [10/10], Step [3300/4063], Loss: 0.7220\n",
      "Epoch [10/10], Step [3310/4063], Loss: 0.9629\n",
      "Epoch [10/10], Step [3320/4063], Loss: 0.7045\n",
      "Epoch [10/10], Step [3330/4063], Loss: 0.7376\n",
      "Epoch [10/10], Step [3340/4063], Loss: 0.8465\n",
      "Epoch [10/10], Step [3350/4063], Loss: 0.7499\n",
      "Epoch [10/10], Step [3360/4063], Loss: 1.1736\n",
      "Epoch [10/10], Step [3370/4063], Loss: 1.0706\n",
      "Epoch [10/10], Step [3380/4063], Loss: 0.9370\n",
      "Epoch [10/10], Step [3390/4063], Loss: 0.9095\n",
      "Epoch [10/10], Step [3400/4063], Loss: 0.7264\n",
      "Epoch [10/10], Step [3410/4063], Loss: 0.5301\n",
      "Epoch [10/10], Step [3420/4063], Loss: 0.8135\n",
      "Epoch [10/10], Step [3430/4063], Loss: 0.8321\n",
      "Epoch [10/10], Step [3440/4063], Loss: 0.8897\n",
      "Epoch [10/10], Step [3450/4063], Loss: 1.3018\n",
      "Epoch [10/10], Step [3460/4063], Loss: 0.9894\n",
      "Epoch [10/10], Step [3470/4063], Loss: 1.2280\n",
      "Epoch [10/10], Step [3480/4063], Loss: 0.9433\n",
      "Epoch [10/10], Step [3490/4063], Loss: 0.9572\n",
      "Epoch [10/10], Step [3500/4063], Loss: 0.8719\n",
      "Epoch [10/10], Step [3510/4063], Loss: 0.8172\n",
      "Epoch [10/10], Step [3520/4063], Loss: 0.8691\n",
      "Epoch [10/10], Step [3530/4063], Loss: 1.3311\n",
      "Epoch [10/10], Step [3540/4063], Loss: 0.9002\n",
      "Epoch [10/10], Step [3550/4063], Loss: 0.8010\n",
      "Epoch [10/10], Step [3560/4063], Loss: 1.0986\n",
      "Epoch [10/10], Step [3570/4063], Loss: 0.7651\n",
      "Epoch [10/10], Step [3580/4063], Loss: 0.9103\n",
      "Epoch [10/10], Step [3590/4063], Loss: 1.2518\n",
      "Epoch [10/10], Step [3600/4063], Loss: 0.9167\n",
      "Epoch [10/10], Step [3610/4063], Loss: 0.8309\n",
      "Epoch [10/10], Step [3620/4063], Loss: 0.9530\n",
      "Epoch [10/10], Step [3630/4063], Loss: 0.7329\n",
      "Epoch [10/10], Step [3640/4063], Loss: 0.7601\n",
      "Epoch [10/10], Step [3650/4063], Loss: 0.8390\n",
      "Epoch [10/10], Step [3660/4063], Loss: 0.8544\n",
      "Epoch [10/10], Step [3670/4063], Loss: 1.0931\n",
      "Epoch [10/10], Step [3680/4063], Loss: 0.8039\n",
      "Epoch [10/10], Step [3690/4063], Loss: 0.8179\n",
      "Epoch [10/10], Step [3700/4063], Loss: 1.3681\n",
      "Epoch [10/10], Step [3710/4063], Loss: 0.3147\n",
      "Epoch [10/10], Step [3720/4063], Loss: 1.0041\n",
      "Epoch [10/10], Step [3730/4063], Loss: 0.9741\n",
      "Epoch [10/10], Step [3740/4063], Loss: 0.9126\n",
      "Epoch [10/10], Step [3750/4063], Loss: 0.7511\n",
      "Epoch [10/10], Step [3760/4063], Loss: 0.9244\n",
      "Epoch [10/10], Step [3770/4063], Loss: 0.4104\n",
      "Epoch [10/10], Step [3780/4063], Loss: 0.9844\n",
      "Epoch [10/10], Step [3790/4063], Loss: 0.9829\n",
      "Epoch [10/10], Step [3800/4063], Loss: 0.7798\n",
      "Epoch [10/10], Step [3810/4063], Loss: 1.0560\n",
      "Epoch [10/10], Step [3820/4063], Loss: 0.8027\n",
      "Epoch [10/10], Step [3830/4063], Loss: 0.9907\n",
      "Epoch [10/10], Step [3840/4063], Loss: 0.7825\n",
      "Epoch [10/10], Step [3850/4063], Loss: 0.7127\n",
      "Epoch [10/10], Step [3860/4063], Loss: 0.8335\n",
      "Epoch [10/10], Step [3870/4063], Loss: 1.0680\n",
      "Epoch [10/10], Step [3880/4063], Loss: 0.8860\n",
      "Epoch [10/10], Step [3890/4063], Loss: 0.4869\n",
      "Epoch [10/10], Step [3900/4063], Loss: 0.6336\n",
      "Epoch [10/10], Step [3910/4063], Loss: 0.9427\n",
      "Epoch [10/10], Step [3920/4063], Loss: 0.7899\n",
      "Epoch [10/10], Step [3930/4063], Loss: 1.0590\n",
      "Epoch [10/10], Step [3940/4063], Loss: 0.8789\n",
      "Epoch [10/10], Step [3950/4063], Loss: 0.7239\n",
      "Epoch [10/10], Step [3960/4063], Loss: 0.8918\n",
      "Epoch [10/10], Step [3970/4063], Loss: 0.6447\n",
      "Epoch [10/10], Step [3980/4063], Loss: 0.7914\n",
      "Epoch [10/10], Step [3990/4063], Loss: 0.6216\n",
      "Epoch [10/10], Step [4000/4063], Loss: 1.0018\n",
      "Epoch [10/10], Step [4010/4063], Loss: 0.7930\n",
      "Epoch [10/10], Step [4020/4063], Loss: 1.0688\n",
      "Epoch [10/10], Step [4030/4063], Loss: 0.6412\n",
      "Epoch [10/10], Step [4040/4063], Loss: 0.9795\n",
      "Epoch [10/10], Step [4050/4063], Loss: 0.8417\n",
      "Epoch [10/10], Step [4060/4063], Loss: 0.5691\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "第一次的学习：学习率\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ],
   "id": "cc996070f5b6e286"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T15:38:57.245725Z",
     "start_time": "2024-08-29T15:38:57.031923Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs.unsqueeze(1))\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# 生成混淆矩阵\n",
    "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "print(conf_matrix)\n"
   ],
   "id": "eb543de9c2ebbcc3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2836  315    3    2    0    0]\n",
      " [ 328 3128   22    7    0    0]\n",
      " [   0    1 3059   28   23  126]\n",
      " [   0    0    5 2705    2  169]\n",
      " [   1   13  673   22  126  668]\n",
      " [   0    2  640    1  100 1247]]\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "第二次的学习：学习率\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ],
   "id": "84f9773b5ece20a6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T15:47:37.156020Z",
     "start_time": "2024-08-29T15:47:36.980381Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs.unsqueeze(1))\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# 生成混淆矩阵\n",
    "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "print(conf_matrix)"
   ],
   "id": "16325789d8dbd344",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2838    0    0  294    0   24]\n",
      " [ 328 2171    0  923    0   63]\n",
      " [  23    0    0 3116    0   98]\n",
      " [   2    0    0 2709    0  170]\n",
      " [ 115    2   24  731    0  631]\n",
      " [ 104    0    0  729    0 1157]]\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "第三次的学习：学习率\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005) weight_decay=1e-5"
   ],
   "id": "a10867d14481ce8a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T15:49:22.236341Z",
     "start_time": "2024-08-29T15:49:22.052768Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs.unsqueeze(1))\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# 生成混淆矩阵\n",
    "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "print(conf_matrix)"
   ],
   "id": "87ae9a8420a483f3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2852  301    3    0    0    0]\n",
      " [ 339 3096   22    0    0   28]\n",
      " [   1    0 3071   47    0  118]\n",
      " [   0    0   68 2683    3  127]\n",
      " [   7   13  730   19    1  733]\n",
      " [   1    4  729   33    0 1223]]\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "增加惩罚系数在lr最好的",
   "id": "6fec54b2e987c601"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T15:56:45.378504Z",
     "start_time": "2024-08-29T15:56:45.192268Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs.unsqueeze(1))\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# 生成混淆矩阵\n",
    "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "print(conf_matrix)\n"
   ],
   "id": "6aed686d0918458b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2857  239    2   58    0    0]\n",
      " [ 341 2685    0  459    0    0]\n",
      " [   1    3 3081    9    0  143]\n",
      " [   0   87  644 2127    1   22]\n",
      " [   4   68  705   16    2  708]\n",
      " [   2  206  641    3    0 1138]]\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "29a2d7623d4c86d5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "增加惩罚项之后效果很差",
   "id": "3d2171d740bec463"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T15:58:55.068245Z",
     "start_time": "2024-08-29T15:58:54.875506Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs.unsqueeze(1))\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# 生成混淆矩阵\n",
    "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "print(conf_matrix)\n"
   ],
   "id": "caac2d09ecff4f44",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2    0 3154    0    0    0]\n",
      " [   0 2192 1249    0    0   44]\n",
      " [   0    5 3226    0    0    6]\n",
      " [   0   47 2788    0    0   46]\n",
      " [   4  230 1209    0    0   60]\n",
      " [   0  480 1355    0    0  155]]\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "7b1d1222d97cceef"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "感觉仅仅学习率0.001的时候稍微好一点点",
   "id": "66a51aff049fdba0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T15:59:57.978948Z",
     "start_time": "2024-08-29T15:59:57.799944Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs.unsqueeze(1))\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# 生成混淆矩阵\n",
    "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "print(conf_matrix)\n"
   ],
   "id": "14a8bdf3f6645d5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2850   62  241    0    1    2]\n",
      " [ 339 2668  471    7    0    0]\n",
      " [   0    1 3116    0   23   97]\n",
      " [   0    0 2709    0    5  167]\n",
      " [   0   17  743    2  107  634]\n",
      " [   0    7  733    0  100 1150]]\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import serial\n",
    "import numpy as np\n",
    "\n",
    "#   ESP32-C3 使用的波特率为 115200\n",
    "ser = serial.Serial('/dev/ttyACM0', 115200, timeout=1)\n",
    "\n",
    "def read_imu_data():\n",
    "    data = []\n",
    "    while len(data) < 200:  # 读取200个数据点\n",
    "        line = ser.readline().decode('utf-8').strip()\n",
    "        if line:\n",
    "            try:\n",
    "                values = list(map(float, line.split(',')))  # 假设IMU数据以逗号分隔\n",
    "                if len(values) == 6:  # 确保数据格式正确（3个IMU + 3个陀螺仪数据）\n",
    "                    data.append(values)\n",
    "            except ValueError:\n",
    "                pass  # 跳过不正确的行\n",
    "    return np.array(data)\n",
    "\n",
    "# 读取IMU数据\n",
    "imu_data = read_imu_data()\n"
   ],
   "id": "fe7293f0ee8060a8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "\n",
    "# 假设数据格式是 [x_imu, y_imu, z_imu, x_gry, y_gry, z_gry]\n",
    "def prepare_data(imu_data):\n",
    "    # 将数据转换为PyTorch张量\n",
    "    imu_tensor = torch.tensor(imu_data, dtype=torch.float32)\n",
    "    # 添加批次维度和时间步维度，使得输入符合RNN的要求\n",
    "    imu_tensor = imu_tensor.unsqueeze(0).unsqueeze(1)  # (batch_size, seq_length, feature_size)\n",
    "    return imu_tensor\n",
    "\n",
    "# 准备数据\n",
    "input_tensor = prepare_data(imu_data)\n"
   ],
   "id": "7b7df21fef069f0b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "\n",
    "# 假设数据格式是 [x_imu, y_imu, z_imu, x_gry, y_gry, z_gry]\n",
    "def prepare_data(imu_data):\n",
    "    # 将数据转换为PyTorch张量\n",
    "    imu_tensor = torch.tensor(imu_data, dtype=torch.float32)\n",
    "    # 添加批次维度和时间步维度，使得输入符合RNN的要求\n",
    "    imu_tensor = imu_tensor.unsqueeze(0).unsqueeze(1)  # (batch_size, seq_length, feature_size)\n",
    "    return imu_tensor\n",
    "\n",
    "# 准备数据\n",
    "input_tensor = prepare_data(imu_data)\n"
   ],
   "id": "483a783b4c63969c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model = RNNModel(input_size=6, hidden_size=64, output_size=len(label_encoder.classes_), num_layers=1)\n",
    "model.load_state_dict(torch.load('rnn_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "# 使用模型进行推理\n",
    "with torch.no_grad():\n",
    "    output = model(input_tensor)\n",
    "    _, predicted_label = torch.max(output, 1)\n",
    "\n",
    "# 将预测结果转换为实际标签\n",
    "predicted_label_str = label_encoder.inverse_transform(predicted_label.cpu().numpy())\n",
    "print(f'Predicted label: {predicted_label_str[0]}')\n"
   ],
   "id": "20517e71cd2394ed"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import time\n",
    "\n",
    "while True:\n",
    "    imu_data = read_imu_data()\n",
    "    input_tensor = prepare_data(imu_data)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        _, predicted_label = torch.max(output, 1)\n",
    "    \n",
    "    predicted_label_str = label_encoder.inverse_transform(predicted_label.cpu().numpy())\n",
    "    print(f'Predicted label: {predicted_label_str[0]}')\n",
    "\n",
    "    # 可以加一个暂停时间，避免无限循环\n",
    "    time.sleep(1)  # 例如，每秒推理一次\n"
   ],
   "id": "8d59905ebb9e1b2c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
